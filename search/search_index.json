{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Learning Journey  <p>   Hey there! \ud83d\udc4b Welcome to my Learning Journey! \ud83d\ude80 This is like my personal cheat sheet for all things tech related. </p> <p>   This project contains a collection of notes, code snippets, and other resources that I've found useful while diving into Cloud, Architecture, DevOps, Backend tools, and more. </p>"},{"location":"IAM/","title":"Identity and Access Management (IAM)","text":""},{"location":"IAM/#what-is-identity-and-access-management-iam","title":"What is identity and access management (IAM)?","text":"<p>Identity and access management provides control over user validation and resource access. Commonly known as IAM, this technology ensures that the right people access the right digital resources at the right time and for the right reasons. IAM solution is a gatekeeper to the resources you provide to customers as web applications, APIs, etc.</p> <p>IAM includes authentication, authorization, and administration. Authentication is the process of verifying the identity of a user, device, or other entity. Authorization is the process of verifying that the authenticated user has access to the requested resource. Administration is the process of managing digital identities and controlling access to resources.</p>"},{"location":"IAM/#iam-basic-concepts","title":"IAM basic concepts","text":"<p>To understand IAM, you must be familiar with some fundamental concepts:</p> <ul> <li> <p>A digital resource is any combination of applications and data in a computer system. Examples of digital resources include web applications, APIs, platforms, devices, or databases.</p> </li> <li> <p>The core of IAM is identity. Someone wants access to your resource. It could be a customer, employee, member, participant, and so on. In IAM, a user account is a digital identity. User accounts can also represent non-humans, such as software, Internet of Things devices, or robotics.</p> </li> <li> <p>Authentication is the process of verifying the identity of a user, device, or other entity. Authentication is the first step in the process of access control. Someone (or something) authenticates to prove that they're the user they claim to be.</p> </li> <li> <p>Authorization is the process of verifying that the authenticated user has access to the requested resource. Authorization is the second step in the process of access control. Someone (or something) is authorized to access a resource if they have permission to do so.</p> </li> </ul>"},{"location":"IAM/#authentication-vs-authorization","title":"Authentication vs Authorization","text":"<p>Authentication and authorization are two different processes. Authentication proves a user\u2019s identity, while authorization grants or denies the user\u2019s access to certain resources.</p> <p>Authentication: When you enter the building, you must show your photo ID badge to the security guard. The guard compares the photo on the badge to your face. If they match, the guard lets you through the door to try to access different areas of the building. The guard doesn\u2019t tell you what rooms you can access; they only get proof that you are who you claim to be. This is authentication: confirming user identity.</p> <p>Authorization: In this scenario, imagine the elevators and doorways in the building have key sensors for access. The chip in your badge gives you access only to the first floor, which your company occupies. If you swipe your badge to enter any other floor, your access is denied. You can access your private office but not those belonging to your colleagues. You can enter the supply room but not the server room. This is authorization: granting and denying access to different resources based on identity.</p>"},{"location":"IAM/#what-does-iam-do","title":"What does IAM do?","text":"<p>Identity and access management gives you control over user validation and resource access:</p> <ul> <li>How users become a part of your system</li> <li>What user information to store</li> <li>How users can prove their identity</li> <li>When and how often users must prove their identity</li> <li>The experience of proving identity</li> <li>Who can and cannot access different resources</li> </ul> <p>You integrate IAM with your application, API, device, data store, or other technology. This integration can be very simple. For example, your web application might rely entirely on Facebook for authentication, and have an all-or-nothing authorization policy. Your app performs a simple check: if a user isn\u2019t currently logged in to Facebook in the current browser, you direct them to do so. Once authenticated, all users can access everything in your app.</p> <p>It\u2019s unlikely that such a simple IAM solution would meet the needs of your users, organization, industry, or compliance standards. In real life, IAM is complex. Most systems require some combination of these capabilities:</p> <ul> <li> <p>Seamless signup and login experiences \u2014 Smooth and professional login and signup experiences occur within your app, with your brand\u2019s look and language.</p> </li> <li> <p>Multiple sources of user identities \u2014 Users expect to be able to log in using a variety of social (such as Google or Linkedin), enterprise (such as Microsoft Active Directory), and other identity providers.</p> </li> <li> <p>Multi-factor authentication (MFA) \u2014 In an age when passwords are often stolen, requiring additional proof of identity is the new standard. Fingerprint authentication and one-time passwords are examples of common authentication methods.</p> </li> <li> <p>Step-up authentication \u2014 Access to advanced capabilities and sensitive information require stronger proof of identity than everyday tasks and data. Step-up authentication requires additional identity verification for selected areas and features.</p> </li> <li> <p>Attack protection \u2014 Preventing bots and bad actors from breaking into your system is fundamental to cybersecurity.</p> </li> <li> <p>Role-based access control (RBAC) \u2014 As the number of users grows, managing the access of each individual quickly becomes impractical. With RBAC, people who have the same role have the same access to resources.</p> </li> </ul> <p>Facing this level of complexity, many developers rely on an IAM platform instead of building their own solutions.</p>"},{"location":"IAM/#how-does-iam-work","title":"How does IAM work?","text":"<p>IAM is not one clearly defined system. IAM is a disciplina and a type of framework for solving the challenge of secure access to digital resources. Thers is no limit to the different approaches for implementing an IAM system.</p> <p>IAM systems are often built using a combination of these components:</p> <ul> <li> <p>Identity provider (IdP) \u2014 An identity provider is a service that authenticates users and gives them access to resources. IdPs are the core of IAM. They are the source of truth for user identities. IdPs can be built in-house or purchased from a third-party vendor.</p> <p>In the past, the standard for identity and access management was for a system to create and manage its own identity information for its users. Each time a user wanted to use a new web application, they filled in a form to create an account. The application stored all of their information, including login credentials, and performed its own authentication whenever a user signed in.</p> <p>As the internet grew and more and more applications became available, most people amassed countless user accounts, each with its own account name and password to remember. There are many applications that continue to work this way. But many others now rely on identity providers to reduce their development and maintenance burden and their users\u2019 effort.</p> <p>An identity provider creates, maintains, and manages identity information, and can provide authentication services to other applications. For example, Google Accounts is an identity provider. They store account information such as your user name, full name, job title, and email address. Slate online magazine lets you log in with Google (or another identity provider) rather than go through the steps of entering and storing your information anew.</p> <p>Identity providers don\u2019t share your authentication credentials with the apps that rely on them. Slate, for example, doesn\u2019t ever see your Google password. Google only lets Slate know that you\u2019ve proven your identity.</p> <p>Other identity providers include social media (such as Facebook or LinkedIn), enterprise (such as Microsoft Active Directory), and legal identity providers (such as Swedish BankID).</p> </li> <li> <p>Authentication Factors - Authentication factors are the different ways/methods that a user can prove their identity. The most common authentication factors are (IAM systems require one or many authentication factors to verify identity.):</p> <ul> <li> <p>Knowledge (something you know) \u2014 Something the user knows, such as a password, PIN, or the answer to a security question.</p> </li> <li> <p>Possession (something you have) \u2014 Something the user has, such as a mobile phone, hardware token, or smart card.</p> </li> <li> <p>Inherence (something you are) \u2014 Something the user is, such as a fingerprint, retina, or face.</p> </li> <li> <p>Location (somewhere you are) \u2014 Somewhere the user is, such as a specific IP address or GPS coordinates.</p> </li> <li> <p>Time \u2014 When the user is trying to access the resource, such as a specific time of day or a time-limited one-time password.</p> </li> <li> <p>Behavioral \u2014 How the user behaves, such as their typing speed or the way they move their mouse.</p> </li> <li> <p>Transaction \u2014 The transaction itself, such as the amount of money being transferred or the type of data being accessed.</p> </li> </ul> </li> <li> <p>Authentication and Authorization Standards - Authentication and authorization standards are open specifications and protocols that define how authentication and authorization work and provides guidance on how to: Design IAM system, Move personal data securely, decide who can access resources, and more. These IAM industry standards are considered the most secure, reliable, and pratical to implement:</p> <ul> <li> <p>OAuth 2.0 \u2014 OAuth 2.0 is a delegation protocol for accessing APIs and is the industry-standard protocol for IAM. An open authorization protocol, OAuth 2.0 lets an app access resources hosted by other web apps on behalf of a user without ever sharing the user\u2019s credentials. It\u2019s the standard that allows third-party developers to rely on large social platforms like Facebook, Google, and Twitter for login.</p> </li> <li> <p>OpenID Connect (OIDC) \u2014 OIDC is an authentication standard that uses JSON Web Tokens (JWTs) to pass identity information between identity providers and applications. OIDC is built on top of the OAuth 2.0 authorization framework. OIDC makes it easy to verify a user\u2019s identity and obtain basic profile information from the identity provider. OIDC is another open standard protocol.</p> </li> <li> <p>JSON web tokens (JWTs) - JWTS are an open standard that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. JWTs can be verified and trusted because they\u2019re digitally signed. They can be used to pass the identity of authenticated users between the identity provider and the service requesting the authentication. They also can be authenticated and encrypted.</p> </li> <li> <p>Security Assertion Markup Language (SAML) \u2014 SAML is an open-standard, XML-based data format that lets businesses communicate user authentication and authorization information to partner companies and enterprise applications that their employees use.</p> </li> <li> <p>WS-Federation \u2014 Developed by Microsoft and used extensively in their applications, this standard defines the way security tokens can be transported between different entities to exchange identity and authorization information.</p> </li> </ul> </li> </ul>"},{"location":"IAM/#popular-iam-solutions","title":"Popular IAM solutions","text":"<ul> <li>Auth0</li> <li>Okta</li> <li>Keycloak (open-source)</li> </ul>"},{"location":"IAM/#refs","title":"Refs","text":"<ul> <li>Auth0 identity-and-access-management</li> </ul>"},{"location":"domain-driven-design/","title":"Domain Driven Design","text":"<p>Defines how to model software in general.</p> <p>It is a way of developing software with a focus on the heart of the application - what we call the domain - with the objective of understanding its rules, processes and complexities, thus separating them from other complex points that are normally added during the development process.</p> <p>DDD is about modeling ubiquitous (universal) language within a bounded context.</p>"},{"location":"domain-driven-design/#complex-software","title":"Complex software","text":"<ul> <li>DDD is / should be applied for cases of complex software projects;</li> <li>Large projects have many areas, many business rules, many people with different views in different contexts;</li> <li>There is no way not to use advanced techniques in highly complex projects;</li> <li>Much of the complexity of this type of software does not come from technology, but from communication, context separation, understanding the business from different angles;</li> <li>People (Product Manager, Owner, Developer, Experts, Architects, ...)</li> </ul> <p>DDD should be applied to large projects, which have several areas, several business areas, and people who are in different contexts.</p> <p>DDD was created to provide more clarity in the project, to understand that the software is not just another one. Our role as a developer is to understand this and know how to model for each type of system and context.</p> <p>Domain-Driven Design is, above all,\u00a0communication. In DDD modeling and implementation go hand in hand.</p> <p>Domain experts (users, analysts and other experts in the field), along with developers and architects work hand in hand with a common goal:\u00a0build domain-driven software to meet customer needs.</p> <p>To do this, in the first place,\u00a0it is necessary that everyone use a common language\u00a0and that there is no translation in the communication between team members.</p>"},{"location":"domain-driven-design/#how-can-ddd-help","title":"How can DDD help?","text":"<ul> <li> <p>Understand in depth the domain (domain) and subdomains of the application. Domain is something that is connected with the main function of the application that we are going to develop, it is the heart of the software that we are going to develop; Subdomain is the separation of the domain into pieces, which make the pieces fit together to create the core of the application. It is important to understand the limit of each subdomain.</p> </li> <li> <p>Have a universal language (ubiquitous language) among all involved. Use jargon from the area that is used in that context.</p> <p>Ex: the customer of a B2B company are companies, but the customer of a bakery is people, so the entity 'customer' will not always be the same thing, it all depends on the context.</p> <p>If the company calls invoice ABC, in our application we should also call it that because it will facilitate communication between Solution Architect - Company, keeping everyone in the same context</p> </li> <li> <p>Create the strategic design using Bounded Contexts. Understand the context / boundaries of each subdomain</p> </li> <li> <p>Create the tactical design to be able to map and aggregate the application's entities and value objects, as well as domain events.</p> </li> <li> <p>Clarity of what is business complexity and technical complexity.</p> </li> </ul>"},{"location":"domain-driven-design/#domain-vs-subdomain","title":"Domain vs Subdomain","text":"<p>Domain is the problem as a whole, when we explore this domain/problem further, we begin to identify some points that can be treated in isolation, and each point has a degree of importance within the main domain.</p> <p>Example: Within Netflix, the main core (core domain) is the streaming service, listing the catalog and playing the movies, if you don't have this, it makes no sense for the application to exist</p> <p>Core Domain:</p> <ul> <li>Heart of the business;</li> <li>Competitive advantage of the company;</li> </ul> <p>Support subdomain:</p> <ul> <li> <p>Support the Domain on a day-to-day basis;</p> <p>For example: within an ecommerce, the products and checkout are the core domain, but a warehouse (distribution center) is needed, which works as the support subdomain</p> </li> <li> <p>Makes domain operation possible;</p> </li> </ul> <p>Generic subdomain</p> <ul> <li>Supports the subdomain and core domain but does not generate a competitive advantage for the company;</li> <li>Are easily replaceable;</li> <li>Auxiliary software;</li> </ul>"},{"location":"domain-driven-design/#problem-space-vs-solution-space","title":"Problem Space vs Solution Space","text":"<p>DDD \u2192 Understand the problem and how you model the problem so you can solve it.</p> <p>With that, we end up having a problem space and a solution space</p> <pre><code>Problem                             Solution\n-----------------------      ------------------------------\n(1) Domain overview      =&gt;   (3) Analysis and modeling of the \n                                  domain and its complexities.\n\n        \u2b07\ufe0f                             \u2b07\ufe0f\n        \u2b07\ufe0f                             \u2b07\ufe0f\n\n(2) Subdomains           =&gt;   (4) Delimited contexts\n</code></pre> <p>When we have an overview of the domain and its complexities, we begin to understand and separate the subdomains, still in the problem space.</p> <p>Solution \u21d2 How can I understand this problem and organize it in a way that I can solve all this, take each subdomain and delimit contexts, each generated context ends up becoming a sub product;</p> <p>This way we can have the domain modeled and the contexts delimited so that we can understand what we have to do and what is the priority.</p> <p>Subdomains with delimited contexts \u21d2 specific problem we have to solve.</p> <p>Bounded Contexts</p> <p>When we have a subdomain we can delimit them to create a bounded context.</p> <p>A bounded context is an explicit division within a domain model. A border/boundary of a domain we are modeling.</p> <p>One of the ways we manage to bring this delimitation is through ubiquitous language, we can understand where we are through the language that is being spoken in that context.</p> <p>When everyone is speaking the same language we realize that we are in the same context, when that language starts to change we begin to identify that we are going to another context.</p> <p>\u201cContext is King\u201d</p> <p>The context will always determine in which area of the company we are working, what type of problem we are trying to solve, and also the language can be the same from one place to another but have different meanings.</p> <p>Examples:</p> <ul> <li>Ticket sales \u21d2 Ticket // ticket purchase</li> <li>Customer support \u21d2 Ticket // support request identifier</li> </ul> <p>When we have two words that are the same but that represent different things, we are in a different context.</p> <p>If we are in a monolithic system, we will have to create different entities for each one according to its context.</p> <ul> <li>Finance \u21d2 eNFS</li> <li>Inventory \u21d2 XPTO (name for invoice)</li> </ul> <p>When we have two different words that means the same thing we are probably in a different context.</p> <p>Cross elements</p> <p>Often, despite being in different contexts, these models end up talking between entities / transversal elements.</p> <p>Example:</p> <ul> <li>Ticket sales \u21d2 Customer</li> <li>Customer support \u21d2 Customer</li> </ul> <p>Despite being different context is the same entity, however, despite being the same entity they will have different information.</p> <ul> <li>Ticket sales \u21d2 Customer \u2192 Event, Ticket, Location, Seller...</li> <li>Customer support \u21d2 Customer \u2192 Ticket, Question, Department, Responsible,...</li> </ul> <p>For each context, an entity will have to be customized for it, as having a class that wants to handle everything ends up being unfeasible and difficult to maintain</p>"},{"location":"domain-driven-design/#entities","title":"Entities","text":"<p>Entities are unique, they must have unique IDs, each one is different from the other;</p> <p>Entities carry attributes that can change over time (removing or adding);</p> <p>Anemic entities <pre><code>class Customer {\n    _id: string;\n    _name: string;\n    _address: string\n\n    constructor(id: string, name: string, address: string) {\n        this._id = id;\n        this._name = name;\n        this._address = address\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    get address(): string {\n        return this._address\n    }\n\n    set name(name: string) {\n        this._name = name\n    } \n\n    set address(address: string) {\n        this._address = address\n    }\n}\n</code></pre></p> <p>Anemic entities are the type that only carry data and change the name of properties. And we usually create these entities to be manipulated by the ORM, these are entities that don't have a significant value in themselves.</p> <p>Rich entities are entities that have unique value, data can be changed, have behavior and carry business rules, and it is in these business rules that the heart of the business lives.</p> <p>Now, instead of entities just loading data, they will define how the entity should behave (business rules, self-validation, etc...), because if the client says that the XPTO entity now behaves differently, that's it entity that we must change.</p> <p>The first thing we must do when we think of an Entity is to think about what kind of behavior it will carry.</p> <p>Ex:</p> <p>This type of change doesn't have any expressiveness, it's just there for the sake of it, it's a method of changing an attribute <pre><code>    set name(name: string) {\n        this._name = name\n    } \n</code></pre></p> <p>After:</p> <p>It means that this Entity, its business rule, allows a user to change its name, the entity needs this method <pre><code>changeName(name: string) {\n        this._name = name\n    } \n</code></pre> Consistency in the first place: an Entity will always have to represent the correct and current state of that element, that is, if in the database the entity has (_id, name, address and status) and all fields are mandatory, ALWAYS ALWAYS all data from that entity must have these attributes, if by any chance a piece of data does not have an address, it means that it is violating the DDD and the rich domain rules.</p> <p>Whenever we consult this entity, the data has to be consistent, since it is not consistent, we cannot validate business rules. When we talk about DDD we must be able to trust 100% of the time in the current state of the object.</p> <p>Principle of self-validation: An Entity, by default, it must always self-validate</p> <p>Problem:</p> <p>If we leave the responsibility to another system resource, the data may be inconsistent. <pre><code>class Customer {\n    _id: string;\n    _name: string;\n\n    constructor(id: string, name: string) {\n        this._id = id;\n        this._name = name;\n        this.isValid()\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    changeName(name: string) {\n        this._name = name\n        this.isValid()\n    }\n\n    isValid() {\n        if(this.name.length &lt; 5) {\n            throw ValidationError(\"name length must the greater than 5.\")\n        }\n    }\n}\n\nconst newCustomer = new Customer(\"1\", \"\")\n</code></pre></p> <p>Architecture: <pre><code>src/...\n\n    // Business complexity / business rules\n    // A single way to run, the way the customer is asking.\n    /domain/... \n\n    // Accidental Complexity / Conversation with the Outside World / Storing Data...\n    // n ways to solve, Excel, Sass, DB, Cloud,...\n    /infra/... &lt;- \n</code></pre></p>"},{"location":"domain-driven-design/#object-values","title":"Object Values","text":"<p>\u201cWhen you only care about the attributes of a model element, classify it as a Value Object\u201d</p> <p>A Value Object is immutable, it doesn't change it is replaced by another VO.</p> <p>EX: <pre><code>class Address {\n    _street: string;\n    _number: number;\n    _zip: string;\n    _city: string;\n\n    constructor(street: string, number: number, zip: string, city: string) {\n        this._street = street;\n        this._number = number;\n        this._zip = zip;\n        this._city = city\n    }\n}\n\nclass Customer {\n    _id: string;\n    _name: string;\n    _address!: Address;\n\n    constructor(id: string, name: string) {\n        this._id = id;\n        this._name = name;\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    get address(): Address {\n        return this._address\n    }\n\n    set name(name: string) {\n        this._name = name\n    } \n\n    set address(address: Address) {\n        this._address = address\n    }\n}\n</code></pre></p>"},{"location":"domain-driven-design/#domain-services","title":"Domain Services","text":"<p>A domain service is a stateless operation that performs a domain-specific task. Often the best indication that you should create a Service in the domain model is when the operation you need to perform doesn't seem to fit as a method on an Aggregate or a Value Object.</p> <p>When a significant process or transformation in the domain is not the natural responsibility of an ENTITY or Value Object, add an operation to the model as a stand-alone interface declared as a SERVICE. Set the interface in based on the domain model language and make sure the operation name is part of the UBIQUITOUS LANGUAGE. Make the SERVICE stateless.</p> <p>Domain Service is stateless;</p> <p>Any alteration / transformation / transaction,..., anything that affects the domain and that we cannot perform in the Entity class itself, because we need access to another Entity, OV, or something external is because we need to use a Domain Service .</p> <p>These services run on the Domain layer, where the business rules take place.</p> <p>When to create domain service?</p> <ul> <li>Can an entity perform an action that will affect all entities?</li> <li>How to perform a batch operation?</li> <li>How to calculate something whose information is contained in more than one entity?</li> </ul> <p>Careful:</p> <ul> <li>When there are many Domain Services in your project, MAYBE this could indicate that your aggregates are anemic. (only get and set)</li> <li>Domain Services are Stateless (do not keep state)</li> </ul>"},{"location":"domain-driven-design/#repositories","title":"Repositories","text":"<p>A repository commonly refers to a place of storage, generally considered a place of security or preservation of the items stored there. When you store something in a repository and then go back to retrieve it, you expect it to be in the same state it was in when you put it there. At some point, you can choose to remove the stored item from the repository.</p> <p>These objects, similar to collections, are about persistence. Every Persistent Aggregate type will have a Repository. Generally speaking, there is a 1-1 relationship between an Aggregate type and a Repository.</p>"},{"location":"domain-driven-design/#domain-events","title":"Domain Events","text":"<p>Use a domain event to capture an instance of something that happened in the domain.</p> <p>The essence of a domain event is that you use it to capture things that might trigger a change in the state of the application you are developing. These event objects are processed to cause system changes and stored to provide an AuditLog.</p> <p>Perform an operation based on an event or store a log;</p> <p>Every event must be represented in an action performed in the past;</p> <p>Ex:</p> <ul> <li>UserCreated;</li> <li>OrderPlaced;</li> <li>EmailSent;</li> </ul> <p>When to use?</p> <p>Normally a Domain Event should be used when we want to notify other Bounded Contexts of a state change.</p> <p>Communication with external contexts;</p> <p>Components</p> <ul> <li>Event \u2192 contains the message along with the timestamp of when it occurred;</li> <li>Handler \u2192 concrete implementation of the event that is triggered, performs processing when an event is called, an event can have multiple handlers. ex: send email, call external api, send push notification,...</li> <li>Event Dispatcher \u2192 Responsible for storing and executing the handlers of an event when it is triggered; logs all events and their handlers.</li> </ul> <p>Flow:</p> <ul> <li>Create an \u201cEvent Dispatcher\u201d</li> <li>Create an \u201cEvent\u201d</li> <li>Create a \u201cHandler\u201d for the \u201cEvent\u201d</li> <li>Registers the Event, along with the Handler in the \u201cEvent Dispatcher\u201d</li> </ul> <p>Now to trigger an event, just execute the \u201cnotify\u201d method of the \u201cEvent Dispatcher\u201d. At that moment, all Handlers registered in the event will be executed.</p>"},{"location":"domain-driven-design/#modules","title":"Modules","text":"<p>In a DDD context, Modules in your model serve as named containers for classes of domain objects that are highly cohesive with each other. The goal should be loose coupling between classes that are in different modules. Since the Modules used in DDD are not anemic or generic storage bins, it is also important to properly name the Modules.</p> <ul> <li>Represents the application represented by the domain;</li> <li>The modules must respect the universal (ubiquitous) language;</li> <li>Low coupling;</li> <li>One or more aggregates must be together only if they make sense;</li> <li>Organized by domain / subdomain and not by type of objects</li> <li>They must respect the same division when they are in different layers (infra, domain, ...) it is easier to maintain a correlation;</li> </ul>"},{"location":"domain-driven-design/#factories","title":"Factories","text":"<p>Shift the responsibility for creating instances of complex objects and Aggregates to a separate object, which may not have responsibility in the domain model but is still part of the domain design. Provide an interface that encapsulates all complex creation and doesn't require the client to reference the concrete classes of the objects being instantiated. Create entire Aggregates at once, enforcing their invariants.</p> <p>Every time we need to create a complex object, we can delegate this to the factory.</p> <pre><code>Client =&gt; (Input, Specifies what it wants) =&gt;\nFactory =&gt; (Makes object that satisfies client and internal rules) =&gt;\nObject =&gt; Output\n</code></pre>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Notes</p> <ul> <li>K8S is made available through a set of APIs;</li> <li>We normally access the API using the CLI: kubectl;</li> <li>Everything is state based. You configure the state of each object;</li> <li>Kubernetes Master: Control the entire process of what the others node are going to do;</li> <li>Kube-apiserver</li> <li>Kube-controller-manager</li> <li>Kube-scheduler</li> <li>Other Nodes:</li> <li>Kubelet</li> <li>Kubeproxy</li> </ul>"},{"location":"kubernetes/#structure","title":"Structure","text":"<p>Master(Control Plane) \u2192 Manage the entire process; it must be high available;</p> <ul> <li>Etcd \u2192 Database, key value store for critical info, it stores the current state of the cluster, define which container is going to each node, what time they are getting loaded, etc;</li> <li>Scheduler \u2192 Decide which worker node will be best to deploy the next pods, it takes multiple factors in the scheduleing decision, such as resource requirements of the container, policy, data locality's, internal workload,\u2026;</li> <li>Controller Manager \u2192 Ensures proper states of cluster components, configuration defined through a manifest file, ex: Desired State: 3 nodes, Current State: 2, it works to match the state requirements;</li> <li>API \u2192 Exposes kubernetes api, how we communicate with kubernetes master;</li> </ul> <p>Node(Data Plane) \u2192 Host containers, ex: a ec2 can work as a node,</p> <ul> <li> <p>Kubelet \u2192 Agent that runs in each node in the cluster, it makes sure the containers are running ok, verify the status of the node and containers to Control Plane, and, is how it communicate with the master node.</p> </li> <li> <p>Kube Proxy \u2192 How applications communicate with each other, it maintains network rules, this rules allows network communication to your conntainer from inside or outside of your cluster.</p> </li> </ul>"},{"location":"kubernetes/#components","title":"Components","text":"<p>Cluster: Set of machines</p> <p>Pods: Unit that contains the provisioned containers</p> <p>Pod represents the process running on the cluster. Normally a pod represents a container but we can have more than one container running in the same pod (even though is not so commom)</p> <p>ReplicaSet: it makes sure the desired amount of pods will be running, if a pod goes down it will create another one automatically</p> <p>Ex:</p> <pre><code>B = Backend \u21d2 3 replicas\nF = Frontend \u21d2 2 replicas\n</code></pre> <p>If we ask to add another Frontend Pod in the cluster and it has no more resources, the task will be pending until it has more computational resources or we create another node (cluster) to place this task on the next available node.</p> <p>k8s is monitoring the health of clusters and pods to be able to recreate if they are no longer available</p> <p>Deployment: Wrapper around replica set, it has the objective of provisioning the pods, it will define how many replicas we want from each Pod, but also it helps on the upgrade container version, scale, undo,\u2026</p> <p>Deployment will restore replicaset with running pods</p> <p></p> <pre><code>replicas: 3\n...\nminReadySeconds:\nstrategy:\n    # when update the container image to a new version\n    rollingUpdate:\n        maxSurge: 1 # it will create a new pod with the new image version and when available it will remove the older version\n        maxUnavailable: 0 # the update has to be done in such a way that there is at lest three pods running\n    type: RollingUpdate\n\n# 3 replicas + 1 maxSurge, a total of 4 pods at time\n</code></pre> <p>Service</p> <p>Expose an application running on a set of pods as a network service.</p> <p>Instead of a pod communicate with another using IP address it can use a service that distribute traffic to the pods.</p> <p>Is a service discovery tool that also works as load balancer.</p> <p>If a pod/node goes down this service will redirect the traffic for the new pods as well. it keeps track of new pods/nodes</p> <p>It works with label selector.</p> <p>Different types of service:</p> <ul> <li>ClusterIP   Default type of service, only accessible from within cluster, if you are outside the cluster and you want to access the service you can't do it.</li> <li>LoadBalancer   Cloud specific implementation, accessible from outside cluster, has dns name, ssl termination, WAF integration, Access Logs, Health Check, etc.   when we run a 'kubectl get services' in a cloud provider the LoadBalancer type will generate an External IP to us so we can use this IP to access our service</li> <li>Nodeport   Accessible from outside cluster, creates cluster wide pord. NodePort[30000-32767]   ex: 10.16.10.01:32000, that way you can access your application</li> </ul> <p>Namespaces</p> <p>Isolated environment, we can group resources separately like a database, app1, app2, ...</p>"},{"location":"kubernetes/#eks","title":"EKS","text":"<p>AWS manages the Control Plane(Master Node)</p> <ul> <li>AWS maintains high availability - Multiple EC2s in Multiple AZs</li> <li>Detects and replaces unhealthy control plane instances</li> <li>Scales control plane</li> <li>Maintain etcd</li> <li>Provides automated version upgrade and patching</li> <li>supports native and upstream kubernetes</li> </ul> <p>EKS Data Plane</p> <ul> <li>Amazon EC2 - Self Managed Node Groups</li> <li>You maintain worker EC2s</li> <li>You orchestrate version upgrade, security patching, AMI Rehydration, keeping pods up during upgrade;</li> <li>Can use custom AMI</li> <li>Amazon EC2 - Managed Node Groups</li> <li>AWS manages worker EC2s</li> <li>AWS provides AMI with security patches, version upgrade,...</li> <li>AWS manages pod disruption during upgrade</li> <li>Doesn't work with custom AMI</li> <li>AWS Fargate</li> <li>No worker EC2 whatsoever !</li> <li>You define and deploy pods</li> <li>Container + Serverless</li> </ul>"},{"location":"rabbitmq/","title":"RabbitMQ","text":"<p>RabbitMQ is an open-source message broker software, highly consolidated, used to work with communication between systems. Operating asynchronously, it acts as an intermediary that processes our messages between producers and consumers, in addition to having queues that have different forwarding options. It is widely used in distributed systems to enable communication between applications and services.</p>"},{"location":"rabbitmq/#features","title":"Features","text":"<ul> <li>Message Broker;</li> <li>Implements AMQP, MQTT, STOMP and HTTP;</li> <li>Developed in Erlang;</li> <li>Allow decoupling between services;</li> <li>Fast and Powerful; Stores data in memory;</li> <li>Durability, the messages can be stored in memory or on disk, depending on the configuration.</li> <li>Scalability</li> <li>Priority, which means that messages with higher priority are processed first.</li> <li>Many companies use it, well tested.</li> </ul>"},{"location":"rabbitmq/#under-the-hood","title":"Under the Hood","text":"<p>RabbitMQ is based on the message queue model, where messages are sent from producers (publishers) to consumers through a broker (RabbitMQ server).</p> <p>It only opens a single persistent connection (which speeds up the process as it doesn't need to create TCP connections all the time) and then configures the channels (sub-connection) for consumers to retrieve the data they need;</p> <p>1 Thread per channel; New thread for new channel;</p> <p>Basic Functioning:</p> <p>Publisher \u21d2 Publishes the message for whoever wants to consume it. It sends messages to an exchange, which routes the messages to the appropriate queue.</p> <p>Consumer \u21d2 Retrieves messages from a queue.</p> <p>Queue \u21d2 The Publisher does not send a message directly to the consumer. Instead, the message falls into a queue, and the consumer reads that queue of messages, and each message they read is deleted. Multiple consumers can be connected to the same queue, and messages are distributed to consumers in a round-robin fashion by default. Basically, is a routing agent that receives messages from producers and routes them to one or more queues.</p> <p>Exchange \u21d2 Retrieves the message the publisher sent and discovers which queue the message will be sent to, because in some cases, the message may go to more than one queue.</p> <p>Flow:</p> <p>[Publisher] \u21d2 [Exchange] \u21d2 [Queue] \u21d2 [Consumer]</p> <p>The publisher publishes a message, the exchange redirects it to the queue(s), and the consumer listens to the queue to process the message and delete it afterwards.</p>"},{"location":"rabbitmq/#types-of-exchange","title":"Types of Exchange","text":"<ul> <li>Direct: The Exchange sends the message specifically to a certain queue</li> <li>Fanout: The exchange sends the message to all the queues that are binded/related to this exchange, if there are 10 related queues the message is sent to the 10 queues</li> <li>Topic: It has rules, e.g.: depending on the 'route key' it will forward to the queue we want.</li> <li>Headers: In the message header, we determine which queue we want the exchange to deliver to. It's not very common to use.</li> </ul> <p>The system can have several exchanges with several queues and each queue can be related to one or more exchanges.</p> <p>Playground: http://tryrabbitmq.com/</p>"},{"location":"rabbitmq/#direct-exchange","title":"Direct Exchange","text":"<p>Uses a message routing key to transport messages to queues. The routing key is a message attribute that the producer adds to the message header. You can consider the routing key to be an \u201caddress\u201d that the exchange uses to determine how the message should be routed. A message is delivered to the queue with the binding key that exactly matches the message\u2019s routing key</p> <p>The direct exchange\u2019s default exchange is \u201camq. direct\u201c, which AMQP brokers must offer for communication</p> <p>As is shown in the figure, queue A (create_pdf_queue) is tied to a direct exchange (pdf_events) with the binding key \u201cpdf_create\u201d. When a new message arrives at the direct exchange with the routing key \u201cpdf_create\u201d, the exchange sends it to the queue where the binding key = routing key; which is queue A in this example (create_pdf_queue).</p> <p></p> <p>The exchange binds the related queue with an exchange so it can forward the messages The message passes the Routing Key and the Exchange redirects to the respective queue.</p> <p></p>"},{"location":"rabbitmq/#fanout-exchange","title":"Fanout Exchange","text":"<p>A fanout exchange, like direct and topic exchange, duplicates and routes a received message to any associated queues, regardless of routing keys or pattern matching. Here, your provided keys will be entirely ignored.</p> <p>Fanout exchanges are useful when the same message needs to be passed to one or perhaps more queues with consumers who may process the message differently</p> <ul> <li>A single message goes to all queues;</li> <li>Does not have routing key;</li> <li>If we have several consumers consuming the same queue, RabbitMQ will create a kind of loadbalancer and send a distributed message to all</li> </ul> <p>Ex: An e-commerce system has several sectors (purchase, marketing, invoice, log, ...)</p> <p>If we want that when a user makes a purchase the message is sent to all queues we can use this model, the queues are connected in the exchange and we send the messages.</p> <p></p>"},{"location":"rabbitmq/#topic-exchange","title":"Topic Exchange","text":"<p>Sends messages to queues depending on wildcard matches between the routing key and the queue binding\u2019s routing pattern. Messages are routed to one or more queues based on a pattern that matches a message routing key.</p> <ul> <li>Routing keys have rules, similar to Regex; Ex: X.LOG, T.Y, ...</li> </ul> <p></p>"},{"location":"rabbitmq/#queues","title":"Queues","text":"<ul> <li>FIFO \u21d2 First In, First Out   The message that enters first is the first to leave.</li> <li>Properties:</li> <li>Durable: If it should be saved even after the broker restarts, persist on disk or keep it only in memory. If we restart the broker and the queue is not durable, it will be removed.</li> <li>Auto-delete: Automatically removed when the consumer disconnects. If the consumer disconnects, the queue will be deleted.</li> <li>Expiry: Defines the time that no messages or clients are consuming. For example, if no client is consuming this queue for 3 hours, it will be deleted.</li> <li>Message TTL: Message's lifetime, if not consumed during this time, the message will be automatically removed.</li> <li>Overflow:<ul> <li>Drop head (remove the oldest one), if it reaches the limit of messages for this queue and a new one arrives, remove the oldest one.</li> <li>Reject publish, does not allow more inputs; will receive an error.</li> </ul> </li> <li>Exclusive: Only the channel that created it can access it.</li> <li>Max Length or bytes: Maximum number of messages or maximum size in bytes allowed. For example, the queue can have a maximum of 10 messages. After that, it will fall under the overflow rule, or the queue can only have 2MB of messages.</li> </ul>"},{"location":"rabbitmq/#dead-letter-queues","title":"Dead Letter Queues","text":"<p>Some messages can't be delivered for any reason, no one read or processed them. After that, we can configure them to fall into a specific exchange that routes messages to a dead letter queue.</p> <p>Such messages can be consumed and examined later.</p>"},{"location":"rabbitmq/#lazy-queues","title":"Lazy Queues","text":"<p>When the message flow is too large for consumers to process everything, and the RabbitMQ memory limit is reached, these messages are stored on disk to ensure that more messages can arrive and not be lost.</p> <ul> <li>It requires high I/O, so it is more costly.</li> <li>When there are millions of messages in a queue, for any reason, there is the possibility of freeing up memory by specifically moving the messages from the relevant queue to disk.</li> </ul>"},{"location":"rabbitmq/#reliability","title":"Reliability","text":"<ul> <li>How to ensure that messages will not be lost along the way?</li> <li>How to ensure that messages could be correctly processed by consumers?</li> <li>What if each message was worth 1 million dollars?</li> </ul> <p>RabbitMQ features to solve such situations:</p> <ul> <li>Consumer acknowledgement \u21d2 consumer confirms receiving the message;</li> <li>Publisher confirms \u21d2 ensure that the message has reached the exchange;</li> <li>Durable/persisted queues and messages \u21d2 should not be used always because it costs memory and will slow down the system</li> </ul> <p>Types of Consumer Acknowledgement</p> <ul> <li>Basic.Ack   Every time we send a message to the consumer and it responds with \"I received and processed the message\"</li> <li>Basic.Reject   If the consumer receives the message and can't resolve it (throw an exception), the message goes back to the queue because the consumer couldn't resolve it.</li> <li>Basic.Nack   Same as Reject, but can reject more than one message at the same time.</li> </ul> <p>Publisher Confirms</p> <p>When we want to make sure that the message is going to the exchange,</p> <p>In this case, the message has an ID (we pass this integer ID in the message, ex 1), when the message receives this message, it returns to the publisher saying it received the message with ID 1. Ack: ID = 1</p> <p>If for some reason, the exchange has some internal problem, it will tell the publisher that it could not process the message, Nack: ID = 1</p> <p>For important messages, we should use Publisher confirms.</p>"},{"location":"rabbitmq/#use-cases","title":"Use Cases","text":"<ul> <li>Microservices: Used to enable communication between services. Each service can send and receive messages through RabbitMQ, which provides a scalable and reliable messaging solution.</li> <li>Real-time data processing: Can be used for real-time data processing, such as stream processing, event sourcing, and complex event processing (CEP). RabbitMQ can handle large volumes of messages and provide low-latency processing.</li> <li>Message-driven architecture: Messages can trigger actions in the system. For example, a message could trigger a workflow, a notification, or an update to a database.</li> <li>IoT: It can be used to enable communication between devices and services.</li> </ul>"},{"location":"rabbitmq/#refs","title":"Refs","text":"<ul> <li>rabbitmq-exchange-type</li> </ul>"},{"location":"AWS/1.regions-and-zones/","title":"Regions and Zones","text":""},{"location":"AWS/1.regions-and-zones/#aws-region","title":"AWS Region","text":"<p>An AWS Region is like a big cluster of computer houses around the world. Each AWS Region consists of multiple independent and physically separate Availability Zones (AZ) within a geographic area. All Regions currently have three or more Availability Zones.</p> <ul> <li>AWS has Regions all around the world</li> <li>Regions have names like us-east-1, eu-west-3.</li> <li>Most AWS services are region-scoped</li> <li>A region is a cluster of data centers</li> <li>Each region has many Availability Zones (usually 3, minimum is 2, maximum is 6).</li> <li>The resources and data that you create in one Region do not exist in any other Region unless you explicitly use a replication or copy feature offered by an AWS service or replicate the resource yourself.</li> </ul> <p>Example:</p> <p>Region: Australia (Sydney) Region</p> <p>Availability Zones:</p> <ol> <li>ap-southeast-2a</li> <li>ap-southeast-2b</li> <li>ap-southeast-2c</li> </ol>"},{"location":"AWS/1.regions-and-zones/#how-do-you-choose-an-aws-region","title":"How do you choose an AWS Region?","text":"<p>it depends, on some factors that may impact your choice:</p> <ul> <li>Compliance: with data governance and legal requirements, data never leaves a region without your explicit permission. Some countries want their data to stay in their own country. For example, if you're in Brazil, you might need to use the Sao Paulo region.</li> <li>Proximity to customers: reduced latency. So if most of your users are going to be in America, it makes a lot of sense to deploy your application in America, close to your users, because they will have a reduced latency. If you deploy your application in Australia and your users are in America, they will have a lot of lag in using your app.</li> <li>Available services within a Region: new services and new features aren't available in every Region. Some regions do not have services. So if you're leveraging a service with your application you need to make sure you're deploying into is available and does have that services.</li> <li>Pricing: The cost can change based on the Region. Look at the service pricing page for details.</li> </ul>"},{"location":"AWS/1.regions-and-zones/#availability-zones","title":"Availability Zones","text":"<p>Availability Zones (AZ) consist of one or more physical data centers that are redundantly connected to each other and the internet.</p> <p>AWS operates over 100 Availability Zones within several Regions around the world.</p> <ul> <li>An Availability Zone is one or more discrete data centers with independent and redundant power infrastructure, networking, and connectivity in an AWS Region;</li> <li>Availability Zones in a Region are meaningfully distant from each other, up to 60 miles (~100 km) so that they're isolated from disasters, but close enough to use synchronous replication with single-digit millisecond latency.</li> <li>They are designed to stay safe from common problems like power issues, water problems, earthquakes, fires, tornadoes, or floods.</li> <li>When AWS updates its services, deployments to Availability Zones in the same Region are separated in time to prevent correlated failure.</li> <li>All Availability Zones in a Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber. Each Availability Zone in a Region connects to the internet through two transit centers where AWS peers with multiple tier-1 internet providers.</li> </ul> <p>These features provide strong isolation of Availability Zones from each other, which we refer to as Availability Zone Independence (AZI).</p> <p></p>"},{"location":"AWS/1.regions-and-zones/#aws-point-of-presence-edge-locations","title":"AWS Point of Presence (Edge Locations)","text":"<p>In addition to the AWS Regions and Availability Zones, AWS also operates a globally distributed point of presence (PoP) network. These are spread around the world and have special places called Edge Locations.</p> <p>These Edge Locations are used by AWS to cache content closer to the users and these PoPs also host:</p> <ol> <li>Amazon CloudFront: a content delivery network (CDN) that is like a delivery service for internet content. It helps things load quickly for users all over the world. it delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and it's a developer-friendly environment.</li> <li>Amazon Route 53: a public Domain Name System (DNS) resolution service, which translates human-readable domain names like www.example.com into the numeric IP addresses.</li> <li>AWS Global Accelerator (AGA): an edge networking optimization service, which improves the availability and performance of your applications for local and global users. It optimizes how data moves around.</li> </ol> <p>The global edge network currently consists of over 410 PoPs, including more than 400 Edge Locations, and 13 regional mid-tier caches in over 90 cities across 48 countries.</p> <p></p> <p>Each PoP is isolated from the others, which means a failure affecting a single PoP or metropolitan area does not impact the rest of the global network.</p> <p>Some key things about these PoPs:</p> <ul> <li>Amazon has over 400 points of presence (400+ Edge Locations &amp; 10+ Regional Caches) in 90+ cities across 40+ countries.</li> <li>Content is delivered to end users with lower latency.</li> </ul>"},{"location":"AWS/2.iam/","title":"IAM","text":""},{"location":"AWS/2.iam/#overview","title":"Overview","text":"<p>IAM stands for Identity and Access Management and is a global service. in IAM we're going to create our users and assign them to groups and give them permissions to do certain things.</p> <p>Summary:</p> <ul> <li>IAM = Identity and Access Management, Global Service.</li> <li>When we create an AWS account, a root account is created by default. This account has full admin access to everything in our account. We should never use this account for security reasons..</li> <li>Users are people within your organization, and can be grouped; Users are mapped to a physical user, has a password for AWS Console.</li> <li>Groups only contain users, not other groups, can't be nested.</li> <li>Users don't have to belong to a group, and a user can belong to multiple groups.</li> <li>Policies \u21d2 JSON document that outlines permissions for users or groups</li> <li>Roles \u21d2 for EC2 instances or AWS services</li> <li>Security \u21d2 MFA + Password Policy</li> <li>Access Keys provide programmatic access to the AWS API, CLI, SDK, and other development tools.</li> <li>Audit \u21d2 IAM Credential Reports &amp; IAM Access Advisor</li> </ul> <p></p>"},{"location":"AWS/2.iam/#iam-permissions","title":"IAM Permissions","text":"<p>Users or Groups can be assigned JSON documents called polices.</p> <p>Example:</p> policy.json<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1234567890\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:ListAllMyBuckets\", \"s3:GetBucketLocation\"],\n      \"Resource\": [\"arn:aws:s3:::*\"]\n    },\n    {\n      \"Sid\": \"Stmt1234567891\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ec2:Describe*\"],\n      \"Resource\": [\"*\"]\n    },\n    {\n      \"Sid\": \"Stmt1234567892\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudwatch:ListMetrics\",\n        \"cloudwatch:GetMetricStatistics\",\n        \"cloudwatch:Describe*\"\n      ],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}\n</code></pre> <p>In the policy you define what a user or group is allowed to do in AWS.</p> <p>Policies consists of:</p> <ul> <li>Version: policy language version, always include \"2012-10-17\"</li> <li>Id: an identifier for the policy (optional)</li> <li>Statement: one or more individual statements (required)</li> <li>Statements consists of:<ul> <li>Sid: and identifier for the statement (optional)</li> <li>Effect: whether the statement allows or denies access (Allow/Deny)</li> <li>Principal: account/user/role to which this policy applied to</li> <li>Action: list of actions this policy allows or denies based on effect</li> <li>Resource: list of resources to which the actions applied to</li> <li>Condition: conditions for when this policy is in effect (optional)</li> </ul> </li> </ul> <p>It's recommended to apply the least privilege principle, which means that you should only give the minimum permissions required to do the job.</p>"},{"location":"AWS/2.iam/#adding-admin-user","title":"Adding Admin User","text":"<p>By default when we create a new AWS account we'll have a root account, but it's not a best practice to use the root account for security reasons. So therefore, we want to create users such as admin users that will allow us to use our account more safely.</p> <p>IAM \u21d2 Users tab \u21d2 Add Users:</p> <ul> <li>Provide a name;</li> <li>Select the option to allow access to the AWS Management Console;</li> <li>User type: IAM user;</li> <li>Console password \u21d2 custom password</li> <li>Require password reset \u21d2 doesn't need</li> <li>Next</li> <li>Add user to group</li> <li>Create a group =&gt; Admin</li> <li>Add the policy <code>AdministratorAccess</code></li> <li>Tags \u21d2 information's, ex: <code>Department: engineer</code></li> <li>Now we can create the user.</li> </ul>"},{"location":"AWS/2.iam/#login-with-iam-user","title":"Login With IAM User","text":"<p>In the IAM Dashboard, we can define an alias for our account so users can login with the alias instead of the account id.</p> <p>Go to IAM \u21d2 Dashboard</p> <p>In the right bar we have <code>Account Alias</code> where we can set an alias for our account. once updated, the sign-in url of our account will be updated to the alias.</p> <p>Now we can login with the IAM user we created.</p>"},{"location":"AWS/2.iam/#security","title":"Security","text":"<p>Now that we have created users and groups, it's time for us to protect these users and groups from being compromised. For this, we can have two defense mechanism.</p> <ol> <li> <p>Password Policy</p> <ul> <li>the stronger the password you use, the harder it is to crack it.</li> <li> <p>you can setup password policiy with different options:</p> <ol> <li>Minimum password length</li> <li>Require specific character types (uppercase letters, number,...,)</li> <li>Allow all IAM users to change their own passwords</li> <li>Require users to change their passwordd after some time (password expiration)</li> <li>Prevent password re-use</li> </ol> </li> </ul> </li> <li> <p>Multi Factory Authentication - MFA</p> <ul> <li>Users have access to your account and can possibly change configuration or delete resources in your AWS account.</li> <li>You want to protect your Root Accounts and IAM users.</li> <li>MFA = password you know + security device you own.</li> <li>Main benefit of MFA: if a password is stolen or hacked, the account is not compromised because the hacker would need the security device to login. </li> </ul> <p>MFA devices options in AWS:</p> <ol> <li>Virtual MFA device - Authenticate using a code generated by an app installed on your mobile device or computer. (Google Authenticator, Authy,...,)</li> <li>Universal 2nd Factor (U2F) security key - Authenticate using a physical key plugged into your computer's USB port. (YoubiKey by Yubico(3rd party))</li> <li>Hardware Key Fob MFA device - Authenticate using a code displayed on a hardware Time-based one-time password (TOTP) token. (Gemalto(3rd party),...,)</li> </ol> </li> </ol>"},{"location":"AWS/2.iam/#securing-your-account","title":"Securing your Account","text":"<p>IAM \u21d2 Account settings \u21d2 Edit Password Policy</p> <p>In the password policy options you can either use the <code>IAM default</code> or customize the settings to improve the security of our accounts.</p> <p>To set up MFA, you need to click on your account name in the top right corner and select <code>Security Credentials</code>. There you can set up MFA for your account.</p>"},{"location":"AWS/2.iam/#how-can-users-access-aws","title":"How can users access AWS?","text":"<ul> <li>To access AWS we have three options:<ol> <li>AWS Management console \u21d2 (Protected by password + MFA)</li> <li>AWS Command Line Interface (CLI) \u21d2 Protected by access keys</li> <li>AWS Software Developer Kit (SDK) \u21d2 Protected by access keys</li> </ol> </li> <li>Access Keys are generated through the AWS console</li> <li>Users manage their own access key</li> <li>Access Keys are secret, just like password<ul> <li>Access Key ID ~= Username</li> <li>Secret Access Key ~= Password</li> </ul> </li> </ul>"},{"location":"AWS/2.iam/#aws-cli","title":"AWS CLI","text":"<ul> <li>AWS CLI is a tool that enables you to interact with AWS services using commands in your command line shell</li> <li>Direct access to the public APIs of AWS services</li> <li>Follow this guid to install: docs.aws.amazon.com/cli</li> </ul>"},{"location":"AWS/2.iam/#aws-sdk","title":"AWS SDK","text":"<ul> <li>AWS Software Development Kit (AWS SDK)</li> <li>Language-specific APIs (set of libraries)</li> <li>Enables you to access and manage AWS services programmatically</li> <li>Embedded within your application</li> <li>Supports<ul> <li>SDKs (JavaScript, Python, PHP, Go, Node.js,...)</li> <li>Mobile SDKs (Android, iOS)</li> <li>IoT Devices SDKs (Embedded C, Arduino,...)</li> </ul> </li> </ul>"},{"location":"AWS/2.iam/#creating-access-keys","title":"Creating Access Keys","text":"<p>Note: we should never create access key for our root account for security reasons</p> <p>IAM \u21d2 Users \u21d2 Choose a user \u21d2 Security credentials</p> <ul> <li>In this tab we can create a access key</li> <li>From your terminal:       <pre><code>aws configure\n</code></pre></li> <li>You will be prompted to set the:<ul> <li>Access Key ID;</li> <li>Secret Key</li> <li>Region: ex: us-east-1</li> <li>Default output format</li> </ul> </li> <li>Test:       <pre><code>aws iam list-users\n</code></pre></li> </ul>"},{"location":"AWS/2.iam/#aws-cloudshell","title":"AWS CloudShell","text":"<p>AWS CloudShell is a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources using the AWS CLI and SDKs.</p> <ul> <li>Terminal in the cloud of AWS;</li> <li>Pre-authenticated with your console credentials;</li> <li>The environments also include the Python and Node runtimes;</li> <li>You can download files from CloudShell to your local machine, and also upload files from your local machine to CloudShell;</li> <li>The shell environment is based on Amazon Linux 2. You can store up to 1 GB of files per region in the home directory and they\u2019ll be available each time you open a shell in the region.</li> </ul> <p>Features:</p> <ul> <li>Timeouts &amp; Persistence \u2013 Each CloudShell session will timeout after 20 minutes or so of inactivity, and can be reestablished by refreshing the window:</li> <li>Persistent Storage \u2013 Files stored within $HOME persist between invocations of CloudShell with a limit of 1 GB per region; all other storage is ephemeral. This means that any software that is installed outside of $HOME will not persist, and that no matter what you change (or break), you can always begin anew with a fresh CloudShell environment.</li> <li>Network Access \u2013 Sessions can make outbound connections to the Internet, but do not allow any type of inbound connections. Sessions cannot currently connect to resources inside of private VPC subnets, but that\u2019s also on the near-term roadmap.</li> <li>Pricing \u2013 You can use up to 10 concurrent shells in each region at no charge. You only pay for other AWS resources you use with CloudShell to create and run your applications.</li> </ul>"},{"location":"AWS/2.iam/#iam-roles-for-services","title":"IAM Roles for Services","text":"<ul> <li>Some AWS services will need to perform actions on your behalf, on your account, for example:<ul> <li>EC2 instances need to be able to access S3 buckets;</li> <li>Lambda functions may need to access DynamoDB tables (to read/write data);</li> <li>And so on...</li> </ul> </li> <li>So we need to assign permissions to AWS service with IAM Roles</li> <li>IAM roles is just like users, but they are not intended to be used by physical people, but instead they will be used by AWS services.</li> </ul> <p>Role is a way to give AWS entities permissions to do stuff on AWS. As of now, we have 5 types of entities that can assume roles:</p> <ol> <li>AWS Service =&gt; AWS services like EC2, Lambda, or others to perform actions in your account.</li> <li>AWS Account =&gt; Entities in other AWS accounts belonging to you or a 3rd party to perform actions in your account.</li> <li>Web Identity =&gt; Entities/Users federated by the specified external web identity provider (Amazon, Facebook, Google, or any OpenID Connect (OIDC)compatible provider) to perform actions in your account.</li> <li>SAML 2.0 Federation =&gt; Users federated with SAML 2.0 from a corporate directory</li> <li>Custom Trust Policy =&gt; Custom trust policy to enable others to perform actions;</li> </ol>"},{"location":"AWS/2.iam/#example-ec2-instance-iam-access","title":"Example - EC2 Instance + IAM Access","text":"<p>The most common entity type for a Role is the <code>AWS Service</code>, which means that we're going to give permissions to an AWS service to perform actions in our account.</p> <p>Example: EC2 instance needs to access to read IAM users.</p> <p>IAM \u21d2 Roles \u21d2 Create Role</p> <ul> <li>Trusted entity \u21d2 AWS Service</li> <li>Use case - Service \u21d2 EC2</li> <li>Use case for the specified service =&gt; EC2</li> </ul> <p>And now we can give the permissions to which services the EC2 will have access to.</p> <ul> <li>Attach permissions policies \u21d2 <code>IAMReadOnlyAccess</code></li> <li>Role name \u21d2 <code>EC2-IAM-Read-Only</code></li> <li>Create role</li> </ul> <p>Trusted Entity:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n              \"Service\": \"ec2.amazonaws.com\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n      }\n  ]\n}\n</code></pre> <p>this policy is saying, this role can be assumed by the ec2 service in our account.</p> <p>Now, If you create an EC2 instance and assign this role to it, the EC2 instance will be able to read IAM users.</p>"},{"location":"AWS/2.iam/#iam-security-tools","title":"IAM Security Tools","text":"<ul> <li> <p>IAM Credentials Report (account-level)</p> <ul> <li>a report that lists all your account's users and the status of their various credentials.</li> </ul> </li> <li> <p>IAM Access Advisor (user-level)</p> <ul> <li>Shows the service permissions granted to a user and when those services were last accessed.</li> <li>You can use this information to revise your policies and see which permissions are not used and reduce the permission a user can get to be inline with the principle of the least privilege.</li> </ul> </li> </ul>"},{"location":"AWS/2.iam/#credentials-report","title":"Credentials Report","text":"<p>IAM =&gt; from the left hand side menu, click on <code>Credential Report</code></p> <p>Now we can download the report (csv format) and see the status of our users and their credentials.</p> <p>In this report you will find useful information's such as if the user has a password, access keys, MFA enabled, etc.</p>"},{"location":"AWS/2.iam/#access-advisor","title":"Access Advisor","text":"<p>IAM =&gt; Users =&gt; Select a user =&gt; Access Advisor</p> <p>In this tab we can see the services that the user has access to, the Policies grating permission to the user, and when was the last time the user accessed that service.</p>"},{"location":"AWS/2.iam/#iam-guidelines-best-practices","title":"IAM Guidelines &amp; Best Practices","text":"<ul> <li>Don't use the root account expect for AWS account setup</li> <li>One physical user = One AWS user</li> <li>Assign users to groups and assign permissions to groups</li> <li>Create a strong password policy</li> <li>Use and enforce the use of Multi Factor Authentication (MFA)</li> <li>Create and use Roles for giving permissions to AWS services</li> <li>Use Access Keys for Programmatic Access (CLI / SDK)</li> <li>Audit permission of your account with the IAM Credentials Report &amp; IAM Access Advisor</li> <li>Never share IAM users &amp; Access Keys</li> </ul>"},{"location":"AWS/3.budget-and-cost/","title":"Budget And Cost","text":""},{"location":"AWS/3.budget-and-cost/#activate-iam-access-to-billing-information","title":"Activate IAM Access to Billing Information","text":"<p>By default, IAM users and roles won't have access to Billing information. We need to enable a configuration using the root account to allow them to access the Billing and Cost Management console.</p> <p>Set permission:</p> <ul> <li>Log in as root user</li> <li>Go to the Account Dashboard</li> <li>Scroll down to \"IAM user and role access to Billing information\"</li> <li>Enable the checkbox \"Activate IAM Access\"</li> </ul> <p>Now the Admin user should be able to access the Billing and Cost Management console.</p> <p>Note: The Activate IAM Access setting alone doesn\u2019t grant IAM users and roles the necessary permissions for these console pages. In addition to activating IAM access, you must also attach the required IAM policies to those users or roles. If this setting is deactivated, IAM users and roles in this account can\u2019t access the Billing and Cost Management console pages, even if they have administrator access or the required IAM policies.</p>"},{"location":"AWS/3.budget-and-cost/#billing-overview","title":"Billing Overview","text":"<p>In the Bills section, you can find useful information about your billing such as the total amount of your bill, which services you are using, and how much you are spending on each service, in which region, and so on.</p> <p>You can also see the free tier usage of your services, so you can know how much you are using and how much you have left.</p>"},{"location":"AWS/3.budget-and-cost/#creating-budget-alarm","title":"Creating Budget Alarm","text":"<p>To get alerts about upcoming costs, the best thing to do is to create a budget alarm so that we can track our cost and get notified when we reach a certain threshold.</p>"},{"location":"AWS/3.budget-and-cost/#free-tier-zero-spend-budget","title":"Free Tier - Zero spend budget","text":"<p>Create a budget that notifies you once your spending exceeds $0.01 which is above the AWS Free Tier limits.</p> <p>Account =&gt; Budgets =&gt; Create budget</p> <ul> <li>Use a template (simplified)</li> <li>Template: Zero spend budget</li> <li>Budget name: My Zero-Spend Budget</li> <li>Add your email to the recipients list</li> <li>Create budget</li> </ul> <p>Now you will be notified via email when any spend is incurred.</p>"},{"location":"AWS/3.budget-and-cost/#monthly-cost-budget","title":"Monthly Cost Budget","text":"<ul> <li>Use a template (simplified)</li> <li>Template: Monthly cost budget</li> <li>Budget name: My Monthly Cost Budget</li> <li>Budgeted amount: $10</li> <li>Add your email to the recipients list</li> <li>Create budget</li> </ul> <p>You will be notified when:</p> <ol> <li>your actual spend reaches 85%</li> <li>your actual spend reaches 100%</li> <li>if your forecasted spend is expected to reach 100%.</li> </ol>"},{"location":"AWS/4.ec2/","title":"EC2 Fundamentals","text":""},{"location":"AWS/4.ec2/#overview","title":"Overview","text":"<ul> <li>EC2- Elastic Compute Cloud = Infrastructure as a Service</li> <li>It's composed of many things at a high level. It mainly consists in the capability of:<ul> <li>Renting virtual machines (EC2)</li> <li>Storing data on virtual drives (EBS)</li> <li>Distributing load across machines (ELB)</li> <li>Scaling the services using an auto-scaling group (ASG)</li> </ul> </li> </ul>"},{"location":"AWS/4.ec2/#ec2-sizing-configuration","title":"EC2 Sizing &amp; Configuration","text":"<ul> <li>Operating System (OS): Linux, Windows or MacOS</li> <li>How much compute power &amp; cores (CPU)</li> <li>How much random access memory (RAM)</li> <li>How much storage space:<ul> <li>Network attached (EBS &amp; EFS)</li> <li>Hardware attached (EC2 Instance Store)</li> </ul> </li> <li>Network card: speed of the card, public IP address</li> <li>Firewall rules (security groups)</li> <li>Bootstrap script (configure at first launch): EC2 User Data<ul> <li>Used to run commands at first launch and automate tasks such as installing updates, softwares, downloading common files from the internet,...</li> <li>It runs with the root user</li> <li>The more you add to the bootstrap script, the longer it will take for the EC2 instance to start</li> </ul> </li> </ul> <p>You can choose pretty much how you want your Virtual Machine to be and you can rent it from AWS.</p>"},{"location":"AWS/4.ec2/#launching-an-ec2-instance-with-a-web-server","title":"Launching an EC2 Instance with a Web Server","text":"<p>EC2 =&gt; Instances =&gt; Launch Instance</p> <ul> <li>Name: MyWebServer</li> <li>AMI: Amazon Linux 2023 AMI (Free tier elegible)</li> <li>Instance Type: t2.micro (Free tier elegible)</li> <li>Create a new key pair so you can SSH into your instance</li> <li>Network Settings:<ul> <li>Default VPC</li> <li>Subnet: No preference</li> <li>Auto-assign Public IP: Enable</li> <li>Security Group:<ul> <li>Create a new security group</li> <li>Security group name: MyWebServer-SG</li> <li>Description: Allow HTTP traffic</li> <li>Inbound rules:<ul> <li>Type: HTTP</li> <li>Source: Anywhere</li> </ul> </li> </ul> </li> </ul> </li> <li>Use default storage settings (just make sure Delete on Termination is enabled)</li> <li> <p>Advanced Details -&gt; User Data</p> <ul> <li>Add the following script:</li> </ul> <pre><code>#!/bin/bash\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"&lt;h1&gt;Hello World from $(hostname -f)&lt;/h1&gt;\" &gt; /var/www/html/index.html\n</code></pre> </li> <li> <p>Launch the instance</p> </li> </ul> <p>When you open the public IP address of your instance in your browser, you should see the following message:</p> <pre><code>&lt;h1&gt;Hello World from ip-172-31-22-123.ec2.internal&lt;/h1&gt;\n</code></pre> <p>Now, if we're not using the EC2 instance anymore, we can Stop it to avoid being charged for it. If we want to use it again, we can Start it again.</p> <ul> <li>If we decide to stop an instance then AWS will not bill you for it.</li> <li>The instance state is kept because you have a volume attached to it.</li> <li>You can get rid of the instance by terminating it. This will delete the instance and the volume attached to it.</li> <li>If you stop an instance and start it again, AWS will give you a new public IP address.</li> </ul>"},{"location":"AWS/4.ec2/#ec2-instance-types","title":"EC2 Instance Types","text":"<ul> <li>You can use different types of EC2 instances that are optimised for different use cases. see: https://aws.amazon.com/ec2/instance-types/</li> <li> <p>AWS has the following naming convention: <code>m5.2xlarge</code></p> <ul> <li>m: instance class</li> <li>5: generation (AWS improves them over time)</li> <li>2xlarge: size within the instance class, the more the size, the more the CPU and Memory you get</li> </ul> </li> <li> <p>Compare instances types: https://instances.vantage.sh/</p> </li> </ul>"},{"location":"AWS/4.ec2/#general-purpose","title":"General Purpose","text":"<ul> <li>These instances are ideal for applications that use these resources in equal proportions such as web servers and code repositories. </li> <li>Great for a diversity of workloads.</li> <li>Balance between compute, memory and networking resources</li> <li>Use Cases:<ul> <li>Websites and web applications, development environments, build servers, code repositories, micro services, test and staging environments, and line of business applications.  </li> </ul> </li> </ul> <p>Some examples of general purpose instances are: <code>t2.micro</code>, <code>t2.small</code>, <code>t2.medium</code>, <code>m5.large</code>, <code>m5.xlarge</code>, <code>m5.2xlarge</code>, <code>m5.4xlarge</code>, <code>m5.12xlarge</code>, <code>m5.24xlarge</code></p>"},{"location":"AWS/4.ec2/#compute-optimised","title":"Compute Optimised","text":"<ul> <li>Compute Optimized instances are ideal for compute bound applications that benefit from high performance processors.</li> <li>Great for compute-intensive tasks that require high performance processors</li> <li>Use Cases:<ul> <li>High performance web servers, scientific modelling, batch processing workloads, media transcoding, distributed analytics, high-performance computing (HPC), machine/deep learning inference, ad serving, dedicated gaming servers, and video encoding.</li> </ul> </li> </ul> <p>Some examples of compute optimised instances are: <code>c4.large</code>, <code>c5.large</code>, <code>c5.4xlarge</code>, <code>c5.metal</code>,...</p>"},{"location":"AWS/4.ec2/#memory-optimised","title":"Memory Optimised","text":"<ul> <li>Fast performance for workloads that process large data sets in memory (RAM).</li> <li>Use Cases:<ul> <li>Memory-intensive workloads such as open source databases, in-memory caches, and real-time big data analytics.</li> <li>High performance, relational and NoSQL databases, distributed web scale cache stores, in-memory databases optimized for BI (Business Intelligence), applications performing real time processing of big unstructured data.</li> </ul> </li> </ul> <p>Some examples of memory optimised instances are: <code>r5.large</code>, <code>r5.4xlarge</code>, <code>x1.16xlarge</code>, <code>x1e.32xlarge</code>,...</p>"},{"location":"AWS/4.ec2/#accelerated-computing","title":"Accelerated Computing","text":"<ul> <li>These instances use hardware accelerators, or co-processors, to perform some functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs.</li> <li>Use Cases:<ul> <li>Generative AI applications, including question answering, code generation, video and image generation, speech recognition, and more.</li> <li>Inferencing for ML models, including image and speech recognition, natural language processing, and recommendation engines.</li> </ul> </li> </ul> <p>Some examples of accelerated computing instances are: <code>p2.xlarge</code>, <code>p3.2xlarge</code>, <code>g3.4xlarge</code>, <code>g3.8xlarge</code>, <code>f1.2xlarge</code>,...</p>"},{"location":"AWS/4.ec2/#storage-optimised","title":"Storage Optimised","text":"<ul> <li>Great for storage-intensive task that require high, sequential read and write access to large data sets on local storage.</li> <li>They are optimized to deliver tens of thousands of low-latency, random I/O operations per second (IOPS) to applications.</li> <li>Use Cases:<ul> <li>High frequency online transaction processing (OLTP) systems, relational &amp; NoSQL databases, caching for in-memory databases (Redis, Memcached), Data warehousing applications, distributed file systems.</li> </ul> </li> </ul> <p>Some examples of storage optimised instances are: <code>h1.2xlarge</code>, <code>i3.large</code>, <code>i3.4xlarge</code>,...</p>"},{"location":"AWS/4.ec2/#hcp-optimised","title":"HCP Optimised","text":"<ul> <li>High performance computing (HPC) instances are purpose built to offer the best price performance for running HPC workloads at scale on AWS. HPC instances are ideal for applications that benefit from high-performance processors such as large, complex simulations and deep learning workloads.</li> <li>Use Cases:<ul> <li>computational fluid dynamics (CFD), weather forecasting, molecular dynamics, multiphysics simulations, finite element analysis (FEA) for crash simulations, seismic reservoir simulations, and structural simulations.</li> </ul> </li> </ul>"},{"location":"AWS/4.ec2/#security-groups","title":"Security Groups","text":"<ul> <li>A security group is a virtual firewall that controls inbound and outbound traffic for your EC2 instances.</li> <li>It controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.</li> <li>Fundamental of network security in AWS</li> <li>Only contain <code>allow</code> rules</li> <li>Rules can reference by IP or by security group</li> <li>They regulate:<ul> <li>Access to Ports</li> <li>Authorized IP ranges - IPv4 and IPv6</li> <li>Control of inbound network (from other to the instance)</li> <li>Control of outbound network (from the instance to the other)</li> </ul> </li> </ul> <p>Example:</p> Type Protocol Port Range Source SSH TCP 22 122.1499.196.85/32 HTTP TCP 80 0.0.0.0/0 Custom TCP Rule TCP 3000 sg-0a1b2c3d4e5f67890 <p></p> <p>Good to know:</p> <ul> <li>Can be attached to multiple instances</li> <li>An instance can have multiple security groups</li> <li>Locked down to a region/VPC combination, so if we change our vpc or region we need to create a new security group</li> <li>Live 'outside' the EC2 - if traffic is blocked the EC2 instance won't see it -- firewal outside the instance</li> <li>It's good to maintain one separate security group for SSH access</li> <li>If your application is not accessible (timeout), then it's a security group issue</li> <li>If your application gives a \"connection refused\" error, then it's an application error or it's not launched -- the SG actually worked, the traffic went through and the app was errored.</li> <li>By default, all inbound traffic is blocked and all outbound traffic is authorized</li> </ul> <p>Security Groups can reference other security groups. This is good because it means we don't have to maintain IP addresses in our security groups for our applications to communicate with each other.</p> <p>For example, if we have an EC2 instance (<code>EC2-1</code>) and it has a SG called <code>SG-1</code>, and this SG authorizes <code>SG-1</code> and <code>SG-2</code> inbounds. So if we launch another EC2 instance (<code>EC2-2</code>) and we attach <code>SG-2</code> to it, then <code>EC2-2</code> will be able to communicate with <code>EC2-1</code> in the port defined by the <code>SG-1</code> because <code>SG-2</code> is authorized in <code>SG-1</code>.</p> <p></p>"},{"location":"AWS/4.ec2/#classic-ports-to-know","title":"Classic Ports to Know","text":"<ul> <li>22 = SSH (Secure Shell) - Log into a Linux instance</li> <li>21 = FTP (File Transfer Protocol) - upload files into a file share</li> <li>22 = SFTP (Secure File Transfer Protocol) - upload files using SSH</li> <li>80 = HTTP - access unsecured websites</li> <li>443 = HTTPS - access secured websites</li> <li>3389 = RDP (Remote Desktop Protocol) - windows instance</li> </ul>"},{"location":"AWS/4.ec2/#adding-roles-to-ec2-instances","title":"Adding Roles to EC2 Instances","text":"<p>Roles are a secure way to grant permissions to instances to access AWS services.</p> <ul> <li>Launch a Web Server EC2 instance</li> <li>In the Security Group, allow inbound traffic on port 22 from your IP address so that you can SSH into the instance.</li> <li> <p>When the instance is launched, SSH into it:</p> <pre><code># Locate your private key file and Run this command, if necessary, to ensure your key is not publicly viewable.\nchmod 400 my-key.pem\n\n# Connect to your instance using its Public DNS:\nssh -i \"my-key.pem\" ec2-user@ec2-34-230-32-123.compute-1.amazonaws.com\n</code></pre> </li> <li> <p>Try to run the AWS command to list users: <pre><code>$ aws iam list-users\n\nUnable to locate credentials. You can configure credentials by running \"aws configure\".\n</code></pre></p> </li> <li> <p>You should never use <code>aws configure</code> on an EC2 instance, it's not secure because the credentials are stored in the instance. Instead, we can use Roles.</p> </li> <li> <p>Go to IAM =&gt; Roles =&gt; Create Role <code>EC2-IAM-Read-Only</code> and attach the <code>IAMReadOnlyAccess</code> policy to it.</p> </li> <li> <p>Go to EC2 =&gt; Select your Instance =&gt; Actions =&gt; Security =&gt; Modify IAM Role =&gt; Select the role you just created</p> </li> <li> <p>Now, if you run the same command, it should work.</p> </li> </ul> <pre><code>$ aws iam list-users\n\n{\n    \"Users\": [\n        {\n            \"Path\": \"/\",\n            \"UserName\": \"gabriel-brotas\",\n            \"UserId\": \"...\",\n            \"Arn\": \"...\",\n            \"CreateDate\": \"...\",\n            \"PasswordLastUsed\": \"...\"\n        }\n    ]\n}\n</code></pre>"},{"location":"AWS/4.ec2/#ec2-purchasing-options","title":"EC2 Purchasing Options","text":""},{"location":"AWS/4.ec2/#on-demand","title":"On-Demand","text":"<ul> <li>Pay for what you use:<ul> <li>Linux or Windows - billing per second, after the first minute</li> <li>All other Operating Systems - billing per hour</li> </ul> </li> <li>Has the highest cost but no upfront payment</li> <li>No long-term commitment</li> <li>Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave</li> </ul>"},{"location":"AWS/4.ec2/#reserved","title":"Reserved","text":"<ul> <li>Reserved Instances provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Up to 72% discount compared to On-demand</li> <li>Not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.</li> <li>You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS)</li> <li>Reservation Period - 1-year (+discount) or 3-year (+++discount).</li> <li> <p>Payment options:</p> <ul> <li> <p>All Upfront (+++): Full payment is made at the start of the term, with no other costs or additional hourly charges incurred for the remainder of the term, regardless of hours used.</p> </li> <li> <p>Partial Upfront (++): A portion of the cost must be paid upfront and the remaining hours in the term are billed at a discounted hourly rate, regardless of whether the Reserved Instance is being used.</p> </li> <li> <p>No Upfront (+): billed a discounted hourly rate for every hour within the term, regardless of whether the Reserved Instance is being used. No upfront payment is required. (contractual obligation to pay monthly for the entire term of the reservation. For this reason, a successful billing history is required before you can purchase No Upfront Reserved Instances.)</p> </li> </ul> </li> <li> <p>Reserved Instance's Scope - Regional or Zonal (reserve capacity in an AZ)</p> </li> <li>Recommended for steady-state usage applications (think database)</li> <li>You can buy and sell in the Reserved Instance Marketplace</li> <li> <p>Offering class, If your computing needs change, you might be able to modify or exchange your Reserved Instance, depending on the offering class:</p> <ul> <li> <p>Standard: provide the most significant discount, but can only be modified. Standard Reserved Instances can't be exchanged.</p> </li> <li> <p>Convertible: provide a lower discount than Standard Reserved Instances, but can change the EC2 instance type, family, OS, scop and tenancy.</p> </li> </ul> </li> </ul>"},{"location":"AWS/4.ec2/#savings-plans","title":"Savings Plans","text":"<ul> <li>Get a discount based on long-term usage (up to 72% - same as RI)</li> <li>Commit to a certain type of usage (e.g. $10/hour for a 1 or 3 year term)</li> <li>Usage beyond EC2 Savings Plans is billed at the On-Demand price</li> <li>Locked to a specific instance family &amp; AWS Region (e.g. M5 in us-east-1)</li> <li>Flexible across:<ul> <li>Instance Size (e.g., m5.xlarge, m5.2xlarge)</li> <li>OS (e.g. Linux, Windows)</li> <li>Tenancy (Host, Dedicated, Default)</li> </ul> </li> <li>Also applies to AWS Fargate and AWS Lambda usage.</li> </ul>"},{"location":"AWS/4.ec2/#spot-instances","title":"Spot Instances","text":"<ul> <li>Can get a discount of up to 90% compared to On-demand taking advantage of unused EC2 capacity</li> <li>Instances that you can \"lose\" at any point if your max price is less than the spot price</li> <li>The most cost-efficient instances in AWS</li> <li>Useful for workloads that are resilient to failure:<ul> <li>Batch Jobs</li> <li>Data Analysis</li> <li>Background processing</li> <li>Any distributed workload and optional tasks</li> <li>Workloads with a flexible start and end time</li> </ul> </li> <li>Not suitable for critical jobs or databases</li> </ul>"},{"location":"AWS/4.ec2/#dedicated-hosts","title":"Dedicated Hosts","text":"<ul> <li>Physical server with EC2 instance capacity fully dedicated to your use.</li> <li>Allows you address corporate compliance and regulatory requirements, and use your existing server-bound software license (per-socket, per-core, per-VM software licenses)</li> <li>Purchasing Options:<ul> <li>On-demand - pay per second for active Dedicated Host</li> <li>Reserved - 1 or 3 years</li> </ul> </li> <li>Most expensive option</li> <li>Useful for software that have complicated licensing model (BYOL - Bring Your Own License)</li> <li>Or for companies that have strong regulatory or compliance needs</li> </ul>"},{"location":"AWS/4.ec2/#dedicated-instances","title":"Dedicated Instances","text":"<ul> <li>Instances run on hardware that's dedicated to you -- EC2 instances that run on hardware that's dedicated to a single customer.</li> <li>Dedicated Instances might share hardware with other instances from the same AWS account that are not Dedicated Instances.</li> <li>Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level.</li> <li>No control over instance placement (can move hardware after stop/start)</li> </ul> <p>DH vs DI =&gt; Dedicated Instances means that you have your own instance on your own hardware, whereas dedicated host, you get access to the physical server itself and it gives you visibility into the lower level hardware.</p>"},{"location":"AWS/4.ec2/#capacity-reservations","title":"Capacity Reservations","text":"<ul> <li>Reserve On-Demand instances capacity in a specific AZ for any duration</li> <li>You always have access to EC2 capacity when you need it</li> <li>No time commitment (create/cancel anytime), no billing discounts</li> <li>The only purpose is to reverse capacity</li> <li>Combine with Regional Reserved Instances and Savings Plans to benefit from billing discounts</li> <li>You're charged at On-Demand rate wheter you run instances or not -- that means that your reserved capacity, you're going to be billed for it, and you know for sure that if you want to launch instances they're going to be available, but if you don't launch them, you're still going to get charged</li> <li>Suitable for short-term, uninterrupeted workloads that needs to be in a specific AZ</li> </ul>"},{"location":"AWS/4.ec2/#which-purchasing-option-is-right-for-me","title":"Which Purchasing Option is Right For Me?","text":"<ul> <li>On Demand: coming and staying in resort whenever we like, we pay the full price.</li> <li>Reserved: like planning ahead and if we plan to stay a long time, we may get a good discount.</li> <li>Savings Plans: pay a certain amount per hour for certain period and stay in any room type (e.g., King, Suite, Sea View,...,) -- for example, I know for sure I'm going to spend $300 per month every month for the next 12 months, and therefore, you may wannt change room type over time.</li> <li>Spot Instances: the hotel allows people to bid for the empty rooms and the highest bidded keeps the rooms. You can get kicked out at any time.</li> <li>Dedicated Host: we book an entire building of the resort.</li> <li>Capacity Reservations: you book a room for a period with full price even you don't stay in it.</li> </ul>"},{"location":"AWS/4.ec2/#ec2-spot-instances-spot-fleet","title":"EC2 Spot Instances &amp; Spot Fleet","text":"<ul> <li>Unused EC2 instance that is available for less than the On-Demand price</li> <li>The hourly price for a Spot Instance is called a Spot price</li> <li>Can be up to 90% cheaper than On-Demand instances. However, Spot Instances carry the risk of interruption</li> <li>Define max spot price and get the instance while current spot price &lt; max<ul> <li>The hourly spot price varies based on offer and capacity</li> <li>If the current spot price &gt; your max price, Amazon EC2 provides a Spot Instance interruption notice, which gives the instance a two-minute warning before Amazon EC2 interrupts it. You can't enable termination protection for Spot Instances.</li> </ul> </li> <li>Work well for big data, batch jobs, containerized workloads, CI/CD, stateless web servers, high performance computing (HPC), and rendering workloads that are resilient to failures.</li> <li>Not great for inflexible, stateful, fault-intolerant, tightly coupled between instance nodes, critical jobs or databases</li> </ul>"},{"location":"AWS/4.ec2/#how-spot-instances-work","title":"How Spot Instances Work","text":"<ul> <li> <p>Spot Request</p> <ul> <li>Maximum price</li> <li>Number of instances</li> <li>Launch specification (AMI, Instance Type, SG,...,)</li> <li>When your request is valid from and until, but it can be infinite</li> <li>Request Type:<ul> <li>One-time: As soon as your spot request is fulfilled, your instances are going to be launched and then your spot request will go away because it was a one-time request type.</li> <li>Persistent: it means that we want this number of instances to be valid as long as the spot request is valid from to valid until. And so, if somehow your instances do get stopped or interruped based on the spot price, then your spot request will go back into action and when things can be validated it will restart the spot instances for you. If you stop a spot instance in persistent mode and your spot request is sill active, your spot request automatically will be smart enough to restart and launch the instance for you</li> </ul> </li> </ul> </li> <li> <p>If you cancel a spot request, it's not going to terminate the instances that were launched by the spot request. It's just going to cancel the spot request itself. If you want to terminate the instances, you need to do it manually.</p> </li> </ul>"},{"location":"AWS/4.ec2/#spot-fleets","title":"Spot Fleets","text":"<ul> <li>Collection of Spot Instances and optionally On-Demand Instances</li> <li>The Spot Fleet will try to meet the target capacity with price constraints<ul> <li>Define possible launch pools: instance type (m5.large), OS, AZ...</li> <li>Can have multiple launch pools so that the fleet can choose</li> <li>Spot Fleet stops launching instances when reaching capacity or max cost</li> </ul> </li> <li>Spot Fleet maintains the target capacity by launching replacement instances when Spot Instances are terminated.</li> <li>Spot Fleet re-balances instances across availability zones if an AZ is experiencing a price spike.</li> <li>Spot instances need to be carefully managed, because they are terminated by Amazon at short notice when the market price goes about your bidding price</li> <li>Strategies to allocate Spot Instances:<ul> <li><code>lowestPrice</code>: from the pool with lowest price (cost optmization, short workload)</li> <li><code>diversified</code>: distributed across all pools (great for availability, long workload)</li> <li><code>capacityOptimized</code>: pool with the optimal capacity for the number of instances</li> <li><code>priceCapacityOptimized</code> (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads)</li> </ul> </li> <li>Spot Fleets allow us to automatically request Spot Instances with the lowest price</li> </ul>"},{"location":"AWS/4.ec2/#launching-a-spot-request","title":"Launching a Spot Request","text":""},{"location":"AWS/5.ec2-pt2/","title":"EC2 Fundamentals - Part 2","text":""},{"location":"AWS/5.ec2-pt2/#instances-private-vs-public-ip-vs-elastic-ip","title":"Instances Private vs Public IP vs Elastic IP","text":"<ul> <li>Public IP: <ul> <li>Can be accessed from the internet</li> <li>Is unique across the whole web</li> <li>The public IP will change every time you stop and start the instance</li> <li>If your instance is stopped, you will lose the public IP</li> </ul> </li> <li>Private IP:<ul> <li>Can only be accessed within the private network</li> <li>Is unique across the private network</li> <li>The private IP of your instance will not change during its lifetime, meaning that even if you stop and start the instance, it will keep the same private IP</li> <li>If your instance is stopped, you will not lose the private IP</li> </ul> </li> <li>Elastic IP:<ul> <li>Is a public IPv4 IP you own, and is yours until you release it.</li> <li>You can attach an Elastic IP to an instance, and detach it whenever you want</li> <li>AWS charges you a small hourly charge if the Elastic IP address is not associated with a running instance, or if it is associated with a stopped instance or an unattached network interface. While your instance is running, you are not charged for one Elastic IP address associated with the instance, but you are charged for any additional Elastic IP addresses associated with the instance.</li> <li>When associated to an Instance, you have to specify a private IP address to associate the Elastic IP with.</li> </ul> </li> </ul>"},{"location":"AWS/5.ec2-pt2/#placement-groups","title":"Placement Groups","text":"<p>To meet the needs of your workload, you can launch a group of interdependent EC2 instances into a placement group to influence their placement.</p> <ul> <li>Allows you to influence the placement of instances within the AWS infrastructure.</li> <li>That strategy can be defined using placement groups.</li> <li>When you create a placement group, you specify one of the following strategies for the group<ul> <li>Cluster <ul> <li>Packs instances close together inside an Availability Zone.</li> <li>This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of high-performance computing (HPC) applications.</li> <li>clusters instances into a low latency group in a single availability zone (high performance and high risk)</li> </ul> </li> <li>Spread - Spreads instances across underlying hardware (max 7 instances per group per AZ) - critical applications</li> <li>Partition - Spreads instances across many different partitions (which rely on different sets of racks/hardware) within an AZ. Scales to 100s of EC2 instances per group(Hadoop, Cassandra, Kafka)</li> </ul> </li> <li> <p>There is no charge for creating a placement group.</p> </li> <li> <p>Rack: A rack is a physical unit within an AWS data center that contains a group of servers, networking equipment, and other hardware components. These components within a rack share some common physical characteristics and resources.</p> </li> </ul>"},{"location":"AWS/5.ec2-pt2/#cluster","title":"Cluster","text":"<ul> <li>Logical grouping of instances within a single Availability Zone.</li> <li>All the EC2 instances are on the same rack, which means same hardware and it is in the same availability zone</li> <li>The EC2 instances are physically close together, which means low latency and high throughput</li> <li>Pros: Great network (10 Gbps bandwidth between instances), super low latency</li> <li>Cons: If the rack fails, all the instances fails at the same time</li> <li>use case:<ul> <li>Big Data job that needs to complete fast</li> <li>Application that needs extremely low latency and high network throughput and we're ok if there is a risk of simultaneous failure</li> </ul> </li> </ul>"},{"location":"AWS/5.ec2-pt2/#spread","title":"Spread","text":"<ul> <li>A spread placement group is a group of instances that are each placed on distinct hardware.</li> <li>Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other.</li> <li>All EC2 instances are going to be located on different hardware</li> <li>Pros:<ul> <li>Can span across multiple AZ</li> <li>Reduced risk of simultaneous failure</li> <li>EC2 Instances are on different physical hardware</li> </ul> </li> <li>Cons:<ul> <li>Limited to 7 instances per AZ per placement group</li> </ul> </li> <li>Use Case:<ul> <li>Application that needs to maximize high availability</li> <li>Critical applications where each instance must be isolated from failure from each other</li> </ul> </li> </ul>"},{"location":"AWS/5.ec2-pt2/#partition","title":"Partition","text":"<ul> <li>Partition placement groups help reduce the likelihood of correlated hardware failures for your application.</li> <li>No two partitions within a placement group share the same racks, allowing you to isolate the impact of hardware failure within your application.</li> <li>A partition placement group can have partitions in multiple Availability Zones in the same Region.</li> <li>Up to 7 partitions per AZ.</li> <li>Up to 100s of EC2 instances.</li> <li>The instances in a partition do not share racks with the instances in the other partitions -- isolated from failure.</li> <li>A partition failure can affect many EC2 but won't affect other partitions.</li> <li>EC2 instances get access to the partition information as metadata.</li> <li>Use cases:<ul> <li>large distributed and replicated workloads, such as HDFS, HBase, Cassandra, Kafka.</li> </ul> </li> </ul>"},{"location":"AWS/5.ec2-pt2/#elastic-network-interface-eni","title":"Elastic Network Interface (ENI)","text":"<ul> <li>Logical component in a VPC that represents a virtual network card</li> <li>You can attach to an EC2 instance in the same AZ.</li> <li>Each ENI lives within a particular subnet of the VPC (and hence within a particular Availability Zone)</li> <li>The ENI can have the following attributes:<ul> <li>Primary private IPv4, one or more secondary IPv4</li> <li>One Elastic IP (IPv4) per private IPv4</li> <li>One public IPv4</li> <li>One or more security groups</li> <li>A MAC address</li> </ul> </li> <li>You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover</li> <li>It provides networking capabilities to your EC2 instances, allowing them to communicate with other instances, services, and the internet.</li> <li>You can attach one or more ENIs to an EC2 instance.</li> <li>Each ENI is assigned a private IP address from the IP address range of your VPC, and it can also be associated with an Elastic IP address for public access.</li> <li>ENIs are associated with security groups and network access control lists (ACLs) to control inbound and outbound traffic.</li> <li>ENIs are often used to enhance the high availability and redundancy of applications by attaching them to instances in different Availability Zones or by moving them between instances.</li> <li>When an EC2 instance is terminated, the attached ENIs are automatically detached, and their IP addresses are released.</li> </ul>"},{"location":"AWS/5.ec2-pt2/#ec2-hibernate","title":"EC2 Hibernate","text":"<ul> <li> <p>We know we can stop, terminate instances:</p> <ul> <li>Stop - the data on disk (EBS) is kept intact in the next start</li> <li>Terminate - the data on disk (EBS) is destroyed</li> </ul> </li> <li> <p>On start, the following happens:</p> <ul> <li>First start: the OS boots &amp; the EC2 User Data script is run</li> <li>Following starts: the OS boots up</li> <li>Then your application starts, caches get warmed up, and that can take time because you're booting up from scratch</li> </ul> </li> </ul> <p></p> <ul> <li>Hibernate is a feature that enables you to launch instances from a hibernation snapshot.</li> <li>When you hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume. Amazon EC2 persists the instance's EBS root volume and any attached EBS data volumes. When your instance is started:<ul> <li>The EBS root volume is restored to its previous state</li> <li>The RAM contents are reloaded</li> <li>The processes that were previously running on the instance are resumed</li> <li>Previously attached data volumes are reattached and the instance retains its instance ID</li> </ul> </li> <li>The in-memory (RAM) state is preserved</li> <li>The instance boot is much faster (the OS is not stopped/restarted)</li> <li>Under the hood, the RAM state is written to a file in the root EBS volume.</li> <li>The root EBS volume must be encrypted.</li> <li>You are charged for storage of any EBS volumes, including storage for the RAM contents.</li> <li>Use cases:<ul> <li>Long-running processing.</li> <li>Saving the RAM starte.</li> <li>Services that take time to initialize</li> </ul> </li> <li>Good to Know:<ul> <li>Instance RAM size must be less than 150GB.</li> <li>Not supported for bare metal instances.</li> <li>Root volume must be EBS, encrypted, not instance store, and large.</li> <li>An instance can not be hibernated for more than 60 days.</li> </ul> </li> </ul>"},{"location":"AWS/6.ec2-storage/","title":"EC2 Storage &amp; AMI","text":""},{"location":"AWS/6.ec2-storage/#overview","title":"Overview","text":"<ul> <li>An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run.</li> <li>It allows your instances to persist data, even after their termination.</li> <li>They can only be mounted to one instance at a time.</li> <li>They are bound to a specific Availability Zone.<ul> <li>To move a volume across, you first need to snapshot it.</li> </ul> </li> <li>Free tier: 30GB of free EBS storage of type General Purpose (SSD) or Magnetic per month</li> <li>Analogy: Think of them as a \"network USB stick\" that you can take from a computer and put it into another one.</li> <li>It's a network drive (i.e. not a physical drive)<ul> <li>It uses the network to communicate the instance, which means there might be a bit of latency.</li> <li>It can be detached from an EC2 instance and attached to another one quickly.</li> </ul> </li> <li> <p>Have a provisioned capacity (size in GBs, and IOPS)</p> <ul> <li>You get billed for all the provisioned capacity, not the used capacity.</li> <li>You can increase the capacity of the drive over time.</li> </ul> </li> <li> <p>EC2 Delete on Termination attribute for EBS Volumes:</p> <ul> <li>By default, the root EBS volume of an instance will be deleted upon termination.</li> <li>But, for other volumes (non-root), the default is to keep the EBS volume.</li> <li>This can be controlled by the AWS console / AWS CLI</li> </ul> </li> </ul>"},{"location":"AWS/6.ec2-storage/#ebs-snapshots","title":"EBS Snapshots","text":"<ul> <li>Make a backup of your EBS volume at a point in time.</li> <li>Not necessary to detach volume to do snapshot, but recommended</li> <li>Can copy snapshots across availability zones or region</li> <li>Ex: we can create a snapshot of a volume in <code>us-east-1a</code> and then restore this volume from another region like <code>us-east-1b</code></li> </ul> <p>Features:     - EBS Snapshot Archive         - Move a Snapshot to an \"archive tier\" that is 75% cheaper.         - Takes within 24 to 72 hours for restoring the archive.     - Recycle Bin for EBS Snapshots         - Setup rules to retain deleted snapshots so you can recover them after an accidental deletion.         - Specify retention (from 1 day to 1 year)     - Fast Snapshot Restore (FST)         - Allows you to get your volumes restored in a few seconds.         - Force full initialization of snapshot to have no latency on the first use ($$$)</p>"},{"location":"AWS/6.ec2-storage/#ami-amazon-machine-image","title":"AMI - Amazon Machine Image","text":"<ul> <li>AMI are customization of an EC2 instance<ul> <li>You add your own software, configuration, operating system, monitoring...</li> <li>Faster boot / configuration time because all your software is pre-packaged</li> </ul> </li> <li>AMI are built for a specific region (and can be copied across regions)</li> <li>You can launch EC2 instances from:<ul> <li>A Public AMI: AWS provided</li> <li>Your own AMI: you make and maintain them yourself</li> <li>An AWS Marketplace AMI: an AMI someone else made and published</li> </ul> </li> </ul>"},{"location":"AWS/6.ec2-storage/#creating-an-ami","title":"Creating an AMI","text":""},{"location":"Architecture/architectural-features/","title":"Architectural Features","text":""},{"location":"Architecture/architectural-features/#operational","title":"Operational","text":"<ul> <li>Availability;<ul> <li>Will it be online 24/7? What level of availability do I want in my system? Should it be available in other countries?</li> </ul> </li> <li>Disaster recovery;<ul> <li>How will I recover when my system is down? What if an AWS region goes down? Do I need multi-cloud?</li> <li>Processes must be in place for these cases;</li> </ul> </li> <li>Performance;<ul> <li>How much performance do I want in this system? 5000 requests per second? It will affect the choice of the database, replicas, ...</li> </ul> </li> <li>Recovery (backup);<ul> <li>Test the backup periodically to make sure it's working.</li> </ul> </li> <li>Reliability and security;<ul> <li>Prevent brute force attacks, password policies, captcha, ...</li> </ul> </li> <li>Robustness;<ul> <li>Can it scale? Can it change region if one goes down? Do you have the necessary amount of IP, CIDR?</li> </ul> </li> <li>Scalability;</li> </ul>"},{"location":"Architecture/architectural-features/#structural","title":"Structural","text":"<p>Characteristics of my software for it to function in a flexible way;</p> <ul> <li>Configurability<ul> <li>Use environment variables, abstractions, there should be no need to change the source code to deploy in different environments (dev, stage, prod).</li> </ul> </li> <li>Extensibility<ul> <li>It should be able to grow in a way that things can be plugged into it, for example: Payment Gateway, changing from gateway x to y should be simple, work with interfaces, abstractions. The application should not depend on vendors (databases, gateways, modules, etc.).</li> </ul> </li> <li>Easy installation<ul> <li>Standardization of the environment, Infrastructure as a Code, docker compose for development.</li> </ul> </li> <li>Component reuse<ul> <li>Attach libraries so that all services can use them, for example: npm packages.</li> </ul> </li> <li>Internationalization<ul> <li>For example, if the payment gateway changes, how will the conversion be handled? How about installment payments, etc.?</li> </ul> </li> <li>Easy maintenance<ul> <li>Make the software simple, SOLID, understand the layers of the system. Fixing bugs and adding new features should become easy.</li> </ul> </li> <li>Easy support (logs, debugging)<ul> <li>Standardization in log generation, observability, etc.</li> </ul> </li> </ul>"},{"location":"Architecture/architectural-features/#cross-cutting","title":"Cross-Cutting","text":"<ul> <li>Accessibility<ul> <li>Is it easy for others to access the site? What about people with disabilities? Use appropriate labels, lightweight images, etc.</li> </ul> </li> <li>Data retention and recovery process (how long will the data be kept)<ul> <li>Storage is expensive. Do the data you have today really need to exist for 7, 30, or 60 days? Old data can be kept in different locations after a certain time to make the database cheaper.</li> </ul> </li> <li>Authentication and Authorization<ul> <li>In a distributed architecture, how will it work? Use Keycloak? API Gateway?</li> </ul> </li> <li>Legal<ul> <li>How long will the data be kept? Will it be encrypted?</li> </ul> </li> <li>Privacy<ul> <li>How can you minimize problems related to user data leakage under LGPD? Avoid developers having access to the production database. Separate sensitive information into separate databases. Have a database with fake data for development?</li> </ul> </li> <li>Security<ul> <li>Use web firewall and work with mechanisms that can identify robots, etc.</li> </ul> </li> <li>Usability<ul> <li>How is the API organized? Is there documentation? Is there a readme? Who is my client?</li> </ul> </li> </ul>"},{"location":"Architecture/design-quality-software/","title":"Designing Quality Software","text":""},{"location":"Architecture/design-quality-software/#performance","title":"Performance","text":"<p>When it comes to developing software, performance is an essential factor. It's the software's ability to complete a specific workload. But how can we determine how well a software performs a particular action? To measure performance, we need data. The primary units of measurement for evaluating software performance are latency and throughput.</p> <p>The units of measurement for evaluating software performance are:</p> <ul> <li>Latency or \"response time\" \u21d2 Time it takes for a software application to process a request and complete a workload.</li> <li>Throughput \u21d2 Number of requests a software application can handle.</li> </ul> <p>Having a performative software is different from having a scalable software</p> <p>Objective:</p> <ul> <li>Reduce latency, usually measured in milliseconds; It is affected by the application's processing time, network, and external calls;</li> <li>Increase throughput, allow the software to handle more requests; It is directly linked to latency;</li> </ul> <p>Main reasons for low performance:</p> <ul> <li>Inefficient processing;<ul> <li>Ex: Algorithms, the way the application is handling data, poorly made queries, etc.</li> </ul> </li> <li>Limited computational resources;<ul> <li>CPU, RAM, Memory,...</li> </ul> </li> <li>Working in a blocking way;<ul> <li>Every time you make a request and if that request blocks the application to deal specifically with that request, the application will decrease throughput because it will not be able to handle thousands of requests in parallel.</li> <li>Separate each request into a thread.</li> </ul> </li> <li>Serial access to resources;<ul> <li>Every time you access an API you wait for it to finish and call the next one, one after the other, this decreases throughput.</li> </ul> </li> </ul> <p>Main ways to increase efficiency:</p> <ul> <li>Scale of computational capacity (CPU, Disk, Memory, Network);</li> <li>Logic behind the software (Algorithms, queries, framework overhead);</li> <li>Concurrency and parallelism; Dealing with multiple processes at the same time;</li> <li>Databases (types of databases, schema);</li> <li>Caching;</li> </ul> <p>Computational Capacity: Vertical Scale vs Horizontal Scale</p> <ul> <li> <p>Vertical Scale \u21d2 Increase the computational capacity of the machine, so that the application can receive more requests;</p> </li> <li> <p>Horizontal Scale \u21d2 Increase the number of machines by placing a load balancer in front to balance the loads;</p> </li> </ul> <p>Concurrency and Parallelism</p> <p>\"Concurrency is about dealing with many things at once. Parallelism is about doing many things at once.\"</p> <ul> <li> <p>Concurrency \u21d2 Deals with several things but they are not necessarily executed at the same time; When two or more tasks can start to be executed and finish in overlapping time periods, not meaning that they need to be in execution necessarily at the same time.</p> <p>That is, you have concurrency when:</p> <ul> <li> <p>More than one task progresses at the same time in an environment with multiple CPUs/cores;</p> </li> <li> <p>Or in the case of a single-core environment, two or more tasks may not progress at the exact same moment, but more than one task is processed in the same time interval, not waiting for one task to complete before starting another.</p> </li> </ul> </li> <li> <p>Parallelism \u21d2 Perform actions simultaneously. It happens when two or more tasks are executed, literally, at the same time. Obviously requires a processor with multiple cores, or multiple processors so that more than one process or thread can be executed simultaneously.</p> </li> </ul> <p>Imagining a web server with a worker:</p> <p>Serial way - Working in a serial way - single process - Serving 5 requests</p> <p>If the worker operates in serial mode, it will attend to one request at a time.</p> <pre><code>10ms -&gt; 10ms -&gt; 10ms -&gt; 10ms -&gt; 10ms \n------------------------------------\n              50ms\n</code></pre> <p>Concurrent/parallel way - 5 threads serving 1 request each</p> <p>If it operates in parallel mode, it can handle multiple requests simultaneously, which improves its performance.</p> <pre><code>10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n--------\n  10ms\n</code></pre>"},{"location":"Architecture/design-quality-software/#caching","title":"Caching","text":"<p>Fetching things that have been processed before and returning them more quickly to the end user.</p> <ul> <li> <p>Edge caching / Edge computing</p> <p>The user does not hit the machine (ec2, ecs, lambda,...) because this type of cache will cache the data on an edge that is before the main server. Examples of services: Cloudflare, Cloudfront.</p> <p>Cloudfront caches static files (images, css, js, ...) in an availability zone and users who are close to them will receive the data almost instantly.</p> <p>Types of edge cache:</p> <ul> <li>Static data;</li> <li>Web pages;</li> <li>Internal functions;<ul> <li>Avoid reprocessing heavy algorithms by setting a TTL of 30min, for example;</li> <li>Access to the database (avoid unnecessary I/O).</li> </ul> </li> <li>Objects;</li> </ul> <p>Problem: Netflix has millions of accesses to its web application, if the data center is in the United States, users from all over the world will have to travel from their location to the USA to get these terabytes of data in order to access the site, congesting Netflix's network and the internet in general and generating a lot of latency to access content.</p> <p>Solution: Edge computing:  caching this data in the regions where your users are located, avoiding users having to travel across the entire internet to hit the data center. It will give a better experience to the user (low latency) and we will pay less because we will have fewer requests on our servers.</p> <ul> <li>Cache performed closer to the user. The user does not need to hit the USA to get an image.</li> <li>Avoids the request reaching the Cloud Provider / Infra.</li> <li>Usually static files. (Cheaper, faster)</li> <li>CDN - Content Delivery Network; (Network of servers, spreads your content to other data centers), e.g: Akamai's CDN \u2192 More than 500 points in Brazil, spreads your content (videos) to the regions of Brazil.     If you're in Portugal, your video should be loaded from a CDN in Portugal.     Cost:<ul> <li>CDN cost</li> <li>Distribution cost: If Akamai does not have the video/image on its CDN, it will fetch the content from the server (e.g: us-east-1), download it and make it available to you. Once it has been downloaded, it will perform something called Midgress, which means that it will now take that content and distribute it among strategic points, and this content spreading consumes bandwidth, which you pay for to spread across the edges. Origin \u2192 CDN \u2192 midgress \u2192...</li> </ul> </li> <li>Cloudflare workers \u21d2 Edge computing platform</li> <li>Vercel</li> <li>Akamai</li> </ul> </li> </ul>"},{"location":"Architecture/design-quality-software/#scalability","title":"Scalability","text":"<p>Scalability is the ability of systems to support an increase (or decrease) in workloads by incrementing (or reducing) the cost in equal or lesser proportion.</p> <p>While performance focuses on reducing latency and increasing throughput, scalability aims to have the possibility of increasing or decreasing throughput by adding or removing computational capacity.</p> <p>Vertical Scaling - Horizontal Scaling</p> <p>Scaling Software - Decentralization</p> <p>Machines are disposable, so your system must be independent of the machine being used.</p> <ul> <li>Ephemeral Disk \u2192 Everything you save on disk can be deleted when needed. We need the power to kill the machine when we want to.</li> <li>Application Server vs Assets Server \u2192 Server has the application code, assets are on an assets server (s3 bucket).</li> <li>Centralized Cache \u2192 The cache is not on the machine itself, it is on a server specific for caching. ex: DynamoDB.</li> <li>Centralized Sessions \u2192 The software cannot store state.</li> <li>Upload / File Writing \u2192 Bucket / file server</li> </ul> <p>Everything can be easily destroyed and created.</p> <p>Scaling Databases</p> <ul> <li>Increasing computational resources; More disk, memory, cpu,...;</li> <li>Distributing responsibilities (write vs read); Create replicas;</li> <li>Sharding horizontally; Create multiple read machines;</li> <li>Query and index optimization;</li> <li>Serverless / Cloud databases; (Dynamo, Aurora, Fauna,...);</li> </ul> <p>Use an APM (Application performance monitoring) system to identify bottlenecks in queries and problems that are occurring.</p> <ul> <li>Explain in queries \u2192 identify what is happening in queries</li> <li>CQRS (Command Query Responsibility Segregation)</li> </ul> <p>Reverse Proxy</p> <p>Proxy = Proxy, a person who speaks on your behalf</p> <p>Redirects users;</p> <p>A proxy is a service that routes traffic between the client and another system. It can regulate traffic according to present policies, enforce security, and block unknown IPs.</p> <p>A reverse proxy is a server that sits in front of all servers, it has rules, and these rules cause it to forward your request to certain servers that can resolve it. Unlike a normal proxy, which sits on the client side, the reverse proxy is designed to protect the servers. It accepts the client's request and forwards the request to one or more servers and returns the processed result. The client communicates directly with the reverse proxy and does not know about the server that processed it.</p> <p>Reverse Proxy Solution:</p> <ul> <li>Nginx</li> <li>HAProxy (HA = High Availability)</li> <li>Traefik</li> </ul>"},{"location":"Architecture/design-quality-software/#resilience","title":"Resilience","text":"<p>It is a set of intentionally adopted strategies for adapting a system when a failure occurs.</p> <p>In software, if an error occurs, it either throws an exception or has a strategy to serve the client request, even if partially, despite the error.</p> <p>Resilience is the power to adapt if something happens.</p> <p>How can I register a user even if the Correios API is not working to get the ZIP code? How can I get that sale if my payment service is not working?</p> <p>The only thing we are certain of is that our software will fail at some point, but we must have resilience to deal with these failures.</p> <p>Having resilience strategies allows us to minimize the risks of data loss and important business transactions.</p>"},{"location":"Architecture/design-quality-software/#resilience-strategies","title":"Resilience Strategies","text":"<ul> <li> <p>Protect and be Protected</p> <p>It is very common today for an application to be based on an ecosystem with several other services. A system in a distributed architecture needs to adopt self-preservation mechanisms to ensure its operation with the highest possible quality.</p> <p>A system cannot be \"selfish\" to the point of making more requests on a system that is failing.</p> <p>Example:</p> <p>System A sending a request to know the price table for System B, but System B does not respond, and System A keeps sending several requests in succession, eventually System B will go offline.</p> <p>So, a system cannot keep sending multiple requests to another until it crashes, it is no use kicking a dead dog.</p> <p>A slow system online is often worse than a system offline. (Domino effect).</p> <p>Example:</p> <p>System A calls System B that calls System C, if System C is slow, it will delay B's response, which will consequently delay A's response, and if more requests are coming, it will end up crashing some service and eventually maybe all.</p> <p>So, sometimes if a system is not handling requests, it is better to return 500 for everyone until it stabilizes again.</p> <p>One form of protection is to identify that you cannot handle requests or that the other service is no longer able to handle them or taking too long to respond.</p> </li> <li> <p>Health Check</p> <p>Check the application's health;</p> <p>\"Without vital signs, it is not possible to know the health of a system\".</p> <p>Important to know if it is able to receive requests or not, ..., if it is bad, it already returns a 500 error for the other applications to know if it is unavailable.</p> <p>An unhealthy system has a chance to recover if traffic stops being directed to it temporarily.</p> <p>Self-healing, give the server time to self-recover, stop sending requests to it.</p> <p>Quality Health Check, usually people only put a /health route to check if the application is healthy, but this cannot measure if the application is really healthy because in the other routes, there is usually processing, database query, data formatting, etc.</p> </li> <li> <p>Rate Limiting</p> <p>Protects the system based on what it was designed to support.</p> <p>To know this limit, we can do stress testing and/or the company's budget to know how much traffic it can handle.</p> <p>Example: 100 requests per second, \"it's as much as I can handle\", returns a 500 error if exceeded.</p> <p>We can also work with priority, reserve 60 requests for an important client and 40 for the rest.</p> </li> <li> <p>Circuit breaker</p> <p>It protects the system by denying requests made to it. Example: 500</p> <p>If the API is having problems, the circuit opens the \"switch\" and no longer allows requests to pass through.</p> <p>Closed Circuit = Requests arrive normally;</p> <p>Open Circuit = Requests do not reach the system. Instant error to the client;</p> <p>Half-open = Allows a limited number of requests to verify if the system can fully come back online.</p> </li> <li> <p>API Gateway</p> <p>Centralizes the receipt of all application requests, all requests pass through it and it applies the policy/validation rules before forwarding the request to the resolver.</p> <p>It understands the individual needs of each application.</p> <p>Ensures that \"inappropriate\" requests reach the system:</p> <p>Example: Unauthenticated user.</p> <p>We take the authentication responsibility away from the application and pass it to the API Gateway, just by doing that we already reduce the resource expense to check whether the user is authenticated or not.</p> <p>It's like living in a condominium and the person can only enter the apartment if they go through the gatehouse (API Gateway) first.</p> <p>Implements policies such as Rate Limiting, Health check, etc...</p> <p>Example of services: Kong</p> </li> <li> <p>Service mesh</p> <p>Controls network traffic.</p> <p>Instead of services communicating directly with each other, they hit a proxy called a sidecar and that sidecar sends the request to the sidecar of another system.</p> <pre><code>[ System A ] (Sidecar) ----&gt; (Sidecar) [System B]\n</code></pre> <p>All network communication is performed via proxy, so everything that is passing on the network can be controlled.</p> <p>Avoids protection implementations by the system itself.</p> <p>You can analyze everything that is happening on the network, all traffic, and with that you can define the rules for Rate Limiter, Circuit Breaker, etc...</p> <p>mTLS \u2192 encrypt the network to ensure that Service A is itself and B is itself.</p> </li> <li> <p>Asynchronous communication</p> <p>With less computational resources, it can handle more computational resources than it could otherwise.</p> <p>It does not need to deliver the response to the requests immediately.</p> <p>Avoids data loss;</p> <p>There is no data loss in sending a transaction if the server is down;</p> <p>Example: synchronously, if we make a payment and the server is not available, it will tell us to try again later, however, asynchronously allows us to send the message/information to an intermediary (Redis, RabbitMQ, AWS SNS, Kafka, ...) to handle the action because the other end does not need the response at that moment.</p> <p>The server can process the transaction in its own time when it is online;</p> </li> <li> <p>Delivery guarantees with Retry</p> <p>When we want resilience in the request, we need to make sure that the message we are sending is reaching its destination, but not always the message can reach its destination because the other system may be slow, offline, ... and because of that one of the ways to have resilience and minimize this problem is to have Retry policies, send a message, if the system does not respond, send another, and another, ...</p> <p>But we have a problem, if 10 services send a request and the server cannot handle 10 simultaneous requests, even if the other servers have retry policies and keep sending messages every 2 seconds, it will not help because they are all trying to access the server at the same time and will always return an error.</p> <p>To do this, we have the Exponential backoff - Jitter, where we wait exponentially 2s, 4s, 8s, 16s, etc., to give the server more time to recover, and with Jitter, it has an algorithm that adds random noise to requests so that they do not happen simultaneously. For example, (2.1s, 2.5s, 2.0s), (4.7s, 4.01s, 4.12s), etc., so that even if services are sending requests simultaneously, the noise will make them arrive differently.</p> </li> <li> <p>Delivery guarantees with Kafka</p> </li> <li> <p>Complex situations</p> <ul> <li>What happens if the message broker goes down? (Kafka, RabbitMQ, SNS, etc.)</li> <li>Will messages be lost?</li> <li>Will your system be down?</li> <li>How to ensure resilience in unusual situations?</li> </ul> <p>There are always some Single Points of Failure (SPF).</p> <p>For example, if all the resilience you rely on is in Apache Kafka, it means that your SPF is in Apache Kafka.</p> <p>How can we avoid this, so that if Kafka goes down, the system does not lose information?</p> <p>All of this involves risk management, and the more resilience we guarantee, the more expensive it will be for the company.</p> <p>If AWS goes down? Is it worth working with multi-cloud?</p> <p>There will always be a limit to your resilience, and the more resilience, the more effort and cost.</p> <p>The responsibility for defining the level of resilience of the system lies with the CTO / CEO, who defines the risk that it will generate for the business.</p> </li> </ul>"},{"location":"Architecture/sustainability/","title":"Sustainability","text":"<ul> <li>Developing software is expensive;</li> <li>Software solves a \"pain\";</li> <li>Software needs to pay for itself over time;<ul> <li>The company should profit more because of that software;</li> <li>The software, along with the company, is an organism, as this evolution occurs, the software must be able to sustain itself in a way that the cost is lower than the result it is bringing to the company;</li> </ul> </li> <li>Long-term thinking;<ul> <li>It is very common for a software to reach a point where the developers are demotivated to work and want to start everything from scratch, but often the software hasn't even paid for itself and has to start over, in this way the software will never be able to reach the breakeven point, balance between cost vs what it generates.</li> </ul> </li> <li>Follow the evolution of the business;<ul> <li>For example: imagine if Nubank goes through a refactoring process and customers have to wait for the bugs to be fixed before they can use the product again, this is unfeasible for the business, the software must be built with a long-term perspective and always following the evolution of the business.</li> </ul> </li> <li>The longer the software stays up, the more return it generates;</li> <li>The solution needs to be architected;</li> </ul>"},{"location":"Architecture/sustainability/#software-architecture-pillars","title":"Software Architecture Pillars","text":"<ul> <li>Structuring     -Easy evolution and componentization to meet business objectives;</li> <li>Componentization;</li> <li>Relationship between systems; (External communication, third-party services, networks)</li> <li>Governance; (Standardization, rules, documentation, protocols, ...)<ul> <li>A software must be independent of the developer. If a person leaves, it should be easy for someone else to take on that role and continue the project in the same way it was.</li> </ul> </li> </ul>"},{"location":"Architecture/sustainability/#architectural-requirements","title":"Architectural Requirements","text":"<ul> <li>Performance<ul> <li>Example: Handling 500 requests per second, resilience, etc.</li> </ul> </li> <li>Data storage<ul> <li>Example: Data centers in Brazil, Europe, etc.?</li> </ul> </li> <li>Scalability<ul> <li>Horizontally, vertically, load balancer, etc.</li> </ul> </li> <li>Security<ul> <li>SSL, Rate Limiter, Mutual TLS, etc.</li> </ul> </li> <li>Legal<ul> <li>Comply with the legislation of each country (LGPD, Terms of Use, Retention Period for Data, etc.);</li> </ul> </li> <li>Marketing<ul> <li>Track where each request comes from, etc.</li> </ul> </li> </ul>"},{"location":"Architecture/types-of-architecture/","title":"Types of Architecture","text":"<ul> <li>Technological Architecture</li> <li>Enterprise/Corporate Architecture</li> <li>Solution Architecture</li> <li>Software Architecture</li> </ul>"},{"location":"Architecture/types-of-architecture/#technological-architecture","title":"Technological Architecture","text":"<p>This type of architecture focuses on specific technologies used in the market. A technology architect has in-depth knowledge of a particular tool or technology and can generate value based on their expertise. They are well-versed in the workings of a particular technology, and are able to recommend and implement solutions that leverage that technology. Examples of technology architects include Elastic architects (for Kibana and Elastic Search), Java architects, SQL Server architects, SAP architects...</p> <ul> <li>Specialization in specific market technologies, has a great knowledge of a technology/tool.</li> <li>Generate value based on specialties;</li> <li>In-depth knowledge of the technology;</li> </ul>"},{"location":"Architecture/types-of-architecture/#enterprise-architecture","title":"Enterprise Architecture","text":"<p>Enterprise architecture involves creating policies and rules that strategically impact the organization as a whole. In large companies with thousands of employees, the number of technologies that can be utilized may be endless. An enterprise architect brings solid governance to the infrastructure and technology that will be used, with a focus on evaluating costs that make sense for the company's growth. They evaluate licenses, assess new technologies, standardize technologies, plan large-scale implementations, and oversee core systems and migrations (such as from monolithic to microservices).</p> <ul> <li>Policies and rules that strategically impact the organization as a whole,</li> <li>The number of technologies that can be used can reach N, this architect brings solid governance regarding the infrastructure/technology that will be used with an evaluation of costs that make sense for the company to grow.</li> <li>Evaluation of licenses;</li> <li>Evaluation of possible new technologies;</li> <li>Standardization of technologies;</li> <li>Planning for large deployments (SAP, Salesforce,...)</li> <li>Core systems, migrations (Monolithic, Microservices, ...)</li> </ul>"},{"location":"Architecture/types-of-architecture/#solutions-architecture","title":"Solutions Architecture","text":"<p>Solution architecture sits at the intersection of business and software, translating business requirements into software solutions. Solution architects create architectural designs that map out how the software will function. They use tools like the C4 diagram, UML, and BPMN to analyze the commercial impact of technology choices, assess the company's context and team, and make informed decisions. For example, they may assess whether it makes sense for a company to switch from AWS to Google Cloud Platform, or whether migrating from Oracle to SQL or Postgres would be cost-effective. Solution architects take a short, medium, and long-term view of the software, and may be involved in pre-sales and sales processes to better understand the business and its needs. They also analyze the cost impact of implementing solutions, bringing predictability to the project.</p> <ul> <li>Between the business and software area, transforms business requirements into software solutions;</li> <li>Architectural designs of the software solution to reproduce how it will work;</li> <li>C4 diagram, UML, BPMN;</li> <li>Analyzes commercial impacts regarding a technology choice, analyzes the company's context and team to be able to make that decision (ex: if the company is using AWS, does it make sense to switch to Google just because it's the technology your team knows? Is there a financial advantage in this?, Oracle database, does it make sense to switch to SQL, Postgres,...?);</li> <li>Short, medium, long-term thinking;</li> <li>May participate in the pre-sales and sales process, usually accompanies the client to better understand the business;</li> <li>Analyzes the cost impacts for the business, how much does it cost to implement this CRM? brings predictability</li> <li>High-level vision (macro);</li> </ul>"},{"location":"Architecture/types-of-architecture/#software-architecture","title":"Software Architecture","text":"<p>Software architecture is a discipline of software engineering that is directly tied to the software development process. It has a significant impact on the organizational structure of a company, and involves forming teams and structuring software components. Software architects must consider how to develop the software in components that best meet the business objectives. They must also take into account any restrictions or limitations, such as budget constraints, team capabilities, or technological requirements. Software architecture is the fundamental organization of a system and its components, as well as the principles that guide its design and evolution. A software architect must build a software system with a long-term perspective, and must ensure that the software delivers value to the company. They focus on the micro-level details of design, patterns, tests, implementation, clean architecture, code reviews, and requirements.</p> <ul> <li>Discipline of software engineering, directly linked to the software development process;</li> <li>Directly affects the organizational structure of the company;</li> <li>Formation of teams, structure of software components;</li> <li>How can I develop this software in components? How can these components best meet business objectives? every business has constraints, so I have to adapt my components to these constraints (financial, team, technological, etc.).</li> <li>It is the fundamental organization of a system and its components, their relationships, their environment, as well as the principles that guide its design and evolution.</li> <li>Software must be built with long-term thinking.</li> <li>The role of the software architect is to make the software return value to the company.</li> <li>Micro vision (design, patterns, tests, implementations, clean architecture, code reviews, requirements,....)</li> </ul>"},{"location":"Architecture/types-of-architecture/#role-of-the-software-architect","title":"Role of the Software Architect","text":"<ul> <li>Translate business requirements into architectural patterns;</li> <li>Orchestrate developers and domain experts;</li> <li>Assist in decision-making in times of crisis;</li> <li>Reinforce good development practices.</li> <li>Have a deep understanding of architectural concepts and models;</li> </ul> <p>It is common for us to want to solve a new problem in the same way we solved past ones. For example, if we always used a monolithic pattern, it is very likely that the next challenge will also be a monolithic application. The more the software architect understands architectural models, the greater their range of possibilities to understand and solve the challenge in the best possible way.</p>"},{"location":"Architecture/types-of-architecture/#why-learn-software-architecture","title":"Why learn Software Architecture?","text":"<ul> <li>Ability to navigate from the macro to the micro view of one or more software;</li> <li>Understand the various options we have to develop the same thing and choose the best solution for a given context;</li> <li>Think long-term about the project and its sustainability (Deadlines, People);</li> <li>Make decisions in a more cold and calculated way, thus avoiding being influenced by market \"hypes\";</li> <li>Dive into design patterns and development and their best practices;</li> <li>Have a clearer understanding of the impact that software has on the organization as a whole;</li> <li>Make decisions with more confidence.</li> </ul>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/","title":"Object-Oriented Programming vs Functional Programming","text":"<p>Functional Programming or Object Oriented Programming its more of a personal preference than being a question of which is better than the other.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#functional-programming","title":"Functional Programming","text":"<p>Functional programming is the form of programming that attempts to avoid changing state and mutable data. In a functional program, the output of a function should always be the same, given the same exact inputs to the function.</p> <p>ex:</p> <p>It will always be the same result, and has no side effects </p> <pre><code>function sum(a, b) {\n    return a + b\n}\n</code></pre> <p>This is because the outputs of a function in functional programming purely rely on the function's arguments, and there is no magic behind the scenes. This is called eliminating side effects in your code.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#oop-object-oriented-programming","title":"OOP (Object Oriented Programming)","text":"<p>Object-oriented programming is a programming paradigm in which you program\u00a0uses objects to represent things\u00a0you are programming about (sometimes real-world things). These objects could be data structures. The\u00a0objects hold data about them in attributes. The attributes in the objects are manipulated through methods or functions that are given to the object.</p> <p>For instance, we might have a\u00a0Person object\u00a0that represents all of the data a person would have: weight, height, skin color, hair color, hair length, and so on. Those would be the attributes. Then the person object would also have\u00a0things that it can do\u00a0such as: pick a box up, put box down, eat, sleep, etc. These would be the functions that play with the data the object stores.</p> <p>The main deal with OOP is the ability to encapsulate data from outsiders. Encapsulation is the ability to hide variables within the class from outside access \u2014 which makes it great for security reasons, along with leaky, unwanted, or accidental usage.</p> <p>Object-Oriented Programming is a diverse programming language that provides a clear structure for a program, making it easier to maintain, manage, debug, and reuse the code. This is perhaps the most significant reason why this programming method is so popular. </p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#major-concepts-of-object-oriented-programming","title":"Major Concepts of Object-Oriented Programming","text":""},{"location":"Computer-Science/OOP-vs-Functional-Programming/#encapsulation","title":"Encapsulation","text":"<p>Encapsulation, in general, is nothing but a fancy word for packaging or enclosing things of interest into one entity. By definition, encapsulation describes the idea of bundling data and methods that work on that data within one unit.</p> <p>Many programming languages use\u00a0encapsulation\u00a0frequently in the form of\u00a0classes. A\u00a0class\u00a0is a program-code-template that allows developers to create an object that has both variables (data) and behaviors (functions or methods). A class is an example of encapsulation in computer science in that it consists of data and methods that have been bundled into a single unit.</p> <p>Encapsulation may also refer to a mechanism of restricting the direct access to some components of an object, such that users cannot access state values for all of the variables of a particular object. Encapsulation can be used to hide both data members and data functions or methods associated with an instantiated class or object.</p> <p>Encapsulation in programming has a few key benefits. These include:</p> <ul> <li>Hiding Data:\u00a0Users will have no idea how classes are being implemented or stored. All that users will know is that values are being passed and initialized.</li> <li>More Flexibility:\u00a0Enables you to set variables as red or write-only. Examples include: setName(), setAge() or to set variables as write-only then you only need to omit the get methods like getName(), getAge() etc.</li> <li>Easy to Reuse:\u00a0With encapsulation it's easy to change and adapt to new requirements.</li> </ul>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#abstraction","title":"Abstraction","text":"<p>Abstraction in Object Oriented Programming solves the issues at the design level.</p> <p>Displays essential information and hides unwanted information, such as the inner details of how objects are implemented, from the user. Abstraction is vital to reduce programming complexity as it allows programmers to display only the relevant information of the object to the user.</p> <p>That enables the user to implement more complex logic on top of the provided abstraction without understanding or even thinking about all the hidden complexity.</p> <p>ex:</p> <pre><code>class Employee{\n    private name;\n    private baseSalary;\n\n    setName(val){\n        this.#name = val;\n    }\n    setBaseSalary(val){\n        this.#baseSalary = val;\n    }\n\n    getName(){\n        return this.#name;\n    }\n\n    getSalary(){\n        let bonus = 1000;\n        return this.#baseSalary + bonus;\n    }\n}\nvar emp = new Employee();\nemp.setName(\"abc\");\nemp.setBaseSalary(100);\nconsole.log(emp.getName());\nconsole.log(emp.getSalary());\n</code></pre>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#inheritance","title":"Inheritance","text":"<p>It is a mechanism where you can to derive a class from another class for a hierarchy of classes that share a set of attributes and methods.</p> <p>This concept lets programmers create new objects with the same attributes as old or existing objects. In essence, we are inheriting the attributes of parent objects to child objects.</p> <p>ex:</p> <pre><code>class Fruit {\n    name\n    color\n    constructor(name, color) {\n        this.name = name\n        this.color = color\n    }\n}\n\n// Strawberry is inherited from Fruit\nclass Strawberry extends Fruit {\n    constructor(name, color) {\n        super(name, color)\n    }\n}\n</code></pre>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#polymorphism","title":"Polymorphism","text":"<p>Allows programmers to run a child object exactly like the parent whilst retaining their specific methods.</p> <p>Let\u2019s take a new example to understand this concept further. Say we have an object called \u201cVehicle\u201d, and we create new child objects, called \u201cCar,\u201d \u201cBus,\u201d and \u201cTrain.\u201d</p> <p>Consider that the vehicle object has a method called \u201crun.\u201d</p> <p>Now according to the principles of Polymorphism, this method can be passed on to the child objects as well, but the way each child object implements this method will be different and specific to them.</p> <p>Polymorphism is the practice to design objects in such a way that they share and override behavior from parent objects.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#data-type-vs-data-structure","title":"Data Type vs Data Structure","text":"<p>A Data type is one of the forms of a variable to which the value can be assigned of a given type only. This value can be used throughout the program.</p> <p>A\u00a0data type\u00a0is the most basic and the most common classification of data. It is this through which the compiler gets to know the form or the type of information that will be used throughout the code. So basically data type is a type of information transmitted between the programmer and the compiler where the programmer informs the compiler about what type of data is to be stored and also tells how much space it requires in the memory. Some basic examples are int, string etc. It is the type of any variable used in the code.</p> <p>ex:</p> <pre><code>int a = 5;\nfloat b = 5.0;\nchar c = 'A';\n</code></pre> <p>the variable \u2018a\u2019 is of data type integer which is denoted by int a. So the variable \u2018a\u2019 will be used as an integer type variable throughout the process of the code. And, in the same way, the variables \u2018b\u2019 and \u2018c\u2019 are of type float and character respectively. And all these are kinds of data types.</p> <p>A\u00a0data structure\u00a0is a collection of different forms and different types of data that has a set of specific operations that can be performed. It is a collection of data types. It is a way of organizing the items in terms of memory, and also the way of accessing each item through some defined logic. Some examples of data structure are arrays, lists, stacks, queues, linked lists, binary tree and many more.</p> <p>It\u2019s like a container!</p> <p>For example, you have to store data for many employees where each employee has his name, employee id and a mobile number. So this kind of data requires complex data management, which means it requires data structure comprised of multiple primitive data types. So data structures are one of the most important aspects when implementing coding concepts in real-world applications.</p> <p>Data Structure is the collection of different kinds of data. That entire data can be represented using an object and can be used throughout the entire program.</p>"},{"location":"Computer-Science/core-thread-process/","title":"Computer Concepts (Program, Core, Process, Thread,...)","text":""},{"location":"Computer-Science/core-thread-process/#programs","title":"Programs","text":"<p>A program is a code that is stored on your computer that is intended to fulfill a certain task. There are many types of programs, including programs that help your computer function and are part of the operating system, and other programs that fulfill a particular job. These task-specific programs are also known as \u201capplications\u201d and can include programs such as word processing, web browsing,...</p> <p>Programs are typically stored on disk or in non-volatile memory in a form that can be executed by your computer. Prior to that, they are created using a programming language using instructions that involve logic, data and device manipulation, recurrence, and user interaction. The end result is a text file of code that is compiled into binary form (ones and zeros) in order to run on the computer. Another type of program is called interpreted and instead of being compiled in advance in order to run, is interpreted into executable code at the time it is run. Some common, typically interpreted programming languages, are Python, PHP, JavaScript, and Ruby.</p>"},{"location":"Computer-Science/core-thread-process/#process","title":"Process","text":"<p>The program has been loaded into the computer\u2019s memory in binary form. Now what?</p> <p>The program needs memory and various operating system resources in order to run. A \u201cprocess\u201d is what we call a program that has been loaded into memory along with all the resources it needs to operate. The \u201coperating system\u201d is the brains behind allocating all these resources. The OS handles the task of managing the resources needed to turn your program into a running process.</p> <p>There can be multiple instances of a single program, and each instance of that running program is a process. Each process has a separate memory address space, which means that a process runs independently and is isolated from other processes. It cannot directly access shared data in other processes. Switching from one process to another requires some time (relatively) for saving and loading registers, memory maps, and other resources.</p> <p>This independence of processes is valuable because the operating system tries its best to isolate processes so that a problem with one process doesn\u2019t corrupt or cause havoc with another process. You\u2019ve undoubtedly run into the situation in which one application on your computer freezes or has a problem and you\u2019ve been able to quit that program without affecting others.</p>"},{"location":"Computer-Science/core-thread-process/#thread","title":"Thread","text":"<p>Thread is a single sequential flow of control in a program that allows multiple activities within a single process. A single thread is like one command that runs at a time.</p> <p>Most processor manufacturers use the Simultaneous multithreading (SMT) technique to make sure a single processer can run multiple threads. Multithreading is similar to multitasking in which multiple threads are executed at a time, and a multithread ability manages numerous requests by the same user without opening multiple copies of programs running on the computer.</p> <p>A thread is a unit of execution on concurrent/parallel programming. Multithreading is a technique which allows a CPU to execute many tasks of one process at the same time. These threads can execute individually while sharing their resources.</p>"},{"location":"Computer-Science/core-thread-process/#how-threads-work","title":"How Threads Work","text":"<p>A thread is the unit of execution within a process. A process can have from just one thread to many threads.</p> <p></p> <p>When a process starts, it is assigned memory and resources. Each thread in the process shares that memory and resources. In single-threaded processes, the process contains one thread. The process and the thread are one and the same, and there is only one thing happening.</p> <p>In multithreaded processes, the process contains more than one thread, and the process is accomplishing a number of things at the same time</p> <p></p> <p>Each thread will have its own stack.</p> <p>Threads are sometimes called lightweight processes because they have their own stack but can access shared data. Because threads share the same address space as the process and other threads within the process, the operational cost of communication between the threads is low, which is an advantage. The disadvantage is that a problem with one thread in a process will certainly affect other threads and the viability of the process itself.</p> <p>Review:</p> <ol> <li>The program starts out as a text file of programming code.</li> <li>The program is compiled or interpreted into binary form.</li> <li>The program is loaded into memory.</li> <li>The program becomes one or more running processes.</li> <li>Processes are typically independent of each other.</li> <li>Threads exist as the subset of a process.</li> <li>Threads can communicate with each other more easily than processes can.</li> <li>Threads are more vulnerable to problems caused by other threads in the same process.</li> </ol>"},{"location":"Computer-Science/core-thread-process/#thread-vs-process","title":"Thread vs Process","text":"<p>When Google was designing the Chrome browser, they needed to decide how to handle the many different tasks that needed computer, communications, and network resources at the same time. Each browser window or tab communicates with multiple servers on the internet to retrieve text, programs, graphics, audio, video, and other resources, and renders that data for display and interaction with the user. In addition, the browser can open many windows, each with many tasks.</p> <p>Google had to decide how to handle that separation of tasks. They chose to run each browser window in Chrome as a separate process rather than a thread or many threads, as is common with other browsers. Doing that brought Google a number of benefits. Running each window as a process protects the overall application from bugs and glitches in the rendering engine and restricts access from each rendering engine process to others and to the rest of the system. Isolating a JavaScript program in a process prevents it from running away with too much CPU time and memory and making the entire browser non-responsive.</p> <p>Google made a calculated trade-off with the multi-processing design. Starting a new process for each browser window has a higher fixed cost in memory and resources than using threads. They were betting that their approach would end up with less memory bloat overall.</p> <p>Using processes instead of threads also provides better memory usage when memory gets low. An inactive window is treated as a lower priority by the operating system and becomes eligible to be swapped to disk when memory is needed for other processes. That helps keep the user-visible windows more responsive. If the windows were threaded, it would be more difficult to separate the used and unused memory as cleanly, wasting both memory and performance.</p>"},{"location":"Computer-Science/core-thread-process/#how-multithreading-works","title":"How Multithreading Works?","text":"<p>For example, most modern CPUs support multithreading. A simple app on your smartphone can give you a live demo of the same.</p> <p>When you open an app that requires some data to be fetched from the internet, the content area of the app is replaced by a spinner. This will rotates until the data is fetched and displayed.</p> <p>In the background, there are two threads:</p> <ul> <li>One fetching the data from a network, and</li> <li>One rendering the GUI that displays the spinner</li> </ul> <p>Both of these threads execute one after the other to give the illusion of concurrent execution</p>"},{"location":"Computer-Science/core-thread-process/#working-of-core-and-thread","title":"Working of Core and Thread","text":"<p>The core is a hardware component and performs and has the ability to run one task at one time.</p> <p>But multiple cores can support multiple applications to be executed without any disruptions.</p> <p>If the user is planning to set up a game, some parts of cores are required to run the game, some needed to check other background applications like skype, chrome, Facebook, etc. But the CPU should supports multithreading to executes these effectively to fetch the relevant information from the application within a minimum response time. Multithreading just makes the process fast and organized, and convert into better performance. It increases power consumption but rarely causes a rise in temperature.</p> <p>A CPU performance will depend upon the number of cores on the machine and the speed at which the individual cores can execute instructions.</p>"},{"location":"Computer-Science/core-thread-process/#concurrency-and-parallelism","title":"Concurrency and Parallelism","text":"<p>On a system with multiple processors or CPU cores (as is common with modern processors), multiple processes or threads can be executed in parallel. On a single processor, though, it is not possible to have processes or threads truly executing at the same time. In this case, the CPU is shared among running processes or threads using a process scheduling algorithm that divides the CPU\u2019s time and yields the illusion of parallel execution. The time given to each task is called a \u201ctime slice.\u201d The switching back and forth between tasks happens so fast it is usually not perceptible. The terms, \u201cparallelism\u201d (genuine simultaneous execution) and \u201cconcurrency\u201d (interleaving of processes in time to give the appearance of simultaneous execution), distinguish between the two types of real or approximate simultaneous operation.</p> <p></p>"},{"location":"Computer-Science/core-thread-process/#what-is-concurrency-or-single-core","title":"What is Concurrency or Single Core?","text":"<p>a Single-core CPU will only be able to process one program at a time. However, when you run multiple programs simultaneously, then a single-core processor will divide all programs into small pieces and concurrently execute with time slicing, as you can view in the picture given below.</p> <p>Concurrency is defined as the ability of a system to run two or more programs in overlapping time phases.</p> <p></p> <p>Concurrent execution with time slicing</p> <p>As you can see, at any given time, there is only one process in execution. Therefore, concurrency is only a generalized approximation of real parallel execution. This kind of situation can be found in systems having a single-core processor.</p>"},{"location":"Computer-Science/core-thread-process/#what-is-parallel-execution-or-multi-core","title":"What is Parallel Execution or (Multi-Core)?","text":"<p>A Multicore processor (multiple CPU cores) execute each sub-task simultaneously</p> <p>In parallel execution, the tasks to be performed by a process are broken down into sub-parts, and multiple CPUs (or multiple cores) process each sub-task at precisely the same time.</p> <p></p> <p>As you can see, at any given time, all processes are in execution. In reality, it is the sub-tasks of a process which are executing in parallel, but for better understanding, you can visualize them as processes.</p> <p>Therefore, parallelism is the real way in which multiple tasks can be processed at the same time. This type of situation can be found in systems having multicore processors, which includes almost all modern, commercial processors.</p>"},{"location":"Computer-Science/core-thread-process/#key-differences","title":"Key Differences","text":"<p>\u2022 Cores increase the amount of work accomplished at a time, whereas threads improve throughput, computational speed-up.</p> <p>\u2022 Cores is an actual hardware component whereas thread is a virtual component that manages the tasks.</p> <p>\u2022 Cores use content switching while threads use multiple CPUs for operating numerous processes.</p> <p>\u2022 Cores require only a signal process unit whereas threads require multiple processing units.</p>"},{"location":"Computer-Science/core-thread-process/#cpu-core","title":"CPU Core","text":"<p>A CPU core is the part of something central to its existence or character. In the same way in the computer system, the CPU is also referred to as the core.</p> <p>There are basically two types of core processor:</p> <ol> <li>Single-Core Processor</li> <li>Multi-Core Processor</li> </ol>"},{"location":"Computer-Science/core-thread-process/#what-is-the-main-issue-with-single-core","title":"What is the Main Issue with Single Core?","text":"<p>There are mainly two issues with Single Core.</p> <ul> <li>To execute the tasks faster, you need to increase the clock time.</li> <li>Increasing clock time increases power consumption and heat dissipation to an extremely high level, which makes the processor inefficient.</li> </ul>"},{"location":"Computer-Science/core-thread-process/#the-solution-provided-by-multi-core","title":"The Solution Provided by Multi-Core:","text":"<ul> <li>Creating two cores or more on the same die to increase the processing power while it also keeps clock speed at an efficient level.</li> <li>A processor with two cores running an efficient speed can process instructions with similar speed to the single-core processor. Its clock speed is twice, yet the multicore process consumes less energy.</li> </ul>"},{"location":"Computer-Science/core-thread-process/#benefits-of-multi-core-processor","title":"Benefits of Multi-core Processor","text":"<p>Here are some advantages of the multicore processor:</p> <ul> <li>More transistor per choice</li> <li>Shorter connections</li> <li>Lower capacitance</li> <li>A small circuit can work at fast speed</li> </ul>"},{"location":"Computer-Science/core-thread-process/#compiler-vs-interpreter","title":"Compiler vs. Interpreter","text":"<p>Computers can only understand a program written in a binary system known as machine code.</p> <p>To speak to a computer in its non-human language, we came up with two solutions: interpreters and compilers. Ironically, most of us know very little about them, although they belong to our daily coding life.</p> <p>A compiler is a computer program that transforms code written in a high-level programming language into the machine code. It is a program which translates the human-readable code to a language a computer processor understands (binary 1 and 0 bits). The computer processes the machine code to perform the corresponding tasks.</p> <p>An interpreter is a computer program, which converts each high-level program statement into the machine code. This includes source code, pre-compiled code, and scripts. Both compiler and interpreters do the same job which is converting higher level programming language to machine code. However, a compiler will convert the code into machine code (create an exe) before program run. Interpreters convert code into machine code when the program is run.</p> <ul> <li>A compiler translates a code written in a high-level programming language into\u00a0a lower-level language like assembly language, object code, and\u00a0machine code (binary 1 and 0 bits). It converts the code ahead of time before the program runs.</li> <li>An interpreter translates the code line by line when the program is running. You\u2019ve likely used interpreters unknowingly at some point in your work career.</li> </ul> <p></p> <p>Both compilers and interpreters have pros and cons:</p> <ul> <li>Compiler transforms code written in a high-level programming language into the machine code, at once, before program runs, whereas an Interpreter converts each high-level program statement, one by one, into the machine code, during program run.</li> <li>Compiled code runs faster while interpreted code runs slower.</li> <li>Compiler displays all errors after compilation, on the other hand, the Interpreter displays errors of each line one by one.</li> <li>Compiler is based on translation linking-loading model, whereas Interpreter is based on Interpretation Method.</li> <li>Compiler takes an entire program whereas the Interpreter takes a single line of code.</li> </ul> <p></p> <p>A high-level programming language is usually referred to as \u201ccompiled language\u201d or \u201cinterpreted language.\u201d However, in practice, they can have both compiled and interpreted implementations. C, for example, is called a compiled language, despite the existence of C interpreters. The first JavaScript engines were simple interpreters, but all modern engines use just-in-time (JIT) compilation for performance reasons.</p>"},{"location":"Computer-Science/core-thread-process/#refs","title":"Refs","text":"<ul> <li>https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/</li> </ul>"},{"location":"Computer-Science/https-tls-mtls/","title":"TLS, SSL, mTLS, HTTP, HTTPS","text":""},{"location":"Computer-Science/https-tls-mtls/#what-is-tls","title":"What is TLS?","text":"<p>Transport Layer Security, or TLS, is a widely adopted security\u00a0protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and\u00a0voice over IP (VoIP)</p> <p>TLS, which was formerly called\u00a0SSL, authenticates the server in a\u00a0client-server connection and encrypts communications between client and server so that external parties cannot spy on the communications.</p> <p>There are three important things to understand about how TLS works:</p> <p>1. Public key and private key</p> <p>TLS works using a technique called\u00a0public key cryptography, which relies on a pair of keys \u2014 a public key and a private key. Anything encrypted with the public key can be decrypted only with the\u00a0private\u00a0key.</p> <p>Therefore, a server that decrypts a message that was encrypted with the public key proves that it possesses the private key. Anyone can view the public key by looking at the domain's or server's TLS certificate.</p> <p>2. TLS certificate</p> <p>A TLS certificate is a data file that contains important information for verifying a server's or device's identity, including the public key, a statement of who issued the certificate (TLS certificates are issued by a certificate authority), and the certificate's expiration date.</p> <p>3. TLS handshake</p> <p>The\u00a0TLS handshake\u00a0is the process for verifying the TLS certificate and the server's possession of the private key. The TLS handshake also establishes how encryption will take place once the handshake is finished.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-mtls-work","title":"How does mTLS work?","text":"<p>Normally in TLS, the server has a TLS certificate and a public/private key pair, while the client does not. The typical TLS process works like this:</p> <ol> <li>Client connects to server</li> <li>Server presents its TLS certificate</li> <li>Client verifies the server's certificate</li> <li>Client and server exchange information over encrypted TLS connection</li> </ol> <p></p> <p>mTLS is not a different protocol. It is just an extension of the TLS standard.</p> <p>In mTLS, however, both the client and server have a certificate, and both sides authenticate using their public/private key pair. Compared to regular TLS, there are additional steps in mTLS to verify both parties (additional steps in\u00a0bold):</p> <ol> <li>Client connects to server</li> <li>Server presents its TLS certificate</li> <li>Client verifies the server's certificate</li> <li>Client presents its TLS certificate</li> <li>Server verifies the client's certificate</li> <li>Server grants access</li> <li>Client and server exchange information over encrypted TLS connection</li> </ol> <p></p> <p>Certificate authorities in mTLS</p> <p>The organization implementing mTLS acts as its own certificate authority. This contrasts with standard TLS, in which the certificate authority is an external organization that checks if the certificate owner legitimately owns the associated\u00a0domain.</p> <p>A \"root\" TLS certificate is necessary for mTLS; this enables an organization to be their own certificate authority. The certificates used by authorized clients and servers have to correspond to this root certificate. The root certificate is self-signed, meaning that the organization creates it themselves. (This approach does not work for one-way TLS on the public Internet because an external certificate authority has to issue those certificates.)</p>"},{"location":"Computer-Science/https-tls-mtls/#why-use-mtls","title":"Why use mTLS?","text":"<p>mTLS helps ensure that traffic is secure and trusted in both directions between a client and server. This provides an additional layer of security for users who log in to an organization's network or applications. It also verifies connections with client devices that do not follow a login process, such as Internet of Things devices.</p> <p>mTLS prevents various kinds of attacks, including:</p> <ul> <li>On-path attacks:\u00a0On-path attackers\u00a0place themselves between a client and a server and intercept or modify communications between the two. When mTLS is used, on-path attackers cannot authenticate to either the client or the server, making this attack almost impossible to carry out.</li> <li>Spoofing attacks:\u00a0Attackers can attempt to \"spoof\" (imitate) a web server to a user, or vice versa. Spoofing attacks are far more difficult when both sides have to authenticate with TLS certificates.</li> <li>Credential stuffing:\u00a0Attackers use leaked sets of credentials from a\u00a0data breach\u00a0to try to log in as a legitimate user. Without a legitimately issued TLS certificate,\u00a0credential stuffing\u00a0attacks cannot be successful against organizations that use mTLS.</li> <li>Brute force attacks:\u00a0Typically carried out with\u00a0bots, a\u00a0brute force attack\u00a0is when an attacker uses rapid trial and error to guess a user's password. mTLS ensures that a password is not enough to gain access to an organization's network. (Rate limiting\u00a0is another way to deal with this type of bot attack.)</li> <li>Phishing attacks:\u00a0The goal of a phishing attack\u00a0is often to steal user credentials, then use those credentials to compromise a network or an application. Even if a user falls for such an attack, the attacker still needs a TLS certificate and a corresponding private key in order to use those credentials.</li> <li>Malicious API requests:\u00a0When used for\u00a0API security, mTLS ensures that API requests come from legitimate, authenticated users only. This stops attackers from sending malicious API requests that aim to exploit a vulnerability or subvert the way the API is supposed to function.</li> </ul>"},{"location":"Computer-Science/https-tls-mtls/#websites-already-use-tls-so-why-is-mtls-not-used-on-the-entire-internet","title":"Websites already use TLS, so why is mTLS not used on the entire Internet?","text":"<p>For everyday purposes, one-way authentication provides sufficient protection. The goals of TLS on the public Internet are:</p> <ol> <li>to ensure that people do not visit\u00a0spoofed websites.</li> <li>to keep\u00a0private data\u00a0secure and encrypted as it crosses the various networks that\u00a0comprise the Internet.</li> <li>to make sure that data is not altered in transit. One-way TLS, in which the client verifies the server's identity only, accomplishes these goals.</li> </ol> <p>Additionally, distributing TLS certificates to all end user devices would be extremely difficult. Generating, managing, and verifying the billions of certificates necessary for this is a near-impossible task.</p> <p>But on a smaller scale, mTLS is highly useful and quite practical for individual organizations, especially when those organizations employ a Zero Trust approach to network security. Since a Zero Trust approach does not trust any user, device, or request by default, organizations must be able to authenticate every user, device, and request every time they try to access any point in the network. mTLS helps make this possible by authenticating users and verifying devices.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-the-difference-between-tls-and-ssl","title":"What is the difference between TLS and SSL?","text":"<p>TLS is the successor of SSL.</p> <p>TLS evolved from a previous encryption protocol called Secure Sockets Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-the-difference-between-tls-and-https","title":"What is the difference between TLS and HTTPS?","text":"<p>HTTPS\u00a0is an implementation of TLS encryption on top of the\u00a0HTTP\u00a0protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-http","title":"What is HTTP?","text":"<p>The Hypertext Transfer Protocol (HTTP) is the foundation of the World Wide Web, and is used to load web pages using hypertext links. HTTP is an\u00a0application layer\u00a0protocol designed to transfer information between networked devices and runs on top of other layers of the network protocol stack. A typical flow over HTTP involves a client machine making a request to a server, which then sends a response message.</p>"},{"location":"Computer-Science/https-tls-mtls/#whats-in-an-http-request","title":"What\u2019s in an HTTP request?","text":"<p>An HTTP request is the way internet communications platforms such as web browsers ask for the information they need to load a website. Each HTTP request made across the Internet carries with it a series of encoded data that carries different types of information. A typical HTTP request contains:</p> <ol> <li>HTTP version type</li> <li>a URL</li> <li>an HTTP method</li> <li>HTTP request headers</li> <li>Optional HTTP body.</li> </ol>"},{"location":"Computer-Science/https-tls-mtls/#what-is-https","title":"What is HTTPS?","text":"<p>Hypertext transfer protocol secure (HTTPS) is the secure version of\u00a0HTTP, which is the primary protocol used to send data between a web browser and a website. HTTPS is encrypted in order to increase security of data transfer. This is particularly important when users transmit sensitive data, such as by logging into a bank account, email service, or health insurance provider.</p> <p>Any website, especially those that require login credentials, should use HTTPS. In modern web browsers such as Chrome, websites that do not use HTTPS are marked differently than those that are.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-https-work","title":"How does HTTPS work?","text":"<p>HTTPS uses an\u00a0encryption\u00a0protocol to encrypt communications. The protocol is called\u00a0Transport Layer Security (TLS), although formerly it was known as\u00a0Secure Sockets Layer (SSL). This protocol secures communications by using what\u2019s known as an\u00a0asymmetric public key infrastructure. This type of security system uses two different keys to encrypt communications between two parties:</p> <ol> <li>The private key - this key is controlled by the owner of a website and it\u2019s kept, as the reader may have speculated, private. This key lives on a web server and is used to decrypt information encrypted by the public key.</li> <li>The public key - this key is available to everyone who wants to interact with the server in a way that\u2019s secure. Information that\u2019s encrypted by the public key can only be decrypted by the private key.</li> </ol>"},{"location":"Computer-Science/https-tls-mtls/#why-is-https-important-what-happens-if-a-website-doesnt-have-https","title":"Why is HTTPS important? What happens if a website doesn\u2019t have HTTPS?","text":"<p>HTTPS prevents websites from having their information broadcast in a way that\u2019s easily viewed by anyone snooping on the network. When information is sent over regular HTTP, the information is broken into packets of data that can be easily \u201csniffed\u201d using free software. This makes communication over the an unsecure medium, such as public Wi-Fi, highly vulnerable to interception. In fact, all communications that occur over HTTP occur in plain text, making them highly accessible to anyone with the correct tools, and vulnerable to\u00a0on-path attacks.</p> <p>With HTTPS, traffic is encrypted such that even if the packets are sniffed or otherwise intercepted, they will come across as nonsensical characters. Let\u2019s look at an example:</p> <p>Before encryption:</p> <pre><code>This is a string of text that is completely readable\n</code></pre> <p>After encryption:</p> <pre><code>ITM0IRyiEhVpa6VnKyExMiEgNveroyWBPlgGyfkflYjDaaFf/Kn3bo3OfghBPDWo6AfSHlNtL8N7ITEwIXc1gU5X73xMsJormzzXlwOyrCs+9XCPk63Y+z0=\n</code></pre> <p>In websites without HTTPS, it is possible for Internet service providers (ISPs) or other intermediaries to inject content into webpages without the approval of the website owner. This commonly takes the form of advertising, where an ISP looking to increase revenue injects paid advertising into the webpages of their customers. Unsurprisingly, when this occurs, the profits for the advertisements and the quality control of those advertisements are in no way shared with the website owner. HTTPS eliminates the ability of unmoderated third parties to inject advertising into web content.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-is-https-different-from-http","title":"How is HTTPS different from HTTP?","text":"<p>Technically speaking, HTTPS is not a separate protocol from HTTP. It is simply using TLS/SSL encryption over the HTTP protocol. HTTPS occurs based upon the transmission of\u00a0TLS/SSL certificates, which verify that a particular provider is who they say they are.</p> <p>When a user connects to a webpage, the webpage will send over its SSL certificate which contains the public key necessary to start the secure session. The two computers, the client and the server, then go through a process called an SSL/TLS handshake, which is a series of back-and-forth communications used to establish a secure connection. To take a deeper dive into encryption and the SSL/TLS handshake.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-a-website-start-using-https","title":"How does a website start using HTTPS?","text":"<p>Many website hosting providers and other services will offer TLS/SSL certificates for a fee. These certificates will be often be shared amongst many customers. More expensive certificates are available which can be individually registered to particular web properties.</p>"},{"location":"Computer-Science/https-tls-mtls/#refs","title":"Refs","text":"<ul> <li>Cloudflare</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/","title":"Reactive Programming vs Reactive Systems","text":"<p>Reactive programming \u2014 focusing on computation through ephemeral dataflow chains \u2014 tends to be\u00a0event-driven, while reactive systems \u2014 focusing on resilience and elasticity through the communication, and coordination, of distributed systems\u2014is\u00a0message-driven\u00a0(also referred to as\u00a0messaging).</p> <p>When people talk about \u201creactive\u201d in the context of software development and design, they generally mean one of three things:</p> <ul> <li>Reactive systems (architecture and design)</li> <li>Reactive programming (declarative event-based)</li> <li>Functional reactive programming (FRP)</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#reactive-programming","title":"Reactive Programming","text":"<p>Reactive Programming is an asynchronous programming model where the flow is composed of observable sources and reactions to events without sacrifice.  Is a programming paradigm that deals with asynchronous data streams and the propagation of changes in those streams. It allows you to build reactive and event-driven systems that can easily handle real-time data.</p> <p>Can be divided into two worlds:</p> <ul> <li>Event-loop (I/O Reactor, Netty, AsyncHttp);</li> </ul> <p>The event-loop model is based on the reactor pattern. The main idea behind this pattern is to have a handler (which in Node.js is represented by a callback function) associated with each I/O operation. When an event occurs, it is processed by the event loop, which invokes the appropriate handler to handle the event. In other words, The main idea behind the reactor pattern is to have a\u00a0handler which will be invoked as soon as an event is produced and processed by the event loop.</p> <ul> <li>Reactive-extensions implementation (RxJava, Reactor, Mutiny);</li> </ul> <p>The Reactive Extensions library extends the observer pattern and enables the composition of asynchronous and event-based programs. It lets you treat streams of asynchronous events with the same sort of simple, composable operations that you use for collections of data items, like arrays.</p> <p>Features:</p> <ul> <li>Publisher (Observable source, has a subscribe method)<ul> <li>Multi(0 .. N or error) \u2192 can publish 0, error, or infinite events to its subscribers.</li> <li>Single(0 .. 1 or error) \u2192 can publish 0, error, or 1 event to its subscribers</li> </ul> </li> <li>Lazy evaluation (nothing happens until someone subscribes)</li> <li>Hot vs Cold Publishers \u2192 Hot Publishers do not create new data producers for each new subscription (as the Cold Publisher does). Instead, there will be only one data producer and all the observers listen to the data produced by the single data producer. So all the observers get the same data.</li> <li>Schedulers \u2192 allow you to control the threading and concurrency.</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#the-reactor-pattern","title":"The Reactor Pattern","text":"<p>The reactor pattern is one implementation technique of event-driven architecture. In simple terms, it uses a single-threaded event loop blocking on resource-emitting events and dispatches them to corresponding handlers and callbacks.</p> <p>There is no need to block on I/O, as long as handlers and callbacks for events are registered to take care of them. Events refer to instances like a new incoming connection, ready for read, ready for write, etc.  Those handlers/callbacks may utilize a thread pool in multi-core environments.</p> <p>This pattern decouples the modular application-level code from reusable reactor implementation.</p> <p>There are two important participants in the architecture of Reactor Pattern:</p> <p>1. Reactor</p> <p>A Reactor runs in a separate thread, and its job is to react to IO events by dispatching the work to the appropriate handler. It\u2019s like a telephone operator in a company who answers calls from clients and transfers the line to the appropriate contact.</p> <p>2. Handlers</p> <p>A Handler performs the actual work to be done with an I/O event, similar to the actual officer in the company the client wants to speak to.</p> <p>A reactor responds to I/O events by dispatching the appropriate handler. Handlers perform non-blocking actions.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#asynchronous-programming","title":"Asynchronous Programming","text":"<p>Reactive Programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having a control flow driven by a thread of execution.</p> <p>Asynchronous\u00a0is defined by the Oxford Dictionary as \u201cnot existing or occurring at the same time,\u201d which in this context means that the processing of a message or event is happening at some arbitrary time, possibly in the future. This is a very important technique in reactive programming since it allows for\u00a0non-blocking\u00a0execution\u2014where threads of execution competing for a shared resource don\u2019t need to wait by blocking (preventing the thread of execution from performing other work until current work is done), and can as such perform other useful work while the resource is occupied. Amdahl\u2019s Law\u00a0tells us that contention is the biggest enemy of scalability, and therefore a reactive program should rarely, if ever, have to block.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#why-do-we-need-asynchronous-work","title":"Why do we need Asynchronous work?","text":"<p>The simple answer is we want to improve the user experience.\u00a0We want to make our application more responsive. We want to deliver a smooth user experience to our users without freezing the main thread, slowing them down and we don\u2019t want to provide the jenky performance to our users.</p> <p>To keep the main thread free we need to do a lot of heavy and time-consuming work we want to do in the background. We also want to do heavy work and complex calculations on our servers as mobile devices are not very powerful to do the heavy lifting. So we need asynchronous work for network operations.</p> <p>Reactive programming is generally\u00a0event-driven, in contrast to reactive systems, which are\u00a0message-driven</p> <p>The application program interface (API) for reactive programming libraries are generally either:</p> <ul> <li>Callback-based\u2014where anonymous side-effecting callbacks are attached to event sources, and are being invoked when events pass through the dataflow chain.</li> <li>Declarative\u2014through functional composition, usually using well-established combinators like\u00a0map,\u00a0filter,\u00a0fold\u00a0etc.</li> </ul> <p>The primary benefits of reactive programming are: increased utilization of computing resources on multicore and multi-CPU hardware; and increased performance by reducing serialization points and, by extension, Scalability Law.</p> <p>When using reactive programming, data streams are going to be the spine of your application. Events, messages, calls, and even failures are going to be conveyed by a data stream. With reactive programming, you observe these streams and react when a value is emitted.</p> <p>So, in your code, you are going to create data streams of anything and from anything: click events, HTTP requests, ingested messages, availability notifications, changes on a variable, cache events, measures from a sensor, literally anything that may change or happen. This has an interesting side-effect on your application: it\u2019s becoming inherently asynchronous.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#reactive-system","title":"Reactive System","text":"<p>A reactive system\u00a0is an architectural style that allows multiple individual applications to coalesce as a single unit, reacting to its surroundings, while remaining aware of each other\u2014this could manifest as being able to scale up/down, load balancing, and even taking some of these steps proactively. It\u2019s possible to write a single application in a reactive style (i.e. using reactive programming); however, that\u2019s merely one piece of the puzzle. Though each of the above aspects may seem to qualify as \u201creactive,\u201d in and of themselves they do not make a\u00a0system\u00a0reactive.</p> <p>In a Reactive System, it\u2019s the interaction between the individual parts that makes all the difference, which is the ability to operate individually yet act in concert to achieve their intended result.</p> <p>Messages have a clear (single) destination, while events are facts for others to observe. Furthermore, messaging is preferably asynchronous, with the sending and the reception decoupled from the sender and receiver respectively.</p> <p>Messages are needed to communicate across the network and form the basis for communication in distributed systems, while events on the other hand are emitted locally.</p> <p>Reactive Systems are designed to be responsive, resilient, elastic, and message-driven. They are highly scalable and can handle a large number of requests and events in a timely and efficient manner. They are capable of processing a large number of events in parallel without blocking or slowing down the system. Reactive Systems are highly resilient and can recover quickly from failures.</p> <p>Examples/UseCases:</p> <ul> <li> <p>Netflix =&gt; Netflix uses a microservices architecture and is built using technologies like Kafka, RxJava, Hystrix, Eureka...</p> </li> <li> <p>Real-time Analytics</p> </li> <li> <p>Internet of Things (IoT)</p> </li> </ul>"},{"location":"GitOps/","title":"GitOps","text":""},{"location":"GitOps/#traditional-approach","title":"Traditional Approach","text":"<p>Before dive into what GitOps is, it's important to understand how applications deployment was done, the traditional approachs..., how developers used to rollout applications version, and how they used to create infrastructure components, this way, it's easier to undestand the problems that the GitOps approach solves.</p> <p>Traditional approach</p> <p>Back then, applications were deployed using imperative scripts or manual processes, developers would roll out new versions of their applications by logging into servers and running scripts or commands to manually update the code and dependencies.</p> <p>for example, a developer that is working on a project feature will merge the changes in the main branch, ssh the virtual machine that is hosting that service, execute the commands to manually pull the changes, install dependencies, and re-run the service</p> <p>In nodejs for example:</p> <pre><code>ssh -i \"key.pem\" ec2-user@ec2-ip.compute-1.amazonaws.com\n\ngit clone ...\n\nnpm install\n\nnpm start\n</code></pre> <p>Best case scenario, this approach will have a pipeline defined to execute the imperative scripts, but there are a lot of developers that still do this manually.</p> <p>This way you have the application running with the new version.</p> <p>This approach has a lot of problems:</p> <ul> <li>time-consuming;</li> <li>error-prone;</li> <li>developers can easily make mistakes or overlook steps when running scripts manually, leading to issues and downtime. </li> <li>hard to track changes and ensure consistency across environments.</li> <li>difficult to rollback changes in case something happen.</li> <li>hard to know the current state of your infrastructure.</li> <li>...</li> </ul> <p>Despite these challenges, some developers and companies still use the old approach today. Perhaps they lack the knowledge or resources to adopt a new methodology, or they simply don't see the value in changing their ways. But the reality is that this approach is not reliable and can hold back innovation and development speed because this approach is script-based, the system configuration is tipically based in imperative scripts, in which is a sequence of steps to setup the system and reach a desired state</p> <p>I think you undesrstood the number of problems with this approach so far, needless to say that you have to create all infra components manually and if you're working with multiple environments you have to do this operation for each env (dev, stage, prod...)</p> <p>And because of these problems, using an declarative approach is extremely important to a modern software development to companies of all sizes</p>"},{"location":"GitOps/#gitops_1","title":"GitOps","text":"<p>In a few words, GitOps is infrastructure as Code (IaC), but it goes far beyond that.</p> <p>GitOps is an evolution of IaC with a recommended DevOps practice that utilize Git as the only source of truth and a pipeline to create/update/destroy the infrastructure, application or system architecture.</p> <p>GitOps leverages Git as a single source of truth for both application code and infrastructure configuration, making it easier to manage and automate the deployment process. With GitOps, developers can define their desired state for an environment in code and use Git to manage changes to that code over time. This ensures that infrastructure changes are tracked, versioned, and auditable, and makes it easier to roll back changes if needed.</p> <p>There are a lot of tools that allow us to create IaC such as Terraform, Ansible, Serverless Framework, CloudFormation, Crossplane, and others. However, to fully adopt GitOps, it's important to use these tools in conjunction with a Git-based deployment pipeline.</p> <p>Remember, Is not only by using one of these IaC tools that you're adopting GitOps, if you're still running local commands to apply these changes then there is a gap in there and you can't rely on git to know your system state.</p> <p>For example, let's say you're using Terraform to manage your infrastructure. With GitOps, you would define your infrastructure as code in Terraform, and store it in a Git repository. You would also define a deployment pipeline that automatically updates your infrastructure based on changes to the code in that repository.</p> <p>You might also use a pull-request strategy so that once the changes are approved and merged into the main branch, the pipeline would automatically deploy them to production.</p> <p>By using this approach, you can ensure that your infrastructure changes are tracked and versioned in Git, and that your deployment process is automated and auditable. This makes it easier to manage your infrastructure at scale.</p> <p>Benefits</p> <ul> <li>declarative approach (follows a declaration of an expected state rather than a sequence of commands.), easier to know and understand what's in your environment</li> <li>reduce errors and downtime</li> <li>enable more rapid innovation and experimentation</li> <li>consistent state</li> </ul>"},{"location":"GitOps/#gitops-approachs","title":"GitOps Approachs","text":"<ul> <li> <p>push-based</p> </li> <li> <p>pull-based</p> </li> </ul>"},{"location":"Kafka/","title":"Apache Kafka","text":"<p>Open source project created by Apache foundation</p> <p>Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. </p> <p>Nowadays everything is based on events, and we have to get these events to understand what happen and deal with the data in the most properly way.</p> <p>Questions</p> <ul> <li>Where do I save these events?</li> <li>How can I restore in a fast and simple way these events so that the feedback between one process and another can happen in a fluidly way and in real time?</li> <li>How to scale?</li> <li>How can I have resilience and high availability?</li> </ul> <p>Kafka \"Super Powers\"</p> <ul> <li>High throughput. can handle thousands of events per second;</li> <li>Low latency (2ms);</li> <li>Scalable;</li> <li>High Storage;</li> <li>High availability;</li> <li>It can connect with almost everything (Go, Node, Python,\u2026);</li> <li>Has a lot of libraries that makes easy to work with Kafka;</li> <li>Open source;</li> </ul>"},{"location":"Kafka/#how-it-works","title":"How it works","text":"<pre><code>                                          [Broker A]\n[Producer] -- Data --&gt; [Apache Kafka] --&gt; [Broker B] &lt;-- data -- [Consumer]\n                                          [Broker C]\n</code></pre> <p>Kafka is a cluster, and this cluster is formed by a set of nodes and these nodes are our brokers;</p> <p>Producer \u21d2 Publish messages;</p> <p>Consumer \u21d2 Consume the message, will access the broker to read the messages;</p> <p>Broker \u21d2 Each broker has its own database, each broker is a machine, and the node responsability is just to store the data;</p> <p>Kafka doesn\u2019t send message to anyone, Kafka just stores the message and the consumer read/retrieve the data;</p> <p>Usually when you deploy a cluster the recommendation is to have at least 3 brokers;</p> <p>These brokers communicate all the time and in order to manage this communication and to see what nodes are available or not the Kafka has another service called zookeper that can work as a service discovery, load balancer and so on\u2026</p>"},{"location":"Kafka/#managed-services","title":"Managed Services","text":"<ul> <li>Confluent Cloud \u2192 the most complete enviroment (Expensive)</li> <li>Amazon MSK (Managed Streaming for Apache Kafka) \u2192 (Expensive) Lowest price = $2.5/day</li> <li>Aiven Kafka</li> <li>...</li> </ul>"},{"location":"Kafka/avro/","title":"Avro","text":"<p>Avro is defined by a schema (schema is written in JSON)</p> <p>Advantages:</p> <ul> <li>Data is fully typed</li> <li>Data is compressed automatically(less CPU usage)</li> <li>Schema (defined using JSON) comes along with the data</li> <li>Documentation is embedded in the schema</li> </ul> <p>Primitive types:</p> <ul> <li>null: no value</li> <li>boolean: binary value</li> <li>int: 32-bit signed integer</li> <li>long: 64-bit signed integer</li> <li>float: single precision(32-bit) IEEE 754 floating-point number</li> <li>double: double precision (64-bit) IEEE 754 floating point number</li> <li>bytes: sequence of 8-bit unsigned bytes</li> <li>string: unicode character sequence</li> </ul>"},{"location":"Kafka/avro/#avro-record-schemas","title":"Avro Record Schemas","text":"<p>Avro Record Schemas are defined using JSON</p> <p>it has some common fields:</p> <ul> <li>Name: name of your schema</li> <li>Namespace: \"package\u201d</li> <li>Doc: documentation to explain your schema</li> <li>Aliases: Optional other names for your schema</li> <li>Fields<ul> <li>Name: field name</li> <li>Doc: field doc</li> <li>Type: field value</li> <li>Default: default value</li> </ul> </li> </ul> <p>ex:</p> <pre><code>{\n    \"type\": \"record\",\n    \"namespace\": \"com.example\",\n    \"name\": \"Customer\",\n    \"doc\": \"Avro schema for our customer\",\n    \"fields\": [\n        {\"name\": \"first_name\", \"type\": \"string\", \"doc\": \"\"},\n    ]\n}\n</code></pre> <p>Others types:</p> <ul> <li>Enums</li> </ul> <pre><code>{\"type\": \"enum\", \"name\": \"CustomerStatus\", \"symbols\": [\"BRONZE\", \"SILVER\", \"GOLD\"]}\n</code></pre> <ul> <li>Array</li> </ul> <pre><code>{\"type\": \"array\", \"items\": [\"string\"]}\n</code></pre> <ul> <li>Maps: Key values</li> </ul> <pre><code>{\"type\": \"map\", \"values\": \"string\"}\n</code></pre> <ul> <li>Unions: allow a field value to take different types</li> </ul> <pre><code>{\"name\": \"middle_name\", \"type\": [\"null\", \"string\"], \"default\": null}\n</code></pre>"},{"location":"Kafka/kafka-connect/","title":"Kafka Connect","text":"<p>Open source platform that works as a data hub to create simple centralizations such as database, key-value stores, search indexes and file systems.</p> <p>Kafka Connect is a framework for connecting Kafka with external systems, including databases.</p> <p>A Kafka Connect cluster is a separate cluster from the Kafka cluster. The Kafka Connect cluster supports running and scaling out connectors (components that support reading and/or writing between external systems).</p> <p>The Kafka connector is designed to run in a Kafka Connect cluster to read data from Kafka topics and write the data into Snowflake tables.</p> <p>How it works?</p> <p>Lets suppose I have my data stored on an CRM such as Salesforce and I want to store that data on another system or a database (Postgres, MySQL,\u2026)</p> <p>I can connect my Kafka Connect on Salesforce, the salesforce data will be stored on a Kafka topic and from this topic I can save on my database;</p> <p>Or if I want to put my SQL data on Mongo, it requires some effort to create this integration and Kafka Connect helps us with this problem without us having to write a line of code</p> <pre><code>[Apache Kafka] &lt;----&gt; [Kafka Connect]\n           MySql   &lt;--- [Connector]\n           MongoDb &lt;--- [Connector]\n                        [Connector] ---&gt; Lambda\n                        [Connector] ---&gt; Elasticsearch\n</code></pre> <p>A connector can be a mysql, mongo, lambda, elastic search, salesforce,\u2026</p> <p>Connectors Type:</p> <ul> <li>Data Sources \u21d2 Get data from somewhere to send to Apache Kafka, Ex:<ul> <li>MySQL</li> <li>Mongo</li> <li>SalesForce</li> </ul> </li> <li>Sinks \u21d2 Where to send this information<ul> <li>Elasticsearch</li> <li>AWS Lamda</li> </ul> </li> </ul> <p>Ex:</p> <ul> <li>I can send MySQL data to a AWS Lambda,</li> <li>Get data from MongoDB and send to a ElasticSearch,</li> <li>\u2026</li> </ul> <p>https://www.confluent.io/hub/</p>"},{"location":"Kafka/kafka-connect/#converters","title":"Converters","text":"<p>The tasks use \u201cconverters\u201d to change the data format for read or write purpose;</p> <p>Popular formats</p> <ul> <li>Avro; better undersantd of json files</li> <li>Protobuf;</li> <li>JsonSchema;</li> <li>Json;</li> <li>String;</li> <li>ByteArray;</li> </ul>"},{"location":"Kafka/kafka-connect/#dlq-dead-letter-queue","title":"DLQ - Dead Letter Queue","text":"<p>When there is an invalid record, for any reason, this error can be handled in the connector configuration through the \u201cerrors.tolerance\u201d property. This type of configuration can only be used for \u201cSink\u201d connectors;</p> <ul> <li>none: the task stops immediately</li> <li>all: errors are ignored and the process continue normally</li> <li>errors.deadletterqueue.topic.name = </li> </ul>"},{"location":"Kafka/producers-and-consumers/","title":"Producers and Consumers","text":""},{"location":"Kafka/producers-and-consumers/#producer","title":"Producer","text":"<p>Responsible for producing messages for a specific topic.</p>"},{"location":"Kafka/producers-and-consumers/#delivery-guarantee","title":"Delivery Guarantee","text":"<pre><code>[Producer] ----&gt; [Broker A][Topic A] Leader\n                 [Broker B][Topic A] Follower\n                 [Broker C][Topic A] Follwer\n</code></pre> <p>The Producer will alwasy send the message to the leader broker first.</p> <p>Acks: The acks setting specifies acknowledgements that the producer requires the leader to receive before considering a request complete.</p> <ul> <li>[Ack 0, None] \u2192 Acknowledge 0, None, AKA FF (Fire and Forget)</li> <li>It\u2019s the fastest way to send messages because Kafka doesn\u2019t waste time responding to the user so it can handle way more transactions</li> <li>Uber usecase: The driver send his location every 10 seconds, this location is sent to Kafka to handle that message, if Kafka loses 2 location moments, that wouldn't be a drastic loss for the software, so Uber can afford to lose some data in this case;;</li> <li>[Ack 1, Leader] \u2192 Acknowledge 1, The moment the leader saves the message it will return to the user saying that the message was stored;</li> <li>The speed is a little bit slower</li> <li>Potencial Problem: the Broker A saved the message and then returned to the users saying that the message was stored but in the same moment the node goes down, and the Broker A didn\u2019t have time to replicate the data to the followers;</li> <li>[Ack -1, All] \u2192 Acknowledge -1, The producer will send the message to the leader, the leader will replicate to the followers, the followers will notify the leader saying the message was stored and then the leader will respond the client saying the message was safely saved</li> <li>If you can\u2019t afford lose a message no matter what, you should use this type</li> <li>If Broker A goes down doesn\u2019t matter because the message is stored in other brokers;</li> <li>We\u2019ll lost speed to process the messages</li> </ul>"},{"location":"Kafka/producers-and-consumers/#producer-usecase-idempotent","title":"Producer Usecase: Idempotent","text":"<p>Suppose you have an application that sends customer orders to a Kafka topic. Each order message has a unique identifier. The consumer application that reads the order messages from the topic processes each order exactly once.</p> <p>Now, suppose the producer application encounters a network error while sending an order message to Kafka. As a result, it retries sending the message, and the message is delivered twice to the Kafka topic. The consumer application will then process the same order twice, resulting in inconsistencies in the order processing.</p> <p>However, if the producer application is configured to use the producer idempotent feature, it will ensure that only one copy of the message is delivered to the Kafka topic, even if multiple retries are attempted. This ensures that the consumer application processes each order exactly once, regardless of any duplicate messages in the Kafka topic.</p> <p></p> <p>The message 4 first got an error but after the retry rule it tried again and was successfully stored, but on the first error the producer tried to send the message again and in this case we will have a duplicated data.</p> <p>Kafka has a way to verify when this happens and if we activate this solution it will cause more slowdown in the system, but is a way to avoid duplicated message and order the messages.</p> <p>The producer idempotent feature ensures that even if a producer sends duplicate messages due to network errors or other issues, Kafka will only store and deliver each message once. This helps prevent data duplication and inconsistencies in the event stream.</p> <p>While the producer idempotent feature in Kafka can be very useful in preventing data duplication and ensuring exactly-once message delivery, there are some potential downsides to consider:</p> <ol> <li> <p>Increased latency: The producer idempotent feature requires additional network round trips between the producer and the Kafka broker to ensure that each message is delivered exactly once. This can increase the overall latency of message production.</p> </li> <li> <p>Higher resource utilization: This feature requires additional memory and CPU resources on both the producer and the broker side to maintain and track the message state.</p> </li> </ol>"},{"location":"Kafka/producers-and-consumers/#consumer","title":"Consumer","text":"<p>Responsible for reading the messages that the producer puts on a topic.</p>"},{"location":"Kafka/producers-and-consumers/#consumers-groups","title":"Consumers Groups","text":"<p>The primary role of a Kafka consumer is to read data from an appropriate Kafka broker. A consumer group is a group of consumers that share the same group id. When a topic is consumed by consumers in the same group, every record will be delivered to only one consumer.</p> <p>If all the consumer instances have the same consumer group, then the records will effectively be load-balanced over the consumer instances</p> <p>This way you can ensure parallel processing of records from a topic and be sure that your consumers won\u2019t be stepping on each other toes.</p> <p>First scenario:</p> <p>One consumer read all partitions</p> <pre><code>[Producer] ---&gt; [    Topic    ]\n                [ Partition 0 ]  ---&gt; [Consumer A]\n                [ Partition 1 ]  ---&gt; [Consumer A]\n                [ Partition 2 ]  ---&gt; [Consumer A]\n</code></pre> <p>Second scenario: Consumer Groups</p> <p>When these consumers are inside a group</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer A  ]\n                [ Partition 2 ]  ---&gt; [  Consumer B  ]\n</code></pre> <p>Consumer A and B can be the same software but running on different machines and because they are the same software and process the same transactions we can put them on a group and the data will be distributed across this group. The Partition 0 and 1 will be read by consumer A and the partition 2 by consumer B.</p> <p>best scenario: 3 partition, 3 consumers</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer B  ]\n                [ Partition 2 ]  ---&gt; [  Consumer C  ]\n</code></pre> <p>usecase:</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer B  ]\n                [ Partition 2 ]  ---&gt; [  Consumer C  ]\n                                      [  Consumer D  ]\n</code></pre> <p>in this case, Consumer D will be AFK because consumers inside a group cannot read the same data from another consumer.</p> <p>If the consumer doesn\u2019t have a group the consumer itself will be a standalone group and would read all the partitions.</p>"},{"location":"Kafka/topics/","title":"Topics","text":"<p>Topic is a communication channel responsable for receive and make the kafka messages available.</p> <p>If you want to send a message you have to send a message to a topic, the same thing for reading</p> <pre><code>                    /-- [Consumer]\n[Producer] --&gt; Topic &lt;-- [Consumer]\n                    \\-- [Consumer]\n</code></pre> <p>Kafka is different from RabbitMQ because the same message can be read for differents consumers;</p> <p>RabbitMQ save the data in memory while Kafka saves on disk, that way you can read the same message over and over again.</p> <p>Partitions - Anatomy of a record</p> <p>Record:</p> <pre><code>              [ Headers   ]\n[Offset 0] -&gt; | Key       |\n              | Value     |\n              [ Timestamp ]\n</code></pre> <ul> <li>Headers \u2192 Metadata that can be useful for us;</li> <li>Key \u2192 To ensure the delivery order;</li> <li>Value \u2192 Payload, message content;</li> <li>Timestamp \u2192 Created at;</li> </ul> <p>Each topic can have one or more partitions to ensure distribution and resilience of the data;</p> <p>You can think of a partition as being a drawer, it\u2019s a space on the disk where Kafka will store the message. </p> <p>Ex:</p> <pre><code>        ---&gt; [Partition 1] Broker A\nTopic X ---&gt; [Partition 2] Broker A or Broken B\n        ---&gt; [Partition 3] Broker A, B, C or D....\n</code></pre> <p>You cannot have all the eggs on the same basket;</p> <p>The idea is to not have all the messages on the same partition/broker, every time we increase the amount of partition the messages will be more distributed/separeted; If the broker A goes down at least we can have the message on the broker B, C,\u2026</p> <p>Lets suppose we have 1 million messages and single computer, it will require a lot of computational power from this computer and also a lot of effort to process every message; So we can create another computer and to ensure we will not have the both computers reading the same data we split these datas on differents partitions, each computer reads the same topic but from different partitions;</p> <p>Now we have twice more speed/power;</p> <p>Partitions and Keys</p> <p>How can we guarantee the order of messages?</p> <p>About the \u201cKeys\u201d</p> <pre><code>[Partition 1] &lt;---- Consumer 1 (slow)\n        [Offset 0], [Offset 1]\n\n[Partition 2] &lt;---- Consumer 2 (fast)\n        [Offset 0],\n\n[Partition 3] &lt;---- Consumer 3\n        [Offset 0],...\n</code></pre> <p>The only way we can guarantee the order of the messages is when they are on the same partition;</p> <p>Ex: User A buy a product, then at the same moment he will request a refund, but these messages can be in different partitions, the purchase request can be at partition 1 and the refund at the partition 2, but in this scenario the Consumer 1 is slow, so what if this first message receive an error and consumer 2, which is fast, has already processed the refund transaction?</p> <p>In order to guarantee the order of the messages to be executed these messages must be at the same partition and we can do it by using key.</p> <p>Ex:</p> <pre><code>Transfer Message [0] -&gt; Key=Movimentation\nRefund Message   [1] -&gt; Key=Movimentation\nRandom Message   [2] -&gt; No keys\n</code></pre> <p>In this case the first and the second message will be placed on the same partition and the last will be placed with the kafka default behavior, distributing between partitions;</p> <p>Distributed Partitions</p> <p>What usually happens:</p> <pre><code>               ---&gt; [Broker A][Partition 1]\nTopic: [Sales] ---&gt; [Broker B][Partition 2]\n               ---&gt; [Broker C][Partition 3]\n</code></pre> <p>With Replication Factor:</p> <pre><code>               ---&gt; [Broker A][Partition 1][Partition 3]\nTopic: [Sales] ---&gt; [Broker B][Partition 2][Partition 1]\n               ---&gt; [Broker C][Partition 3][Partition 2]\n</code></pre> <p>Replication Factor = 2</p> <p>Replication factor is a way to guarantee data resilience because if the Broker A goes down we have a copy of the partition 1 and 3 on broker B and C. So the most critical our data is, we can have more replication factor to ensure that we will never lose that data;</p> <p>The more replication we have more disk space will be required.</p> <p>The recommendation is to have 2 replication factor and if the application is very critical you can have 3.</p> <p>Partition Leadership</p> <ul> <li>Leaders = Bold</li> <li>Followers = normal</li> </ul> <p>All partitions are on the same topic. ex: Sale</p> [Broker A ] [Broker B ] [Broker C ] [Broker D ] [Partition 1] [Partition 1] [Partition 4] [Partition 3] [Partition 2] [Partition 2] [Partition 2] [Partition 2] [Partition 4] [Partition 3] [Partition 3] [Partition 4] <p>The bold paritions are the leaders, when a consumer has to read a data he will always go to the leader to retrieve that information, even if you have 10 copy of this partition the consumer always is going to the leader.</p> <p>In case a leader partition goes down, the consumer will read from the next available partition, ex: Broker A goes down and is no longer available, so now Broker B will have Partition 1 and 2 as leader. The follower is just a backup in case a leader goes down.</p>"},{"location":"Microservices/","title":"Microservices-based Architecture","text":""},{"location":"Microservices/#differences-between-microservice-and-monolith","title":"Differences between Microservice and Monolith","text":"<p>Microservices are ordinary applications, there isn't any difference from any other application.</p> <p>The main difference are the goals, microservices has a well defined goal/domain and a monolith has all the ecosystem/responsability on the same system.</p> <p>Microservices are part of an ecosystem, it isn't an isolated application, usually are part of a bigger context.</p> <p>They are independent and communicate all the time directly or indirectly. </p> <p>Another big difference is the deployment process, microservices has less risk because the process is more independent.</p> <p>Another difference is the team organization, we can have one team per microservice</p>"},{"location":"Microservices/#when-to-use","title":"When to use","text":"<p>Monolith</p> <ul> <li>Start of a project</li> <li>POC - Proof of Concept</li> <li>When you don't understand the domains</li> <li>Ensure technology governance</li> <li>Easier to understand the code flow</li> <li>Everything on the same spot</li> </ul> <p>Microservice</p> <ul> <li>Scale teams</li> <li>Well defined contexts / business rules/area</li> <li>Maturity in the delivery process</li> <li>Technical Maturity</li> <li>When you have the necessity of scale just one part of the system</li> <li>When you need different technologies</li> </ul>"},{"location":"Microservices/#migration","title":"Migration","text":"<ul> <li>Context separation;</li> <li>Avoid excess granularity;</li> <li>Verify dependencies to avoid distributed monolith;</li> <li>Migrate databases;</li> <li>Think in events;</li> <li>Eventual consistency;</li> <li>CI/CD/Tests/Environments;</li> <li>Start from the edges, parts of the system that doesn't affect the main domain;</li> </ul>"},{"location":"Microservices/#resiliency","title":"Resiliency","text":"<ul> <li>Health Check</li> <li>Rate Limiting</li> <li>Circuit breaker</li> <li>API Gateway</li> <li>Service Mesh</li> <li>Retry</li> <li> <p>Transactional Outbox</p> <p>Temporary table, is a local queue;</p> <p>First we save the message on this table and then we send the message to kafka, if the message was sent successfully we delete the message from the Outbox table, but, if some error happen to send the message to kafka we know that it'll be secure in our db</p> </li> <li> <p>Fallback policies</p> </li> <li>Observability</li> <li>Idempotency - deal with duplicated messages</li> </ul> <p>Complex Situations:</p> <ul> <li>What if the message broker goes down? how the system should behave?</li> <li>It will have message loss?</li> </ul> <p>Exponential backoff and Jitter:\u00a0https://aws.amazon.com/pt/blogs/architecture/exponential-backoff-and-jitter/</p> <p>OTEL -\u00a0https://opentelemetry.io/</p>"},{"location":"Microservices/#choreography-vs-orchestration","title":"Choreography vs Orchestration","text":"<p>A choreographed system uses by definition event-driven communication, whereas microservice orchestration uses command-driven communication. An event is something which happened in the past and is a fact. The sender does not know who picks up the event or what happens next. An example can be the event \u201cCredit checked.\u201d</p> <p>Choreography</p> <p>All microservices talks to each other, there isn't a master service to orchestrate the communication, if service A needs something from service C it will talk directly.</p> <ul> <li>Loose coupling: Choreography allows microservices to be loosely coupled, which means they can operate independently and asynchronously without depending on a central coordinator. This can make the system more scalable and resilient, as the failure of one microservice will not necessarily affect the other microservices.</li> <li>Ease of maintenance: Choreography allows microservices to be developed and maintained independently, which can make it easier to update and evolve the system.</li> <li>Decentralized control: Choreography allows control to be decentralized, which can make the system more resilient and less prone to failure.</li> <li>Asynchronous communication: Choreography allows microservices to communicate asynchronously, which can be more efficient and scalable than synchronous communication.</li> </ul> <p>Orchestration</p> <p>Has a \"maestro\" that control the moment where each step/service will execute.</p> <p>When talking about orchestration you can picture a big orchestra which features multiple instruments and a conductor who makes sure that everyone stays in tact. He tells when which instrument needs to play to ensure that the song sounds as it should like.</p> <ul> <li>Simplicity: Orchestration can be simpler to implement and maintain than choreography, as it relies on a central coordinator to manage and coordinate the interactions between the microservices.</li> <li>Centralized control: With a central coordinator, it is easier to monitor and manage the interactions between the microservices in an orchestrated system.</li> <li>Visibility: Orchestration allows for a holistic view of the system, as the central coordinator has visibility into all of the interactions between the microservices.</li> <li>Ease of troubleshooting: With a central coordinator, it is easier to troubleshoot issues in an orchestrated system.</li> </ul>"},{"location":"Microservices/#durs-principle","title":"DURS Principle","text":"<p>Each service can be independently DURS (deployed, updated, replaced, and scaled)</p> <ul> <li>Microservice</li> <li>Domain-Driven Design</li> <li>Failure Isolation</li> <li>Continuous Delivery</li> <li>Decentralization</li> <li>DevOps</li> <li>Scalability</li> <li>Resilience</li> </ul>"},{"location":"Microservices/#failure-isolation","title":"Failure Isolation","text":"<ul> <li>What happens when that request fails?</li> <li>What is our average response time on that request?</li> <li>What would our support team change about the user experience?</li> </ul> <p>The microservices architecture moves application logic to services and uses a network layer to communicate between them. Communicating over a network instead of in-memory calls brings extra latency and complexity to the system which requires cooperation between multiple physical and logical components. The increased complexity of the distributed system leads to a higher chance of particular\u00a0network failures.</p> <p>One of the biggest advantage of a microservices architecture over a monolithic one is that teams can independently design, develop and deploy their services. They have full ownership over their service\u2019s lifecycle. It also means that teams have no control over their service dependencies as it\u2019s more likely managed by a different team. With a microservices architecture, we need to keep in mind that provider\u00a0services can be temporarily unavailable\u00a0by broken releases, configurations, and other changes as they are controlled by someone else and components move independently from each other.</p>"},{"location":"Microservices/#strategies","title":"Strategies:","text":"<ul> <li> <p>Automatic Rollouts</p> <p>In a microservices architecture, services depend on each other. This is why you should minimize failures and limit their negative effect. To deal with issues from changes, you can implement change management strategies and\u00a0automatic rollouts.</p> <p>For example, when you deploy new code, or you change some configuration, you should apply these changes to a subset of your instances gradually, monitor them and even automatically revert the deployment if you see that it has a negative effect on your key metrics.</p> <p>Another solution could be that you run two production environments. You always deploy to only one of them, and you only point your load balancer to the new one after you verified that the new version works as it is expected. This is called blue-green, or red-black deployment.</p> </li> <li> <p>Health-check and Load balancing</p> <p>Instances continuously start, restart and stop because of failures, deployments or autoscaling. It makes them temporarily or permanently unavailable. To avoid issues, your load balancer should\u00a0skip unhealthy instances\u00a0from the routing as they cannot serve your customers\u2019 or sub-systems\u2019 need.</p> <p>Application instance health can be determined via external observation. You can do it with repeatedly calling a\u00a0<code>GET /health</code>\u00a0endpoint or via self-reporting. Modern\u00a0service discovery\u00a0solutions continuously collect health information from instances and configure the load-balancer to route traffic only to healthy components.</p> </li> </ul> <p>More:</p> <ul> <li>designing-microservices-architecture-for-failure</li> </ul>"},{"location":"Microservices/characteristics/","title":"Characteristics","text":""},{"location":"Microservices/characteristics/#componentization-via-services","title":"Componentization via Services","text":"<ul> <li>component is a unit of software that is independently replaceable and upgradeable.</li> <li>libraries are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call.</li> <li>Services are independently deployable and scalable, which means that they can be deployed and started, or restarted, independently of other services.</li> <li>Services are also independently replaceable, which means that a new version of a service can be deployed alongside the old version, and traffic can be routed between them.</li> </ul>"},{"location":"Microservices/characteristics/#organized-around-business-capabilities","title":"Organized around Business Capabilities","text":"<ul> <li>Microservices are organized around business capabilities, which means that each service is responsible for a single capability, and that all of the code and data for that capability is encapsulated within the service.</li> <li>\"Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.\" in other words, usually, we structure our software based on the structure of our organization. However, when we start to develop a microservices architecture, we need to think about the business capabilities that we want to deliver to our customers, and then organize our software around those capabilities.</li> <li> <p>For example, if we are building an e-commerce application, we might have services for product search, product recommendations, product reviews, product ratings, shopping cart, checkout, and payment processing. Each of these services would be responsible for a single capability, and for each service we would have a separate team of developers.</p> </li> <li> <p>How big is a microservice?</p> <ul> <li>The largest sizes reported follow Amazon's notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the smaller size scale we've seen setups where a team of half-a-dozen would support half-a-dozen services.</li> </ul> </li> </ul>"},{"location":"Microservices/characteristics/#products-not-projects","title":"Products not Projects","text":"<ul> <li>Microservices are products, not projects, which means that they are long-lived, and evolve over time.</li> <li>Most application development efforts use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded.</li> <li>Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime.</li> <li>\"You build, you run it\" is a common mantra in the microservice world, meaning that a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden.</li> <li>Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability.</li> </ul>"},{"location":"Microservices/characteristics/#smart-endpoints-and-dumb-pipes","title":"Smart endpoints and dumb pipes","text":"<ul> <li>When building communication structures between different processes, we've seen many products and approaches that stress putting significant smarts into the communication mechanism itself. A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules.</li> <li>The microservice community favours an alternative approach: smart endpoints and dumb pipes. Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. </li> <li>Microservices use technology agnostic protocols for communication between services, and implement business logic in services rather than in the transport.</li> <li>The communication channel should not have business logic embedded in it. It should be a dumb pipe that can transport messages from one service to another. The business logic should be in the service itself.</li> <li>when the communication channel is smart, it becomes difficult to change the business logic because it is embedded in the communication channel and we end up creating a coupling between the business logic and the communication channel.</li> </ul>"},{"location":"Microservices/characteristics/#decentralized-governance","title":"Decentralized Governance","text":"<ul> <li>One of the consequences of centralised governance is the tendency to standardise on single technology platforms. However, not every problem is a nail and not every solution a hammer.</li> <li>Splitting the monolith's components out into services we have a choice when building each of them. Use right tool for the job. Of course, just because you can do something, doesn't mean you should - but partitioning your system in this way means you have the option.</li> <li>Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on wheter your services are functioning.</li> </ul>"},{"location":"Microservices/characteristics/#decentralized-data-management","title":"Decentralized Data Management","text":"<ul> <li>\"the conceptual model of the world will differ between systems.\" This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics.</li> <li>A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them.</li> <li>As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions.</li> <li>Distributed transactions are notoriously difficult to implement and as a consequence microservice architectures emphasize transactionless coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations.</li> </ul>"},{"location":"Microservices/characteristics/#infrastructure-automation","title":"Infrastructure Automation","text":"<ul> <li>We want as much confidence as possible that our software is working, so we run lots of automated tests. Promotion of working software 'up' the pipeline means we automate deployment to each new environment.</li> <li>Regardess of the tool (e.g. terraform, ansible, github-actions, etc), the goal is to have an automated process that can deploy the software the environments.<ul> <li>example: compile, unit and funcional tests -&gt; acceptance test -&gt; integration test -&gt; user acceptance test -&gt; performance test -&gt; deploy production</li> </ul> </li> </ul>"},{"location":"Microservices/characteristics/#design-for-failure","title":"Design for failure","text":"<ul> <li>A consequence of using services as components, is that applications need to be designed so that they can tolerate the failure of services.</li> <li>Any service call could fail due to unavailability of the supplier, the client has to respond to this as gracefully as possible. -- This is a disadvantage compared to a monolithic design as it introduces additional complexity to handle it.</li> <li>The consequence is that microservice teams constantly reflect on how service failures affect the user experience.</li> <li>Since services can fail at any time, it's important to be able to detect the failures quickly and, if possible, automatically restore service. Microservice applications put a lot of emphasis on real-time monitoring of the application, checking both architectural elements (how many requests per second is the database getting) and business relevant metrics (such as how many orders per minute are received). Semantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate.</li> <li>Microservice teams would expect to see sophisticated monitoring and logging setups for each individual service such as dashboards showing up/down status and a variety of operational and business relevant metrics. Details on circuit breaker status, current throughput and latency are other examples we often encounter in the wild.</li> </ul>"},{"location":"Microservices/characteristics/#evolutionary-design","title":"Evolutionary Design","text":"<ul> <li>The key property of a component is the notion of independent replacement and upgradeability, which implies we look for points where we can imagine rewriting a component without affecting its collaborators.</li> <li>You want to keep things that change at the same time in the same module. Parts of a system that change rarely should be in different services to those that are currently undergoing lots of churn. If you find yourself repeatedly changing two services together, that's a sign that they should be merged.</li> <li>the preference in the microservice world is to only use versioning as a last resort. We can avoid a lot of versioning by designing services to be as tolerant as possible to changes in their suppliers.</li> <li>When applying evolutionary design it is important to keep the design as clean as possible at all times. This means that you need to keep your code well factored, with a good set of automated tests. This allows you to make changes quickly and safely, and prevents the code from rotting.</li> </ul>"},{"location":"Microservices/characteristics/#trade-offs","title":"Trade-Offs","text":"Pros Cons Strong Module Boundaries Distribution Independent Deployment Eventual Consistency Technology Diversity Operational Complexity"},{"location":"Microservices/characteristics/#refs","title":"Refs","text":"<ul> <li>martinfowler-componentization</li> </ul>"},{"location":"Microservices/resilience/","title":"Resilience","text":""},{"location":"Microservices/resilience/#introduction","title":"Introduction","text":"<ul> <li>Principle: \"At some point, every system will fail.\"</li> <li>Resilience is the ability of a system to recover from failures and continue to function.</li> <li>Resilience is a set of intentionally strategies for the adaptation of a system when a failure occurs.</li> <li>Have resilience strategies allow us to minimize the risk of data loss and crucial transactions for the business.</li> </ul>"},{"location":"Microservices/resilience/#resilience-strategies","title":"Resilience Strategies","text":""},{"location":"Microservices/resilience/#protect-and-be-protected","title":"Protect and be protected","text":"<ul> <li>a system in a distributed arquitecture needs to adopt self-preservation mechanisms to ensure its operation with the highest possible quality.<ul> <li>in a software system quality means that the system is available, reliable, and secure. If your SLA is 99.99% availability, then your system must be available 99.99% of the time. If your system has to respond to a request in 100ms, then it must respond in 100ms or less regardless of the number of requests (100, 1K, 1M).</li> </ul> </li> <li>a system cannot be selfish to the point of making more requests to a failing system. It must be able to protect itself from failures and protect other systems from its own failures.<ul> <li>if the external system is failing, it must not make more requests to that systems, it must wait for the system to recover.</li> </ul> </li> <li>a slow system available most times is worse than an unavailable system (cascade effect).<ul> <li>all systems that rely on the slow system will be affected by the slowness.</li> </ul> </li> </ul>"},{"location":"Microservices/resilience/#health-check","title":"Health check","text":"<ul> <li>a system must be able to check its own health.</li> <li>without vital signs, it's not possible to know if the system is \"alive\" or \"dead\".<ul> <li>health check is not only return a 200 OK response, it's also check if the external dependencies of that system are healthy, the database, the cache,..., everything that is necessary for the system to work.</li> </ul> </li> <li>a system that is not healthy has a chance of self-recover if stop receiving traffic.<ul> <li>if the system is not healthy, it must not receive traffic, it must be isolated from the rest of the system.</li> <li>when the system is not healthy, for example is slow because it received a lot of traffic/requests, in this case, if we stop sending traffic to the system, it will have a chance to process the pending requests and self-recover.</li> </ul> </li> <li>Active health check<ul> <li>the system is responsible for checking its own health.</li> <li>the system is responsible for reporting its health status to the health check system (monitoring system).</li> </ul> </li> <li>Passive health check<ul> <li>the system is not responsible for checking its own health.</li> <li>the health check system is responsible for checking the health of the system.</li> <li>example of passive health check: kubernetes probes. The probes are responsible for checking the health of the system and the system is responsible for reporting its health status to the kubernetes probes.</li> </ul> </li> </ul>"},{"location":"Microservices/resilience/#rate-limiting","title":"Rate limiting","text":""},{"location":"Observability/","title":"Observability","text":"<p>Observability lets us understand a system from the outside, by letting us ask questions about that system without knowing its inner workings. Furthermore, allows us to easily troubleshoot and deal with new problems, and helps us answer the question, \u201cWhy is this happening?\u201d</p> <p>In order to be able to ask those questions of a system, the application must be properly instrumented. That is, the application code must emit\u00a0signals\u00a0such as\u00a0traces,\u00a0metrics, and\u00a0logs. An application is properly instrumented when developers don\u2019t need to add more instrumentation to troubleshoot an issue because they have all of the information they need.</p> <p>It's important to ensure the application is running properly, in a big system we cannot work without observability.</p> <p>\"Imagine you are the pilot of an airplane with 300 people flying, you need the necessary measures (high, velocity,...) to know what decision to take or if a problem happen you need to know where to fix. Our applications is an airplane\"</p> <p>Who doesn't measure doesn't manage;</p> <p>It\u2019s essential to understand when something goes wrong along the application delivery chain so you can identify the root cause and correct it before it impacts your business. Monitoring and observability provide a two-pronged approach. Monitoring supplies situational awareness, and observability helps pinpoint what\u2019s happening and what to do about it.</p> <p>Good monitoring is a staple of high-performing teams. DevOps Research and Assessment (DORA) research shows that a comprehensive monitoring and observability solution, along with a number of other technical practices, positively contributes to continuous delivery.</p>"},{"location":"Observability/#observability-vs-monitoring","title":"Observability vs Monitoring","text":"<p>Observability and monitoring are two related concepts, monitoring is a subset of observability.</p> <p>Monitoring is the process of collecting and analyzing data from a system to assess its health and performance. It involves setting up a predefined set of metrics and logs to track, typically focusing on metrics that are essential for the proper functioning of the system. </p> <p>Observability provides a broader understanding of a system. It involves collecting and analyzing a wide range of data, including metrics, logs, traces, and events to explore properties and patterns not defined in advance.</p>"},{"location":"Observability/#observability_1","title":"Observability","text":"<p>Is tooling or a technical solution that allows teams to actively debug their system. Observability is based on exploring properties and patterns not defined in advance.</p> <p>Observability is the ability to understand a complex system\u2019s internal state by analyzing the data it generates, such as logs, metrics, and traces. it helps teams analyze what\u2019s happening and identify the root cause of a performance problem so they can detect and resolve the underlying causes of issues.</p> <ul> <li>How well you can understand your complex system.</li> <li>It\u2019s a measure of our internal system, how we can get the data output and understand what is happening inside the system.</li> <li>Understand why is wrong, and how that happen?</li> <li>Gain visibility into aspects of the system that were previously unknown or unexpected.</li> <li>Get insight to identify and resolve issues more quickly.</li> <li>Understand how different components are interacting with each other and identify potential points of failure.</li> <li>Enable teams to proactively optimize their systems, by identifying areas for improvement and make data-driven decisions.</li> </ul>"},{"location":"Observability/#monitoring","title":"Monitoring","text":"<p>Monitoring is the task of assessing the health of a system by collecting and analyzing aggregate data based on a predefined set of metrics and logs.</p> <p>It measures the health of the application, such as creating a rule that alerts when the app is nearing 100% disk usage, helping prevent downtime. It shows you not only how the app is functioning, but also how it\u2019s being used over time.</p> <ul> <li>Show when there\u2019s something wrong;</li> <li>Know in advance the signals you want to monitor;</li> <li>Enables teams to proactively detect and diagnose issues before they impact end-users;</li> <li>Detect and diagnose issues before they escalate into more significant problems.</li> <li>Generate alerts when predefined thresholds are crossed or when other predefined conditions are met.</li> </ul> <p></p> <p>For example, in a e-commerce scenario, monitoring would focus on specific metrics that are critical for the proper functioning of each service, such as response time, error rates, and CPU usage. A tool (such as Prometheus) could be used to track these metrics and generate alerts when predefined thresholds are crossed.</p> <p>And the observability would allow teams to explore data in real-time to gain insights into the behavior and performance of the entire system. such as trace requests across the entire system and identify the root cause of the issue.</p> <p>3 Pillars of Observability:</p> <ul> <li> <p>Metrics \u21d2 number, we have 2 types of metrics, business metrics and technical metrics;     technical metrics: CPU: 90%, Memory: 50%...</p> <p>business metrics: 50 new students, 10 students left this month,...</p> </li> <li> <p>Logs \u21d2 result of a specific events;</p> </li> <li> <p>Tracing \u21d2 order of how the event was generated, stacktrace;</p> </li> </ul>"},{"location":"Observability/#tools","title":"Tools","text":"<ul> <li>Elastic Stack</li> <li>Prometheus &amp; Grafana</li> <li>OpenTelemetry</li> <li>Datadog</li> </ul>"},{"location":"Observability/elastic-stack/","title":"Elastic Stack","text":"<p>ELK Stack</p> <ul> <li>Elasticsearch: it can search data more efficiently and scalable; it can do geospatial analysis and visualization; Logging and analytics;<ul> <li>Search engine and analytics;</li> </ul> </li> <li>Logstash: real time data collector engine. get data from many places and transform and send data to many places. Also we can use some plugins to manipulate the data;<ul> <li>data processor through pipeline that can receive, transform and send simultaneous data;</li> </ul> </li> <li>Kibana: Dashboard to visualize and explore the data, is used to: logs, analyse, application monitoring, operational intelligence. Integrated with Elasticsearch, also allow us to aggregate and filter data;<ul> <li>Allow users to visualize the elasticsearch data in different perspective;</li> <li>maps, interactive graphs, dashboards;</li> </ul> </li> </ul>"},{"location":"Observability/elastic-stack/#elk-vs-elastic-stack","title":"ELK vs Elastic Stack ?","text":"<p>ELK \u21d2 Logstash \u2192 Elasticsearch \u2192 Kibana</p> <p>Elastic Stack \u21d2 (Beats, Logstash) \u2192 Elasticsearch \u2192 Kibana</p> <p>Beats \u2192 \u201clightweight data shipper\u201d delivery data on a light way. Data collector agent.</p> <ul> <li>Easy integration with Elasticsearch or Logstash,</li> <li>Get logs, metrics, network data, audit data, uptime monitoring</li> </ul>"},{"location":"Observability/elastic-stack/#elastic","title":"Elastic","text":"<p>Elastic is a company behind the elasticstack, we can use the stack without pay anything but there is some plugins that we need to pay in order to use;</p> <p>Products:</p> <ul> <li>APM \u2192 Application Performance Monitoring</li> <li>Maps</li> <li>Site Search</li> <li>Enterprise search</li> <li>App Search</li> <li>Infrastructure</li> </ul>"},{"location":"Observability/elastic-stack/#kibana","title":"Kibana","text":"<p>Kibana is your window into the Elastic Stack. Specifically, it's a browser-based analytics and search dashboard for Elasticsearch.</p>"},{"location":"Observability/elastic-stack/#projects","title":"Projects","text":"<ul> <li>Observability-ElasticStack</li> </ul>"},{"location":"Observability/open-telemetry/","title":"OpenTelemetry","text":""},{"location":"Observability/open-telemetry/#telemetry-reliability-and-metrics","title":"Telemetry, Reliability and Metrics","text":"<p>Telemetry\u00a0refers to data emitted from a system about its behavior. The data can come in the form of\u00a0Traces,\u00a0Metrics, and\u00a0Logs.</p> <p>Reliability\u00a0answers the question: \u201cIs the service doing what users expect it to be doing?\u201d A system could be up 100% of the time, but if, when a user clicks \u201cAdd to Cart\u201d to add a black pair of pants to their shopping cart, and instead, the system keeps adding a red pair of pants, then the system would be said to be\u00a0unreliable.</p> <p>Metrics\u00a0are aggregations over a period of time of numeric data about your infrastructure or application. Examples include system error rate, CPU utilization, request rate for a given service, and so on\u2026.</p>"},{"location":"Observability/open-telemetry/#logs","title":"Logs","text":"<p>A\u00a0Log\u00a0is a timestamped message emitted by services or other components. Unlike\u00a0Traces, however, they are not necessarily associated with any particular user request or transaction. They are found almost everywhere in software, and have been heavily relied on in the past by both developers and operators alike to help them understand system behavior.</p> <p>Unfortunately, logs aren\u2019t extremely useful for tracking code execution, as they typically lack contextual information, such as where they were called from.</p> <p>They become far more useful when they are included as part of a\u00a0Span.</p>"},{"location":"Observability/open-telemetry/#spans","title":"Spans","text":"<p>A\u00a0Span\u00a0represents a unit of work or operation. It tracks specific operations that a request makes, painting a picture of what happened when that operation was executed.</p> <p>A Span contains name, time-related data,\u00a0structured log messages, and\u00a0other metadata (i.e. Attributes)\u00a0to provide information about the operation it tracks.</p> <p>Below is a sample of the type of information that would be present in a Span:</p> <p>Span Attributes</p> Key Value net.transport IP.TCP net.peer.ip 10.244.0.1 net.peer.port 10243 net.host.name localhost http.method GET http.target /cart http.server_name frontend http.route /cart http.scheme http http.host localhost http.flavor 1.1 http.status_code 200"},{"location":"Observability/open-telemetry/#distributed-traces","title":"Distributed Traces","text":"<p>A\u00a0Distributed Trace, more commonly known as a\u00a0Trace, records the paths taken by requests (made by an application or end-user) as they propagate through multi-service architectures, like microservice and serverless applications.</p> <p>Without tracing, it's challenging to identify the cause of performance issues in a distributed system.</p> <p>It improves the visibility of our application or system\u2019s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for distributed systems, which commonly have nondeterministic problems or are too complicated to reproduce locally.</p> <p>Tracing makes debugging and understanding distributed systems less daunting by breaking down what happens within a request as it flows through a distributed system.</p> <p>A Trace is made of one or more Spans. The first Span represents the Root Span. Each Root Span represents a request from start to finish. The Spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).</p> <p>Many Observability back-ends visualize Traces as waterfall diagrams that may look something like this:</p> <p></p> <p>Waterfall diagrams show the parent-child relationship between a Root Span and its child Spans. When a Span encapsulates another Span, this also represents a nested relationship.</p> <ul> <li>If you have 5 microservices and a user got an error 500, what microservice went wrong? with tracing, we can see the request from microservice to microservice to debug the problem.</li> </ul>"},{"location":"Observability/open-telemetry/#so-what","title":"So what?","text":"<p>In order to make a system observable, it must be instrumented. That is, the code must emit\u00a0traces,\u00a0metrics, and\u00a0logs. The instrumented data must then be sent to an Observability back-end. There are a number of Observability back-ends out there, ranging from self-hosted open-source tools to commercial SaaS offerings.</p> <p>In the past, the way in which code was instrumented would vary, as each Observability back-end would have its own instrumentation libraries and agents for emitting data to the tools.</p> <p>This meant that there was no standardized data format for sending data to an Observability back-end. Furthermore, if a company chose to switch Observability back-ends, it meant that they would have to re-instrument their code and configure new agents just to be able to emit telemetry data to the new tool of choice.</p> <p>Recognizing the need for standardization, the cloud community came together, and two open-source projects were born:\u00a0OpenTracing\u00a0(a\u00a0Cloud Native Computing Foundation (CNCF)\u00a0project) and\u00a0OpenCensus\u00a0(a\u00a0Google Open Source\u00a0community project).</p> <p>In the interest of having one single standard, OpenCensus and OpenTracing were merged to form OpenTelemetry (OTel for short)\u00a0in May 2019. As a CNCF incubating project, OpenTelemetry takes the best of both worlds, and then some.</p>"},{"location":"Observability/open-telemetry/#what-is-opentelemetry-otel","title":"What is OpenTelemetry (OTel)","text":"<p>OpenTelemetry, also known as OTel for short, is a vendor-neutral open source Observability framework for instrumenting, generating, collecting, and exporting telemetry data such as\u00a0traces,\u00a0metrics,\u00a0and logs. As an industry standard, it is\u00a0natively supported by a number of vendors.</p> <p>OpenTelemetry is a collection of tools, APIs, and SDKs. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software\u2019s performance and behavior.</p> <p>OTel\u2019s goal is to provide a set of standardized vendor-agnostic SDKs, APIs, and\u00a0tools\u00a0for ingesting, transforming, and sending data to an Observability back-end (i.e. open-source or commercial vendor).</p>"},{"location":"Observability/open-telemetry/#what-can-opentelemetry-do-for-me","title":"What can OpenTelemetry do for me?","text":"<p>OTel has broad industry support and adoption from cloud providers,\u00a0vendors\u00a0and end users. It provides you with:</p> <ul> <li>A single, vendor-agnostic instrumentation library\u00a0per language\u00a0with support for both automatic and manual instrumentation.</li> <li>A single vendor-neutral\u00a0collector\u00a0binary that can be deployed in a variety of ways.</li> <li>An end-to-end implementation to generate, emit, collect, process and export telemetry data.</li> <li>Full control of your data with the ability to send data to multiple destinations in parallel through configuration.</li> <li>Open-standard semantic conventions to ensure vendor-agnostic data collection</li> <li>The ability to support multiple\u00a0context propagation\u00a0formats in parallel to assist with migrating as standards evolve.</li> <li>A path forward no matter where you are on your observability journey.</li> </ul> <p>Vendors and Tools with different patterns = Lock-In</p>"},{"location":"Observability/open-telemetry/#what-opentelemetry-is-not","title":"What OpenTelemetry is not","text":"<p>OpenTelemetry is not an observability back-end like Jaeger or Prometheus. Instead, it supports exporting data to a variety of open-source and commercial back-ends. It provides a pluggable architecture so additional technology protocols and formats can be easily added.</p> <p>Refs:</p> <ul> <li>OpenTelemetry</li> </ul>"},{"location":"Observability/service-level/","title":"Service Level","text":""},{"location":"Observability/service-level/#sla","title":"SLA","text":"<p>An SLA (service level agreement) is an agreement between provider and client about measurable metrics like uptime, responsiveness, and responsibilities.</p> <p>These agreements are typically drawn up by a company\u2019s new business and legal teams and they represent the promises you\u2019re making to customers\u2014and the consequences if you fail to live up to those promises. Typically, consequences include financial penalties, service credits, or license extensions.</p> <ul> <li>Agreement you make with your clients or users.</li> <li>Agreement between a vendor and a paying customer.</li> <li>Legal contract that if breached, will have financial penalties.</li> </ul>"},{"location":"Observability/service-level/#slo","title":"SLO","text":"<p>An SLO (service level objective) is an agreement within an SLA about a specific metric like uptime or response time. So, if the SLA is the formal agreement between you and your customer, SLOs are the individual promises you\u2019re making to that customer.</p> <p>SLOs are what set customer expectations and tell IT and DevOps teams what goals they need to hit and measure themselves against.</p> <ul> <li>Intended to define a range of what is most and least acceptable for performance standards;</li> <li>Objectives your team must hit to meet that agreement;</li> <li>99% of requests served in &lt; 400ms over a 28-day window;</li> <li>Disaster recovery time;</li> <li>Application availability;</li> </ul>"},{"location":"Observability/service-level/#sli","title":"SLI","text":"<p>An SLI (service level indicator) measures compliance with an SLO (service level objective). So, for example, if your SLA specifies that your systems will be available 99.95% of the time, your SLO is likely 99.95% uptime and your SLI is the actual measurement of your uptime. Maybe it\u2019s 99.96%. Maybe 99.99%. To stay in compliance with your SLA, the SLI will need to meet or exceed the promises made in that document.</p> <ul> <li>Real numbers on performance</li> </ul> <p>Refs:</p> <ul> <li>Atlassian</li> </ul>"},{"location":"SOLID/","title":"SOLID","text":"<p>SOLID are five principles of object-oriented programming that facilitate software development, making them easy to maintain and extend. These principles can be applied to any OOP language.</p> <p>These principles help the programmer write cleaner code, separating responsibilities, reducing couplings, facilitating refactoring, and encouraging code reuse.</p> <ul> <li>S \u21d2 SRC \u21d2 Single Responsibility Principle</li> <li>O \u21d2 OCP \u21d2 Open-closed Principle</li> <li>L \u21d2 LSP \u21d2 Liskov Substitution Principle</li> <li>I \u21d2 ISP \u21d2 Interface Segregation Principle</li> <li>D \u21d2 DIP \u21d2 Dependency Inversion Principle</li> </ul>"},{"location":"SOLID/dependency-inversion/","title":"Dependency Inversion Principle","text":"<p>Dependency Inversion Principle corresponds to\u00a0D among SOLI\u2019D\u2019 Principles. Its principle starts with this statement.</p> <p>High-level modules should not depend on low-level modules. Both should depend on abstractions.</p> <p>What is Abstraction?</p> <p>Code without abstraction: <pre><code>class Benz  {\n    drive() {\n    }\n\n}\n\nclass CarUtil {\n    drive(benz Benz) {\n        benz.drive();\n    }\n}\n</code></pre></p> <p>When you change\u00a0<code>drive()</code>\u00a0method inside\u00a0<code>Benz</code>\u00a0class, <code>CarUtil</code> is directly affected. This is prone to make bugs.</p> <p>Tight Coupling is the most undesirable feature in Software</p> <pre><code>interface Car {\n    drive();\n}\n\nclass Benz implements Car {\n    @Override\n    drive() {\n    }\n\n}\n\nclass Tesla implements Car {\n    @Override\n    drive() {\n    }\n\n}\n\nclass CarUtil {\n    drive(car Car) {\n        car.drive();\n    }\n}\n</code></pre> <p>\u201cAbstractions should not depend on details. Details should depend on abstractions.\u201d</p> <pre><code>import java.util.Arrays;\nimport java.util.List;\n\n// High Level Module \nclass ProductCatalog {\n\n    public void listAllProducts() {\n\n        // High Level Module depends on Abstraction \n        ProductRepository productRepository =  new SQLProductRepository();\n\n        List&lt;String&gt; allProductNames = productRepository.getAllProductNames();\n\n        // Display product names\n    }\n\n}\n\ninterface ProductRepository {\n    List&lt;String&gt; getAllProductNames();\n\n}\n\n// Low Level Module\nclass SQLProductRepository implements ProductRepository {\n\n    public List&lt;String&gt; getAllProductNames() {\n        return Arrays.asList(\"soap\", \"toothpaste\", \"shampoo\");\n    }\n\n}\n</code></pre> <p>Why doing this?</p> <p>First, you don\u2019t know what database you are going to use. It may not be specifically\u00a0<code>SQL</code>.</p> <p>Second, <code>ProductCatalog</code>\u2018s\u00a0<code>listAllProducts()</code>\u00a0does not depend on a specific object. This means, when you change code in\u00a0<code>SQLProductRepository</code>,\u00a0<code>ProductCatalog</code>\u00a0is not directly affected. You just have achieved\u00a0<code>loose-coupling</code>.</p>"},{"location":"SOLID/interface-segregation/","title":"Interface Segregation Principle","text":"<p>No client should be forced to depend on methods it does not use</p> <p>\u201cYOU SHOULD ONLY DEFINE METHODS THAT ARE GOING TO BE USED\u201d.</p> <p>This principle basically says that it's better to create more specific interfaces rather than having a single generic interface.</p> <p>Wrong: <pre><code>interface IMultiFunction {\n    public void print();\n    public void getPrintSpoolDetails();\n    public void scan();\n    public void scanPhoto();\n    public void fax();\n    public void internetFax();\n}\n\nclass CanonPrinter implements IMultiFunction {\n\n    @Override\n    public void print() {}\n\n    @Override\n    public void getPrintSpoolDetails() {}\n\n    /* This machine can't scan */\n    @Override\n    public void scan() {}\n\n    /* This machine can't scan photo */\n    @Override\n    public void scanPhoto() {}\n\n    /* This machine can't fax */\n    @Override\n    public void fax() {}\n\n     /* This machine can't fax on internet */\n    @Override\n    public void internetFax() {}\n\n}\n</code></pre></p> <p>A class should not be required to implement interfaces that it will not use.</p> <p>Correct: <pre><code>interface Movie {\n    public function play();\n}\ninterface AudioControl {\n    public function increaseVolume();\n}\n\nclass TheLionKing implements Movie, AudioControl {\n    public function play() {}\n    public function increaseVolume() {}\n}\n\nclass ModernTimes implements Movie {\n    public function play() {}\n}\n</code></pre></p>"},{"location":"SOLID/liskov-substitution/","title":"Liskov Substitution Principle","text":"<p>Objects should be replaceable with their subtypes without affecting the correctness of the program</p> <p>Subclasses can be substituted for their parent classes.</p> <p>Class Y extending class X, class X must be replaceable by class Y.</p> <p>Example of the duck, a real duck and a rubber duck, the battery duck does the same thing as the real duck, however, it uses batteries, so it is violating this principle because it is not a real duck.</p> <p>If S is a subtype of T, then objects of type T, in a program, can be replaced by objects of type S without changing the program's properties.</p> <p>Wrong: <pre><code>class Car {\n    double getCabinWidth() {\n    // return cabin width \n    }   \n}\n\nclass RacingCar extends Car {\n\n    public double getCabinWidth() {\n        // ...\n    }\n\n    public double getCockpitWidth() {\n        // return cockpit width \n    }\n}\n</code></pre></p> <p>the Car interface cannot be replaced by RacingCar so it is hurting this principle.</p>"},{"location":"SOLID/open-closed/","title":"Open-closed Principle","text":"<p>Software components should be closed for modification, but open for extension.</p> <p>that is, when new behaviors and features need to be added to the software, we should extend and not change the original source code.</p> <p>Example: <pre><code>class InsuranceCompany {\n    discountRate(customer VehicleInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20;\n        }\n        return 10;\n    }\n}\n\nclass VehicleInsuranceCustomer {\n    public boolean isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre></p> <p><code>InsuranceCompany</code> has the\u00a0<code>discountRate()</code> method. But as you can see, the parameter of this method only accepts the <code>VehichleInsuranceCustomer</code> type. But what if we want to introduce another type of customer? we would have to add another method.</p> <p>Ex: <pre><code>class InsuranceCompany {\n\n    discountRate(customer VehicleInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n    discountRate(customer HomeInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n    discountRate(customer LifeInsuranceCustomer) {\n      if(customer.isLoyalCustomer()) {\n          return 20; \n      }        \n        return 10;\n    }\n\n}\n\nclass VehicleInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass HomeInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass LifeInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre></p> <p>This is the problem. You have to keep introducing duplicate code because the method is not \u201copen for extension\u201d, it does not allow dynamic input. This makes the code difficult to maintain and fix.</p> <p>Solution: Work with abstractions <pre><code>class InsuranceCompany {\n    // Parameter is now abstracted \n    discountRate(customer CustomerProfile) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n}\n\n// Core \ninterface CustomerProfile {\n    isLoyalCustomer(): boolean;\n}\n\nclass VehicleInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass HomeInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass LifeInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre></p> <p>Classes that need to communicate with InsuranceCompany must implement an interface, so we're making an abstraction. By doing this we can add more customers (open for extension), and also we are not obliged to modify the InsuranceCompany code (closed for modification).</p> <p>Benefits:</p> <ul> <li>Easy to add new features;</li> <li>Less development and testing cost.</li> <li>Open Closed Principle requires decoupling, which ends up following the Single Responsibility Principle.</li> </ul>"},{"location":"SOLID/single-responsability/","title":"Single Responsibility Principle","text":"<p>A class should have one, and only one, reason to change.</p> <p>This principle states that a class must be specialized in a single subject and have only one responsibility within the software, that is, the class must have a single task or action to perform.</p> <p>A class should have only one responsibility, it cannot do more than it was created to do.</p> <ul> <li>\"Every Software Component should have only one responsibility.\"</li> </ul> <p>A software component can be understood as a class, function, method, or even a module.</p> <p>It's very common when we are developing a software to give a class more than one responsibility and end up creating classes that do it all \u2014 God Class*. At first, this may seem efficient, but as the responsibilities end up getting mixed up, when there is a need to make changes to this class, it will be difficult to modify one of these responsibilities without compromising the others. Every change ends up being introduced with a certain level of uncertainty in our system \u2014 especially if there are no automated tests!</p> <p>*God Class: In object-oriented programming, a class that knows too much or does too much.</p> <p>Bad code: <pre><code>/** Before Refactoring **/  \n// Too Many Responsibilities \n// - Measurements of squares \n// - Rendering images of squares \nclass Square {\n\n    int side = 5;\n\n    public int calculateArea() {\n        return side * side;\n    }\n\n    public int calculatePerimeter() {\n        return side * 4; \n    }\n\n    public void draw() {\n        if (highResolutionMonitor) {\n            // Render a high resolution image of a square\n        } else {\n            // Render a normal image of a square\n        }\n    }\n\n    public void rotate(int degree) {\n        // Rotate the image of the square clockwise to\n        // the required degree and re-render \n    }\n\n}\n</code></pre></p> <p>The Square class has two responsibilities, measuring the area of the square and rendering the image to the screen.</p> <p>Refactoring: <pre><code>/** After Refactoring **/  \n// Responsibility: Measurements of squares \n// After Refactoring\nclass Square {\n\n    int side = 5;\n\n    public int calculateArea() {\n        return side * side; \n    }\n\n    public int calculatePerimeter() {\n        return side * 4; \n    }\n\n}\n\n// Responsibility: Rendering images of squares \nclass SquareUI {\n\n    public void draw() {\n        if (highResolutionMonitor) {\n            // Render a high resolution image of a square\n        } else {\n            // Render a normal image of a square\n        }\n    }\n\n    public void rotate(int degree) {\n        // Rotate the iamge of the square clockwise to\n        // the required degree and re-render \n    }\n\n}\n</code></pre></p> <p>Now Square calculates the area and SquareUI handles the rendering.</p> <ul> <li><code>calculateArea()</code>\u00a0and\u00a0<code>calculatePerimeter()</code>\u00a0are related to the square calculation;</li> <li><code>draw()</code>\u00a0and <code>rotate()</code>\u00a0is related to rendering;</li> </ul> <p>Example 2: <pre><code>// Responsibility 1: Handle core student profile data\n// Responsibility 2: Handle Database Operations \nclass Student {\n\n    private String studentId;\n    private String address;\n\n    public void save() {\n        // Serialize object into a string representation \n        String objectStr = MyUtils.serialzieIntoAString(this);\n        Connection connection = null; \n        Statement stmt = null; \n\n        // We are using MYSQL\n        // What if I want to use another database?\n        // This is why Tight Coupling is bad practices for prgramming \n        try {\n            Class.forName(\"com.mysql.jdbc.Driver\");\n            connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/MyDb\", \"root\", \"password\");\n            stmt = connection.createStatement();\n            stmt.execute(\"INSERT INTO STUDENTS VALUES (\" + objectStr + \")\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n    }\n\n    public String getStudentId() {\n        return studentId;\n    }\n\n    public void setStudentId(String studentId) {\n        this.studentId = studentId;\n    }\n\n}\n</code></pre></p> <p>Refactoring: <pre><code>/** After Refactoring **/ \n// Responsibility: Handle Database Operations  \npublic class StudentRepository {\n    public void save(student Student) {\n        // We are using MYSQL\n        // What if I want to use another database?\n        // This is why Tight Coupling is bad practices for prgramming \n        try {\n            Class.forName(\"com.mysql.jdbc.Driver\");\n            connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/MyDb\", \"root\", \"password\");\n            stmt = connection.createStatement();\n            stmt.execute(\"INSERT INTO STUDENTS VALUES (\" + student + \")\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n    }\n}\n\n// Responsibility: Handle core student profile data\nclass Student {\n\n    private String studentId;\n    private Date studentDOB;\n    private String address;\n\n    public String getStudentId() {\n        return studentId;\n    }\n\n    public void setStudentId(String studentId) {\n        this.studentId = studentId;\n    }\n\n}\n\nstudent = new Student(...)\nstudentRepository = StudentRepository(...)\n\nstudentRepository.save(student)\n</code></pre></p>"},{"location":"ServiceMesh/","title":"Service Mesh","text":""},{"location":"ServiceMesh/#what-is-a-service-mesh","title":"What is a service mesh?","text":"<p>A service mesh is a dedicated infrastructure layer that controls service-to-service communication over a network. This method enables separate parts of an application to communicate with each other.</p> <p>A service mesh controls the delivery of service requests in an application. Common features provided by a service mesh include service discovery, load balancing, encryption and failure recovery. Service meshes can make service-to-service communication fast, reliable and secure.</p> <p>This visible infrastructure layer can document how well (or not) different parts of an app interact, so it becomes easier to optimize communication and avoid downtime as an app grows.</p> <p>Moden applications are often built using a microservices architecture, where each service performs a specific business function and it might need to request data from several other services. But what if some services get overloaded with requests? This is where a service mesh comes in - it routes requests from one service to the next, optimizing how all the moving parts work together.</p>"},{"location":"ServiceMesh/#how-a-service-mesh-works","title":"How a service mesh works","text":"<p>In a microservice application, a sidecar attaches to each service. In a container, the sidecar attaches to each application container, VM or container orchestration unit, such as a Kubernetes pod.</p> <p>Sidecars can handle tasks abstracted from the service itself, such as monitoring and security. In a service mesh, each microservice is deployed with a lightweight network proxy (sidecar). The sidecar proxy sits alongside the microservice and manages all incoming and outgoing traffic for that service. Common sidecar proxy implementations include Envoy, Istio Proxy, and Linkerd's proxy.</p> <p>Service instances, sidecars and their interactions make up what is called the data plane in a service mesh. A different layer called the control plane manages tasks such as creating instances, monitoring and implementing policies for network management and security. Control planes can connect to a CLI or a GUI interface for application management.</p> <p>Without a service mesh, each microservice needs to be coded with logic to govern service-to-service communication, which means developers are less focused on business goals. It also means communication failures are harder to diagnose because the logic that governs interservice communication is hidden within each service.</p>"},{"location":"ServiceMesh/#why-adopt-a-service-mesh","title":"Why adopt a service mesh?","text":"<p>An application structured in a microservices architecture might comprise dozens or hundreds of services, all with their own instances that operate in a live environment. It's a big challenge for developers to keep track of which components must interact, monitor their health and performance and make changes to a service or component if something goes wrong.</p> <p>A service mesh enables developers to separate and manage service-to-service communications in a dedicated infrastructure layer. As the number of microservices involved with an application increases, so do the benefits of using a service mesh to manage and monitor them. That\u2019s because a service mesh also captures every aspect of service-to-service communication as performance metrics. Over time, data made visible by the service mesh can be applied to the rules for interservice communication, resulting in more efficient and reliable service requests.</p> <p>For example, If a given service fails, a service mesh can collect data on how long it took before a retry succeeded. As data on failure times for a given service aggregates, rules can be written to determine the optimal wait time before retrying that service, ensuring that the system does not become overburdened by unnecessary retries.</p>"},{"location":"ServiceMesh/#key-features-of-a-service-mesh","title":"Key features of a service mesh","text":"<p>A service mesh framework typically provides many capabilities that make containerized and microservices communications more reliable, secure and observable.</p> <ul> <li> <p>Reliability: Managing communications through sidecar proxies and the control plane improves efficiency and reliability of service requests, policies and configurations. Specific capabilities include load balancing and fault injection.</p> </li> <li> <p>Observability: Service mesh frameworks can provide insights into the behavior and health of services. The control plane can collect and aggregate telemetry data from component interactions to determine service health, such as traffic and latency, distributed tracing and access logs. Third-party integration with tools, such as Prometheus, Elasticsearch and Grafana, enables further monitoring and visualization.</p> </li> <li> <p>Security: Service mesh can automatically encrypt communications and distribute security policies, including authentication and authorization, from the network to the application and individual microservices. Centrally managing security policies through the control plane and sidecar proxies helps keep up with increasingly complex connections within and between distributed applications. Additionally, it provides features like mutual TLS (mTLS) encryption between services, role-based access control (RBAC), and the ability to enforce security policies at the network level.</p> </li> <li> <p>Service Discovery: Service meshes typically integrate with service discovery mechanisms, such as Kubernetes DNS or etcd, to maintain an up-to-date list of available services and their locations. This enables automatic discovery and routing to services.</p> </li> <li> <p>Traffic Routing: The sidecar proxy controls the routing of traffic between microservices. It intercepts incoming requests and forwards them to the appropriate destination based on routing rules defined in the service mesh configuration. These routing rules can be based on service names, versions, or custom criteria.</p> </li> <li> <p>Load Balancing: Service mesh proxies can implement various load balancing algorithms to distribute incoming requests evenly among multiple instances of a service. This helps optimize resource utilization and improve application performance.</p> </li> <li> <p>Service-to-Service Communication: When one microservice needs to communicate with another, it sends its requests to the local sidecar proxy. The sidecar proxy, based on the routing rules, forwards the request to the destination service's sidecar proxy, effectively establishing secure and reliable communication between services.</p> </li> <li> <p>Traffic Control and Policies: Service meshes provide fine-grained traffic control and policy enforcement capabilities. You can configure features like retries, circuit breaking, timeouts, rate limiting, and security policies like authentication and authorization. These policies help improve the reliability and security of microservices communication.</p> </li> <li> <p>Observability: Service meshes collect telemetry data about the communication between services, including metrics, traces, and logs. This data is often sent to observability tools like Prometheus, Grafana, Jaeger, or Zipkin. Operators and developers can use these tools to monitor and troubleshoot application performance and errors.</p> </li> </ul>"},{"location":"ServiceMesh/#service-mesh-benefits-and-drawbacks","title":"Service mesh benefits and drawbacks","text":"<p>A service mesh addresses some large issues with managing service-to-service communication, but not all. Some advantages of a service mesh are as follows:</p> <ul> <li>Simplifies communication between services in both microservices and containers.</li> <li>Easier to diagnose communication errors, because they would occur on their own infrastructure layer.</li> <li>Supports security features such as encryption, authentication and authorization.</li> <li>Allows for faster development, testing and deployment of an application.</li> <li>Sidecars placed next to a container cluster is effective in managing network services.</li> </ul> <p>Some downsides to service mesh are as follows:</p> <ul> <li>Runtime instances increase through use of a service mesh.</li> <li>Each service call must first run through the sidecar proxy, which adds a step and can add some latency to requests, although this is typically minimal. Overusing features like mTLS can also impact latency.</li> <li>Operational Overhead. Network management complexity is abstracted and centralized, but not eliminated -- someone must integrate service mesh into workflows and manage its configuration.</li> <li>Complexity and Learning Curve</li> <li>The deployment of additional proxy sidecars and observability tools can lead to increased infrastructure costs.</li> </ul>"},{"location":"ServiceMesh/#popular-service-mesh-implementations","title":"Popular service mesh implementations","text":"<ul> <li>Istio</li> <li>Consul</li> <li>AWS App Mesh</li> <li>Linkerd</li> </ul>"},{"location":"ServiceMesh/#refs","title":"Refs","text":"<ul> <li>TechTarget-definition-service-mesh</li> <li>RedHat-what-is-a-service-mesh</li> </ul>"},{"location":"ServiceMesh/istio/","title":"Istio","text":"<p>Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including:</p> <ul> <li>Secure service-to-service communication in a cluster with TLS encryption, strong identity-based authentication and authorization</li> <li>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic</li> <li>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</li> <li>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</li> <li>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress</li> </ul> <p>Istio is designed for extensibility and can handle a diverse range of deployment needs. Istio\u2019s control plane runs on Kubernetes, and you can add applications deployed in that cluster to your mesh, extend the mesh to other clusters, or even connect VMs or other endpoints running outside of Kubernetes.</p>"},{"location":"ServiceMesh/istio/#architecture-overview","title":"Architecture Overview","text":"<p>An Istio service mesh is logically split into a data plane and a control plane.</p> <ul> <li> <p>The control plane is responsible for managing and configuring the data plane components. It acts as the centralized brain of the service mesh. Key components within the control plane include:</p> <ul> <li>Istio Pilot: Pilot is responsible for service discovery and traffic management. It collects and distributes configuration information to the Envoy proxies in the data plane.</li> <li>Istio Citadel: Citadel manages security features like certificate issuance and rotation for service-to-service communication encryption.</li> <li>Istio Galley: Galley validates and ingests configuration data, ensuring that it is correct and consistent before distributing it to other control plane components.</li> </ul> <p>The control plane's main responsibility is to configure and enforce policies for traffic routing, security, and observability across services in the mesh.</p> </li> <li> <p>The data plane is the communication between services. Without a service mesh, the network doesn\u2019t understand the traffic being sent over, and can\u2019t make any decisions based on what type of traffic it is, or who it is from or to. It's composed of a set of intelligent proxies (Envoy) deployed as sidecars, which run alongside each service instance in the mesh. These proxies mediate and control all network communication between microservices and also enforces policies defined in the control plane. They also collect and report telemetry on all mesh traffic. It plays a crucial role in:</p> <ul> <li>Load Balancing: Envoy proxies perform intelligent load balancing and distribute traffic to service instances based on the routing rules defined in the control plane.</li> <li>Traffic Routing: Envoy proxies follow routing rules and policies provided by the control plane to direct traffic to the appropriate service instances.</li> <li>Security: Data plane proxies can enforce mTLS (mutual Transport Layer Security) and authentication policies to secure service-to-service communication.</li> <li>Telemetry and Observability: Envoy collects telemetry data such as request/response metrics and sends them to monitoring tools (e.g., Prometheus, Grafana) for analysis and visualization.</li> </ul> </li> </ul> <p>The following diagram shows the different components that make up each plane:</p> <p></p> <p>The flow of communication in an Istio-enabled service mesh works as follows:</p> <ol> <li>When a client service/microservice/application wants to communicate with another service, it sends a request to its local Envoy proxy (the proxy deployed as a sidecar with the current service).</li> <li>The local Envoy proxy checks with the control plane components (e.g., Pilot) to determine how to route the request based on service discovery and routing rules.</li> <li>The Envoy proxy in the client's data plane handles the request according to the control plane's instructions, including load balancing and security policies.</li> <li>The request is forwarded to the appropriate Envoy proxy in the target service's data plane.</li> <li>The target service processes the request and sends a response, which follows a similar path back to the client through the data plane proxies.</li> </ol>"},{"location":"ServiceMesh/istio/#envoy","title":"Envoy","text":"<p>Envoy is a high-performance proxy developed in C++ to mediate all inbound and outbound traffic for all services in the service mesh. Envoy proxies are the only Istio components that interact with data plane traffic.</p> <p>Envoy proxies are deployed as sidecars to services, logically augmenting the services with Envoy\u2019s many built-in features, for example:</p> <ul> <li>Dynamic service discovery</li> <li>Load balancing</li> <li>TLS termination</li> <li>HTTP/2 and gRPC proxies</li> <li>Circuit breakers</li> <li>Health checks</li> <li>Staged rollouts with %-based traffic split</li> <li>Fault injection</li> <li>Rich metrics</li> </ul> <p>The sidecar proxy model also allows you to add Istio capabilities to an existing deployment without requiring you to rearchitect or rewrite code.</p> <p>Some of the Istio features and tasks enabled by Envoy proxies include:</p> <ul> <li>Traffic control features: enforce fine-grained traffic control with rich routing rules for HTTP, gRPC, WebSocket, and TCP traffic.</li> <li>Network resiliency features: setup retries, failovers, circuit breakers, and fault injection.</li> <li>Security and authentication features: enforce security policies and enforce access control and rate limiting defined through the configuration API.</li> <li>Pluggable extensions model based on WebAssembly that allows for custom policy enforcement and telemetry generation for mesh traffic.</li> </ul>"},{"location":"ServiceMesh/istio/#istiod","title":"Istiod","text":"<p>Istiod provides service discovery, configuration and certificate management.</p> <p>Istiod converts high level routing rules that control traffic behavior into Envoy-specific configurations, and propagates them to the sidecars at runtime. Pilot abstracts platform-specific service discovery mechanisms and synthesizes them into a standard format that any sidecar conforming with the Envoy API can consume.</p> <p>You can use Istio\u2019s Traffic Management API to instruct Istiod to refine the Envoy configuration to exercise more granular control over the traffic in your service mesh.</p> <p>Istiod security enables strong service-to-service and end-user authentication with built-in identity and credential management. You can use Istio to upgrade unencrypted traffic in the service mesh. Using Istio, operators can enforce policies based on service identity rather than on relatively unstable layer 3 or layer 4 network identifiers. Additionally, you can use Istio\u2019s authorization feature to control who can access your services.</p> <p>Istiod acts as a Certificate Authority (CA) and generates certificates to allow secure mTLS communication in the data plane.</p>"},{"location":"ServiceMesh/istio/#traffic-management","title":"Traffic Management","text":"<p>Istio\u2019s traffic routing rules let you easily control the flow of traffic and API calls between services. Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits.</p> <p>Istio\u2019s traffic management model relies on the Envoy proxies that are deployed along with your services. All traffic that your mesh services send and receive (data plane traffic) is proxied through Envoy, making it easy to direct and control traffic around your mesh without making any changes to your services.</p>"},{"location":"ServiceMesh/istio/#introducing-istio-traffic-management","title":"Introducing Istio traffic management","text":"<p>In order to direct traffic within your mesh, Istio needs to know where all your endpoints are, and which services they belong to. To populate its own service registry, Istio connects to a service discovery system. For example, if you\u2019ve installed Istio on a Kubernetes cluster, then Istio automatically detects the services and endpoints in that cluster.</p> <p>Using this service registry, the Envoy proxies can then direct traffic to the relevant services. Most microservice-based applications have multiple instances of each service workload to handle service traffic, sometimes referred to as a load balancing pool. By default, the Envoy proxies distribute traffic across each service\u2019s load balancing pool using a least requests model, where each request is routed to the host with fewer active requests from a random selection of two hosts from the pool; in this way the most heavily loaded host will not receive requests until it is no more loaded than any other host.</p> <p>While Istio\u2019s basic service discovery and load balancing gives you a working service mesh, it\u2019s far from all that Istio can do. In many cases you might want more fine-grained control over what happens to your mesh traffic. You might want to direct a particular percentage of traffic to a new version of a service as part of A/B testing, or apply a different load balancing policy to traffic for a particular subset of service instances. You might also want to apply special rules to traffic coming into or out of your mesh, or add an external dependency of your mesh to the service registry. You can do all this and more by adding your own traffic configuration to Istio using Istio\u2019s traffic management API.</p> <p>The rest of this guide examines each of the traffic management API resources and what you can do with them. These resources are:</p>"},{"location":"ServiceMesh/istio/#virtual-services","title":"Virtual services","text":"<p>A Virtual Service, along with destination rules, defines how incoming traffic should be routed to different versions or subsets of a service based on criteria such as HTTP headers, paths, or request attributes. When a request arrives, Istio's Virtual Service component examines the request to determine the appropriate destination service and version.</p> <p>With a virtual service, you can specify traffic behavior for one or more hostnames. You use routing rules in the virtual service that tell Envoy how to send the virtual service\u2019s traffic to appropriate destinations. Route destinations can be different versions of the same service or entirely different services.</p> <p>A typical use case is to send traffic to different versions of a service, specified as service subsets. Clients send requests to the virtual service host as if it was a single entity, and Envoy then routes the traffic to the different versions depending on the virtual service rules: for example, \u201c20% of calls go to the new version\u201d or \u201ccalls from these users go to version 2\u201d. This allows you to, for instance, create a canary rollout where you gradually increase the percentage of traffic that\u2019s sent to a new service version. The traffic routing is completely separate from the instance deployment, meaning that the number of instances implementing the new service version can scale up and down based on traffic load without referring to traffic routing at all. By contrast, container orchestration platforms like Kubernetes only support traffic distribution based on instance scaling, which quickly becomes complex.</p> <ul> <li>Address multiple application services through a single virtual service. If your mesh uses Kubernetes, for example, you can configure a virtual service to handle all services in a specific namespace. Mapping a single virtual service to multiple \u201creal\u201d services is particularly useful in facilitating turning a monolithic application into a composite service built out of distinct microservices without requiring the consumers of the service to adapt to the transition. Your routing rules can specify \u201ccalls to these URIs of monolith.com go to microservice A\u201d, and so on. You can see how this works in one of our examples below.</li> <li>Configure traffic rules in combination with gateways to control ingress and egress traffic.</li> </ul> <p>In some cases you also need to configure destination rules to use these features, as these are where you specify your service subsets. Specifying service subsets and other destination-specific policies in a separate object lets you reuse these cleanly between virtual services.</p> <p>Example:</p> <p>The following virtual service routes requests to different versions of a service depending on whether the request comes from a particular user.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews # the virtual service is applied to any traffic destined for reviews service\n  http:\n    - match: # the following match clause matches requests with end-user header with value jason\n        - headers:\n            end-user:\n              exact: jason\n      route: # if the request matches the above match clause, it is routed to reviews:v2\n        - destination:\n            host: reviews\n            subset: v2\n    - route: # if the request doesn't match the above match clause, it is routed to reviews:v3\n        - destination:\n            host: reviews\n            subset: v3 # the subset v3 of reviews service is defined in the destination rule\n</code></pre>"},{"location":"ServiceMesh/istio/#destination-rules","title":"Destination rules","text":"<p>Destination Rules specify the traffic policies (such as load balancing algorithms, circuit breakers, and mTLS settings) for a particular service or subset. These rules are associated with a service and define how traffic should be handled at the destination.</p> <p>You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic\u2019s \u201creal\u201d destination.</p> <p>In particular, you use destination rules to specify named service subsets, such as grouping all a given service\u2019s instances by version. You can then use these service subsets in the routing rules of virtual services to control the traffic to different instances of your services.</p> <p>Destination rules also let you customize Envoy\u2019s traffic policies when calling the entire destination service or a particular service subset, such as your preferred load balancing model, TLS security mode, or circuit breaker settings.</p> <p>Example:</p> <p>The following example destination rule configures three different subsets for the my-svc destination service, with different load balancing policies:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: my-svc # the destination service\n  trafficPolicy: # the traffic policy for the destination service\n    loadBalancer:\n      simple: RANDOM # the load balancing policy for the destination service\n  subsets:\n    - name: v1\n      labels:\n        version: v1\n    - name: v2\n      labels:\n        version: v2\n      trafficPolicy: # the load balancing policy for the v2 subset\n        loadBalancer:\n          simple: ROUND_ROBIN\n    - name: v3\n      labels:\n        version: v3\n</code></pre>"},{"location":"ServiceMesh/istio/#gateways","title":"Gateways","text":"<p>You use a gateway to manage inbound and outbound traffic for your mesh (e.g., the internet or other networks), letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads.</p> <p>Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, Istio gateways let you use the full power and flexibility of Istio\u2019s traffic routing. You can do this because Istio\u2019s Gateway resource just lets you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then instead of adding application-layer traffic routing (L7) to the same API resource, you bind a regular Istio virtual service to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Istio mesh.</p> <p>Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. You can also use a gateway to configure a purely internal proxy.</p> <p>Example:</p> <p>The following example shows a possible gateway configuration for external HTTPS ingress traffic:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: ext-host-gwy\nspec:\n  selector:\n    istio: ingressgateway # default gateway proxy deployment name\n  servers:\n    - port:\n        number: 443\n        name: https\n        protocol: HTTPS\n      hosts:\n        - ext-host.example.com # the gateway is applied to requests with this host\n      tls:\n        mode: SIMPLE # # enables HTTPS on this port\n        credentialName: ext-host-cert # name of the certificate, for example using cert-manager\n</code></pre> <p>This gateway configuration lets HTTPS traffic from ext-host.example.com into the mesh on port 443, but doesn\u2019t specify any routing for the traffic.</p> <p>To specify routing and for the gateway to work as intended, you must also bind the gateway to a virtual service. You do this using the virtual service\u2019s gateways field, as shown in the following example:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: virtual-svc\nspec:\n  hosts:\n    - ext-host.example.com\n  gateways:\n    - ext-host-gwy\n</code></pre>"},{"location":"ServiceMesh/istio/#service-entries","title":"Service entries","text":"<p>Service Entries allow you to add external services or domains to the Istio service mesh. These are typically used when your mesh needs to communicate with external services or legacy systems. Service Entries specify how traffic should be routed to external services, including DNS resolution, protocol, and port details.</p> <p>You use a service entry to add an entry to the service registry that Istio maintains internally. After you add the service entry, the Envoy proxies can send traffic to the service as if it was a service in your mesh. Configuring service entries allows you to manage traffic for services running outside of the mesh, including the following tasks:</p> <ul> <li>Redirect and forward traffic for external destinations, such as APIs consumed from the web, or traffic to services in legacy infrastructure.</li> <li>Define retry, timeout, and fault injection policies for external destinations.</li> <li>Run a mesh service in a Virtual Machine (VM) by adding VMs to your mesh.</li> </ul> <p>You don\u2019t need to add a service entry for every external service that you want your mesh services to use. By default, Istio configures the Envoy proxies to passthrough requests to unknown services. However, you can\u2019t use Istio features to control the traffic to destinations that aren\u2019t registered in the mesh.</p> <p>Example:</p> <p>The following example mesh-external service entry adds the ext-svc.example.com external dependency to Istio\u2019s service registry:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: svc-entry\nspec:\n  hosts: # specify the external resource using the hosts field. You can qualify it fully or use a wildcard prefixed domain name.\n    - ext-svc.example.com\n  ports:\n    - number: 443\n      name: https\n      protocol: HTTPS\n  location: MESH_EXTERNAL\n  resolution: DNS\n</code></pre> <p>You can configure virtual services and destination rules to control traffic to a service entry in a more granular way, in the same way you configure traffic for any other service in the mesh. For example, the following destination rule adjusts the TCP connection timeout for requests to the <code>ext-svc.example.com</code> external service that we configured using the service entry:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: ext-res-dr\nspec:\n  host: ext-svc.example.com\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        connectTimeout: 1s\n</code></pre>"},{"location":"ServiceMesh/istio/#sidecars","title":"Sidecars","text":"<p>Sidecar proxies (typically Envoy) are deployed alongside each service instance within the mesh. They handle the actual routing of traffic within the mesh based on the rules and policies defined by Virtual Services, Destination Rules, and other Istio components. Sidecar proxies intercept incoming and outgoing traffic, making routing decisions based on the configuration provided by the control plane.</p> <p>By default, Istio configures every Envoy proxy to accept traffic on all the ports of its associated workload, and to reach every workload in the mesh when forwarding traffic. You can use a sidecar configuration to do the following:</p> <ul> <li>Fine-tune the set of ports and protocols that an Envoy proxy accepts.</li> <li>Limit the set of services that the Envoy proxy can reach.</li> </ul> <p>You might want to limit sidecar reachability like this in larger applications, where having every proxy configured to reach every other service in the mesh can potentially affect mesh performance due to high memory usage.</p> <p>You can specify that you want a sidecar configuration to apply to all workloads in a particular namespace, or choose specific workloads using a <code>workloadSelector</code>. For example, the following sidecar configuration configures all services in the <code>bookinfo</code> namespace to only reach services running in the same namespace and the Istio control plane (needed by Istio\u2019s egress and telemetry features):</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Sidecar\nmetadata:\n  name: default\n  namespace: bookinfo\nspec:\n  egress: # configure egress traffic\n    - hosts: # the hosts that the sidecar can reach\n        - \"./*\" # all services in the same namespace\n        - \"istio-system/*\" # the Istio control plane\n</code></pre> <p>See the Sidecar reference for more details.</p>"},{"location":"ServiceMesh/istio/#traffic-flow-when-a-request-comes-into-the-istio-service-mesh","title":"Traffic Flow When a Request Comes Into the Istio Service Mesh:","text":"<ol> <li> <p>Request Arrival</p> </li> <li> <p>An external request (e.g., an HTTP request from a client) arrives at the Istio Gateway, which is configured to accept incoming traffic for a specific host and port.</p> </li> <li> <p>Gateway Processing:</p> </li> <li> <p>The Gateway examines the request and forwards it to the appropriate Virtual Service based on the host and port configuration.</p> </li> <li> <p>Virtual Service Routing:</p> </li> <li> <p>The Virtual Service, based on its rules and criteria (e.g., matching HTTP paths or headers), determines which destination service or subset should receive the traffic.</p> </li> <li> <p>Destination Rules Application:</p> </li> <li> <p>The Destination Rules associated with the selected destination service are consulted to apply traffic policies, such as load balancing, circuit breaking, or mTLS.</p> </li> <li> <p>Sidecar Proxy Handling:</p> </li> <li> <p>The Sidecar proxies (Envoy) for both the source and destination services are involved in routing the traffic.</p> </li> <li>The source-side Envoy proxy intercepts the outgoing traffic and applies any policies related to retries, timeouts, or authentication.</li> <li> <p>The destination-side Envoy proxy receives the incoming traffic and forwards it to the appropriate service instance based on load balancing rules.</p> </li> <li> <p>Service Execution:</p> </li> <li> <p>The target service processes the request and generates a response.</p> </li> <li> <p>Response Path:</p> </li> <li> <p>The response follows a similar path in reverse, going through the Sidecar proxies, adhering to policies set in Destination Rules and Virtual Services, and eventually returning to the client through the Gateway.</p> </li> </ol>"},{"location":"ServiceMesh/istio/#how-to-examples","title":"How To - Examples","text":"<p>See the examples/service-mesh/istio folder for examples on how to use Istio to manage traffic in your service mesh.</p>"},{"location":"SystemsCommunication/","title":"Systems Communication","text":"<p>Communication between different components or systems is often necessary. Synchronous communication is a type of communication in which a request is sent and the system waits for a response before continuing. This type of communication is used when the system requires an immediate response to continue its processing.</p> <p>On the other hand, asynchronous communication is a type of communication in which a request is sent and the system doesn't require an immediate response. The system can continue its processing without waiting for a response. The response will be processed later when it's available.</p> <p>Synchronous communication \u21d2 sends a request and waits for a response. Examples: phone call, question and answer.</p> <p>Asynchronous communication \u21d2 sends a request and doesn't require an immediate response. Examples: postal service, sends a letter and doesn't expect a response at that moment.</p> <p>In software development, when building a web application, synchronous communication is used between the client and the server. When a user submits a request, the server processes the request and sends back a response immediately. However, asynchronous communication is used when performing background tasks such as sending an email or processing a large file. The user doesn't need to wait for the task to complete before continuing to use the application.</p>"},{"location":"SystemsCommunication/#some-types-of-system-communication","title":"Some Types of System Communication","text":"<ul> <li>REST</li> <li>gRPC</li> <li>GraphQL</li> </ul>"},{"location":"SystemsCommunication/GraphQL/","title":"GraphQL","text":"<p>Query language for APIs that allows you to request and receive only the data you need, in a single request, reducing the number of API requests required. It was developed by Facebook in 2012 and was publicly released in 2015.</p> <ul> <li>Hierarchical: Queries are hierarchical, meaning that the shape of the query matches the shape of the data returned, allowing for more efficient and flexible data retrieval.</li> <li>Strongly Typed: Provides a type system that allows you to specify the structure of your data and catch errors before they happen.</li> <li>Client-Specified Queries: The client specifies the data it needs, allowing for more efficient API requests and reduced bandwidth usage.</li> <li>Schema-Driven: The schema is the contract between the server and the client, defining the data types, fields, and operations that can be performed.</li> </ul> <p>\"Queries\" perform queries and retrieve data according to the request. They are executed in parallel.</p> <p>\"Mutation\" performs the process of create, update, and delete. It is executed serially.</p> <p>In which cases can we use it?</p> <ul> <li>When you need flexible and efficient API for querying data.</li> <li>When you have complex or nested data structures.</li> <li>When you want to reduce the number of API requests required.</li> </ul> GraphQL REST Allow to retrieve data from multiple resources in a single request. Clients must make separate requests for each resource they need. Clients retrieve only the data they need. Clients receive all the data associated with a resource, whether or not they need it. Provides documentation and validation for clients. Does not have a standardized schema, which can make it difficult for clients to understand the structure and relationships of resources."},{"location":"SystemsCommunication/REST/","title":"REST","text":"<p>If you are building REST APIs or REST Services you're using HTTP. Technically, REST services can be provided over any application layer protocol as long as they conform to certain properties. In practice, basically, everyone uses HTTP Protocol.</p> <p>Advantages:</p> <ul> <li>Simplicity</li> <li>Stateless \u2192 The server doesn't maintain any information about the client. Each request contains all the information necessary for the server to process it. Each request is a new request, which is why we need to pass headers all the time (JWT Token, etc.).</li> <li>Cacheable</li> </ul>"},{"location":"SystemsCommunication/REST/#rest-vs-restful","title":"REST vs RESTful","text":"<p>A REST API (also known as RESTful API) is an application programming interface (API or web API) that conforms to the constraints of REST architectural style and allows for interaction with RESTful web services.</p> <p>An API is a set of rules that define how applications can connect to and communicate with each other. A REST API is an API that conforms to the design principles of the REST. For this reason, REST APIs are sometimes referred to RESTful APIs.</p> <p>The only requirement for an API implement REST is that they align to the following six REST design principles - also known as architectural constraints:</p> <ol> <li> <p>Uniform interface. It indicates that the server transfers information in a standard format. All API requests for the same resource should look the same, no matter where the request comes from. The REST API should ensure that the same piece of data, such as the name or email address of a user, belongs to only one uniform resource identifier (URI). Resources shouldn\u2019t be too large but should contain every piece of information that the client might need. Uniform interface imposes four architectural constraints:</p> <ul> <li>Requests should identify resources. They do so by using a uniform resource identifier.</li> <li>Clients have enough information in the resource representation to modify or delete the resource if they want to. The server meets this condition by sending metadata that describes the resource further.</li> <li>Clients receive information about how to process the representation further. The server achieves this by sending self-descriptive messages that contain metadata about how the client can best use them.</li> <li>Clients receive information about all other related resources they need to complete a task. The server achieves this by sending hyperlinks in the representation so that clients can dynamically discover more resources.</li> </ul> </li> <li> <p>Client-server decoupling. In REST\u00a0API design, client and server applications must be completely independent of each other. The only information the client application should know is the URI of the requested resource; it can't interact with the server application in any other ways. Similarly, a server application shouldn't modify the client application other than passing it to the requested data via HTTP.</p> </li> <li> <p>Statelessness. statelessness refers to a communication method in which the server completes every client request independently of all previous requests. Clients can request resources in any order, and every request is stateless or isolated from other requests. This REST API design constraint implies that the server can completely understand and fulfill the request every time. REST APIs are stateless, meaning that each request needs to include all the information necessary for processing it. In other words, REST APIs do not require any server-side sessions. Server applications aren\u2019t allowed to store any data related to a client request. No client information is stored between get requests and each request is separate and unconnected.</p> </li> <li> <p>Cacheability. When possible, resources should be cacheable on the client or server side. Server responses also need to contain information about whether caching is allowed for the delivered resource. The goal is to improve performance on the client side, while increasing scalability on the server side.</p> </li> <li> <p>Layered system architecture. In REST APIs, the calls and responses go through different layers. As a rule of thumb, don\u2019t assume that the client and server applications connect directly to each other. There may be a number of different intermediaries in the communication loop. REST APIs need to be designed so that neither the client nor the server can tell whether it communicates with the end application or an intermediary. In a layered system architecture, the client can connect to other authorized intermediaries between the client and server, and it will still receive responses from the server. Servers can also pass on requests to other servers. You can design your RESTful web service to run on several servers with multiple layers such as security, application, and business logic, working together to fulfill client requests. These layers remain invisible to the client.</p> </li> <li> <p>Code on demand (optional). REST APIs usually send static resources, but in certain cases, responses can also contain executable code (such as Java applets). In these cases, the code should only run on-demand. In REST architectural style, servers can temporarily extend or customize client functionality by transferring software programming code to the client. For example, when you fill a registration form on any website, your browser immediately highlights any mistakes you make, such as incorrect phone numbers. It can do this because of the code sent by the server.</p> </li> </ol>"},{"location":"SystemsCommunication/REST/#maturity-levels","title":"Maturity levels","text":"<p>Level 0: The Swamp of POX</p> <p>Performing an operation on the server without any standardization.</p> <p>The Swamp of POX (Plain Old XML) means that you\u2019re using HTTP. Technically, REST services can be provided over any application layer protocol as long as they conform to certain properties. In practice, basically, everyone uses HTTP.</p> <p>Level zero of maturity does not make use of any of URI, HTTP Methods, and HATEOAS capabilities.</p> <p>Level 1: Use of resources</p> <p>REST\u2019s \u2018resources\u2019 are the core pieces of data that your application acts on. These will often correspond to the Models in your application</p> <p>API design at Level 1 is all about using different URLs to interact with the different resources in your application.</p> URL Operation /products/1 Retrieve /products Insert /products/1 Update /products/1 Remove <p>Level 2: HTTP Verbs</p> <p>If we want to get a list of Pages, we make a GET request to <code>/page</code>, but if we want to create a new Page, we use POST rather than GET to the same resource - <code>/page</code>.</p> Verb Scope Semantics GET collection Retrieve all resources in a collection GET resource Retrieve a single resource HEAD collection Retrieve all resources in a collection (header only) HEAD resource Retrieve a single resource (header only) POST collection Create a new resource in a collection PUT resource Update a resource PATCH resource Update a resource DELETE resource Delete a resource OPTIONS any Return available HTTP methods and other options <p>The use of HTTP verbs like GET, POST, PUT, and DELETE is another important principle of REST. These verbs are used to describe the operation being performed on a resource. For example, GET is used to retrieve information, POST is used to create a new resource, PUT is used to update an existing resource, and DELETE is used to remove a resource.</p> <p>Level 3: HATEOAS: Hypermedia as the Engine of Application State</p> <p>Responds to the endpoint by returning what else can be done based on what was just done.</p> <pre><code>{\n    \"account\": {\n        \"account_number\": 123,\n        \"balance\" : {\n            \"currency\": \"usd\",\n            \"value\": 100.00\n        },\n        \"links\": {\n            \"deposit\": \"/accounts/123/deposit\",\n            \"withdraw\": \"/accounts/123/withdraw\",\n            \"transfer\": \"/accounts/123/transfer\",\n            \"close\": \"/accounts/123/close\"\n        }   \n    }\n\n}\n</code></pre> <p>HATEOAS is an advanced concept in REST that allows a server to provide information to the client about what actions can be performed next. This allows for a more dynamic and flexible API design.</p>"},{"location":"SystemsCommunication/gRPC/","title":"gRPC","text":"<p>gRPC is a high-performance framework developed by Google that facilitates communication between systems in an extremely fast and lightweight way, independent of language. gRPC utilizes Remote Procedure Call (RPC) to allow the client to call a function on the server, enabling developers to build scalable and distributed systems with ease.</p> <p>One of the main benefits of gRPC is its speed. By utilizing Protocol Buffers, a mechanism created and used by Google to serialize structured data, gRPC can send and receive data faster than traditional text-based formats like JSON. Additionally, gRPC supports bidirectional streaming using HTTP/2, which allows data to be sent in binary format and compressed, reducing network resource consumption and latency.</p> <p>gRPC is ideal for microservices architectures and can be used in a variety of environments, including mobile, browsers, and backend applications. It also offers automatic library generation, which can significantly reduce development time.</p> <p>In which cases can we use it?</p> <ul> <li>Ideal for microservices;</li> <li>Mobile, Browsers, and Backend;</li> <li>Automatic library generation;</li> <li>Bidirectional streaming using HTTP/2 (Faster than HTTP 1, allows sending data in binary format, data streaming, etc.)</li> </ul> <p>Languages with official support:</p> <ul> <li>gRPC-GO</li> <li>gRPC-JAVA</li> <li>gRPC-C<ul> <li>C++</li> <li>Python</li> <li>Ruby</li> <li>Objective C</li> <li>PHP</li> <li>C#</li> <li>Node.js</li> <li>Dart</li> <li>Kotlin / JVM</li> </ul> </li> </ul> <p>RPC \u21d2 Remote Procedure Call</p> <p>The client calls the function on the server</p> <pre><code>    Client                          Server\nserver.soma(a,b)                 func soma(int a, int b) {}\n</code></pre> <p>Protocol Buffers \u21d2 is a mechanism created and used by Google to serialize structured data. It defines how you want the data to be structured - in a file with the extension <code>.proto</code>.</p> <pre><code>syntax = \"proto3\"\n\nmessage SearchRequest {\n    string query = 1;\n    int32 page_number = 2;\n    int32 result_per_page = 3;\n}\n</code></pre> <p>Protocol Buffers vs JSON While JSON is a popular format for transmitting data, it has some drawbacks in terms of performance and network resource consumption. Protocol Buffers, on the other hand, offer several advantages over JSON:</p> <ul> <li>Binary files &lt; JSON, JSON is heavier because it is plain text;</li> <li>Lightweight: The process of serialization and deserialization with Protocol Buffers is more lightweight than with JSON, requiring less CPU resources.</li> <li>Network resource consumption: Protocol Buffers consume less network resources than JSON.</li> <li>Process is faster;</li> </ul> <p>HTTP/2</p> <ul> <li>Launched in 2015;</li> <li>The transmitted data is binary and not text like in HTTP 1.1;</li> <li>Uses the same TCP connection to send and receive data from the client and server (Multiplex);</li> <li>Server push;</li> <li>Headers are compressed;</li> <li>Uses fewer network resources;</li> <li>Process is faster.</li> </ul> <p>REST vs gRPC</p> REST gRPC Text / JSON Protocol Buffers Unidirecional Bidirecional e Asynchronous High Latency Low Latency No contract (higher chance of errors) Defined contract (.proto) No streaming support (Request / Response) Streaming support"}]}